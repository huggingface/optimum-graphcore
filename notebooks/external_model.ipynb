{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "First of all, make sure your environment has installed the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install optimum[graphcore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the versions of Transformers and Optimum Graphcore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.20.0\n",
      "0.3.3.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import optimum.graphcore\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(optimum.graphcore.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3KD3WXU3l-O"
   },
   "source": [
    "# Train an external language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAscNNUD3l-P"
   },
   "source": [
    "In this notebook, we'll see how to train a model that is not supported by Optimum Graphcore and not even in [ðŸ¤— Transformers](https://github.com/huggingface/transformers) on a language modeling task.\n",
    "\n",
    "We will see how to easily load and preprocess the dataset for each one of those tasks, and how to use the `IPUTrainer` API to train a model on it.\n",
    "\n",
    "This notebook assumes you have trained a tokenizer on the corpus you are using, see the [How to train a tokenizer](https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb) notebook ([open in colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r_n9OWV3l-Q"
   },
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kswRMhPc3l-Q"
   },
   "source": [
    "For each of those tasks, we will use the [Wikitext 2]() dataset as an example. You can load it very easily with the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n2ZRs1cL3l-R",
    "outputId": "11151c56-be90-4d11-e7df-db85e745ca5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/localdata/jincheng/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d485030f09849d0906e48ed5e98b92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEA1ju653l-p"
   },
   "source": [
    "## Causal Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5io6fY_d3l-u"
   },
   "source": [
    "To tokenize all our texts with the same vocabulary that was used when training the model, we could download a pretrained tokenizer. Though we plan to define our own model, here we borrow GPT2's tokenizer. This is all done by the `AutoTokenizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iAYlS40Z3l-v"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer_checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpOiBrJ13l-y"
   },
   "source": [
    "We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that calls the tokenizer on our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lS2m25YM3l-z"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9xVAa3s3l-2"
   },
   "source": [
    "Then we apply it to all the splits in our `datasets` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NVAO0H8u3l-3",
    "outputId": "30d88b8a-e353-4e13-f709-8e5e06ef747b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7f68f58d21f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734ddedf5a9344a09669daffec8d09e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cf7300a50f4919b98d1c64642b0f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bf54281a1f4518a0b71d37dfd8b7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745d0431c19a4d558562d67f3b6c6549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc1eba7e4e349439816b41cb883118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1801f696697d4bc9ad9f3cb40ec1691b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f526b7c80dd14382aa4c6eabd25839b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57527efa5da6490e8b3a6211817c3bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef049703a6848d89f78702da02e23ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac200d0ca55c4ab1ae4b4812512e46d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46664c76a3b14de8b33dc69269080fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ce1214194b437fb82d31e5290294fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obvgcXda3l--"
   },
   "source": [
    "Then we grab the maximum length our model was pretrained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DVHs5aCA3l-_"
   },
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpNfGiMw3l_A"
   },
   "source": [
    "Then we write the preprocessing function that will group our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iaAJy5Hu3l_B"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGJWXtNv3l_C"
   },
   "source": [
    "Again we apply it to all the splits in our `datasets` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gXUSfBrq3l_C",
    "outputId": "34e55885-3d8f-4f05-cbdb-706ce56a25f8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746035f67ddb46e08c2c240fd64a2466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c6e60355364facb9c4c1d84457262a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f70035ab58940c1aa5626be20e29edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50807f269834072ad9c9f6bfe56c272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e23717919348c1818e47f7937f25f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb86615a777d4a0397ff9efb89971199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f8a66e494c47269c0d20c43a436df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c9018a973b4f05954a76a31ff42cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc42698f4b594f328f860ad3cfba36e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9053c2a7cf4c818cc835c8f0bb323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed09bc3176e482ba32c86bb3b31c91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3013900d9d9147b58c989e96dc2c79c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a customized model, which is just a simple implementation GPT2. Note that there is nothing IPU-specific or ðŸ¤— Transformers-related in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, block_size, vocab_size, d_model, nhead, dim_feedforward, nlayers, dropout=0.1, embd_pdrop=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embeddings = nn.Embedding(block_size, d_model)\n",
    "        self.drop = nn.Dropout(embd_pdrop)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, nlayers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.tie_weights(self.lm_head, self.word_embeddings)\n",
    "\n",
    "\n",
    "    def tie_weights(self, output_embeddings, input_embeddings):\n",
    "        output_embeddings.weight = input_embeddings.weight\n",
    "        output_embeddings.bias.data = nn.functional.pad(\n",
    "            output_embeddings.bias.data,\n",
    "            (\n",
    "                0,\n",
    "                output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
    "            ),\n",
    "            \"constant\",\n",
    "            0,\n",
    "        )\n",
    "        output_embeddings.out_features = input_embeddings.num_embeddings\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, -10000.0).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.word_embeddings.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.position_embeddings.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        device = input_ids.device\n",
    "        input_shape = input_ids.size()\n",
    "\n",
    "        mask = self._generate_square_subsequent_mask(self.block_size).to(device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_ids = torch.arange(0, input_shape[-1], dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        hidden_states = self.transformer_encoder(hidden_states, mask)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return lm_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then subclass the model to inherit from `PipelineMixin`, so that the model will have the `parallelize` and `deparallelize` methods. Here we override the `parallelize` method to customize the optimization. Note that if the model is small and no customized optimization is needed for the model, there is even no need to override `parallelize`. The optimizations we apply here and later are just for demonstration, so some of them are actually not necessary for such a relatively small model with `block_size` of 128.\n",
    "\n",
    "Another change we do here is to override the `forward` method. This is because an external model usually just returns logits, but we need to respect the return format of ðŸ¤— Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "from optimum.graphcore.modeling_utils import PipelineMixin, get_layer_ipu, recomputation_checkpoint, register\n",
    "from optimum.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class IPUTransformerModel(TransformerModel, PipelineMixin):\n",
    "    def parallelize(self):\n",
    "        super().parallelize()\n",
    "        logger.info(\"---------- Device Allocation -----------\")\n",
    "        logger.info(\"Embedding  --> IPU 0\")\n",
    "        self.word_embeddings = poptorch.BeginBlock(self.word_embeddings, \"word_embeddings\", ipu_id=0)\n",
    "        self.position_embeddings = poptorch.BeginBlock(self.position_embeddings, \"position_embeddings\", ipu_id=0)\n",
    "\n",
    "        layer_ipu = get_layer_ipu(self.ipu_config.layers_per_ipu)\n",
    "        for index, layer in enumerate(self.transformer_encoder.layers):\n",
    "            if self.ipu_config.recompute_checkpoint_every_layer:\n",
    "                # Put checkpoints on every encoder layer\n",
    "                h = recomputation_checkpoint(layer)\n",
    "                self._hooks.append(h)\n",
    "            ipu = layer_ipu[index]\n",
    "            logger.info(f\"Encoder {index:<2} --> IPU {ipu}\")\n",
    "            self.transformer_encoder.layers[index] = poptorch.BeginBlock(layer, f\"Encoder{index}\", ipu_id=ipu)\n",
    "\n",
    "        logger.info(f\"Head       --> IPU 0\")\n",
    "        logger.info(\"---------------------------------------\")\n",
    "        self.lm_head = poptorch.BeginBlock(self.lm_head, \"lm_head\", ipu_id=0)\n",
    "        return self\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        lm_logits = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n. Use roll() + ignore_index instead of slicing for better efficiency on IPUs.\n",
    "            labels = torch.roll(labels, -1, 1)\n",
    "            # By default the ignore_index of CrossEntropyLoss is -100\n",
    "            labels[:, -1] = -100\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        output = (lm_logits,)\n",
    "        return (loss,) if loss is not None else output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IPUTransformerModel(\n",
    "    block_size=block_size,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=768,\n",
    "    nhead=12,\n",
    "    dim_feedforward=768 * 4,\n",
    "    nlayers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEmeQ7Xm3l_H"
   },
   "source": [
    "To instantiate an `IPUTrainer`, we first define the `IPUConfig`, which is a class that specifies attributes and configuration parameters to compile and put the model on the device. We usually initialize it with one config name or a path to a JSON file. We could also initialize it from a dict as we are doing here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.graphcore import IPUConfig, IPUTrainer, IPUTrainingArguments\n",
    "\n",
    "# ipu_config = IPUConfig.from_pretrained(\"ipu_config.json\")\n",
    "ipu_config_dict = {\n",
    "    \"embedding_serialization_factor\": 2,                                                                                         \n",
    "    \"recompute_checkpoint_every_layer\": True,                                                                                    \n",
    "    \"optimizer_state_offchip\": True,\n",
    "    \"replicated_tensor_sharding\": True,\n",
    "    \"enable_half_first_order_momentum\": True,                                                                                    \n",
    "    \"enable_half_partials\": True,                                                                                                \n",
    "  \n",
    "    \"device_iterations\": 1,                                                                                                      \n",
    "    \"inference_device_iterations\": 5,\n",
    "    \"replication_factor\": {\"pod4\": 1, \"pod8\": 2, \"pod16\": 4, \"pod32\": 8, \"pod64\": 16, \"default\": 1},\n",
    "    \"inference_replication_factor\": {\"pod4\": 1, \"pod8\": 2, \"pod16\": 4, \"pod32\": 8, \"pod64\": 16, \"default\": 1},\n",
    "    \"gradient_accumulation_steps\": 512,\n",
    "    \"executable_cache_dir\": \"./exe_cache\",                                                                                       \n",
    "  \n",
    "    \"ipus_per_replica\": 4,                                                                                                       \n",
    "    \"layers_per_ipu\": [0, 4, 4, 4],                                                                                              \n",
    "    \"matmul_proportion\": [0.25, 0.25, 0.25, 0.25]\n",
    " }\n",
    "ipu_config = IPUConfig.from_dict(ipu_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing we need to define is the `IPUTrainingArguments`, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YbSwEhQ63l_L"
   },
   "outputs": [],
   "source": [
    "micro_batch_size = 1\n",
    "gradient_accumulation_steps = 64\n",
    "pod_type = \"pod16\"\n",
    "\n",
    "training_args = IPUTrainingArguments(\n",
    "    \"mymodel-wikitext2\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    per_device_eval_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    pod_type=pod_type,\n",
    "    num_train_epochs=10,\n",
    "    loss_scaling=16384,\n",
    "    warmup_ratio=0.1,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=64,\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZRbT9ui3l_N"
   },
   "source": [
    "Finally, we pass along all of those to the `IPUTrainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OEuqwIra3l_N",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding IPU config: gradient_accumulation_steps=64\n"
     ]
    }
   ],
   "source": [
    "trainer = IPUTrainer(\n",
    "    model=model,\n",
    "    ipu_config=ipu_config,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Vvz34Td3l_O"
   },
   "source": [
    "And we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NyZvu_MF3l_P",
    "outputId": "b69d0931-7f1f-4f2d-fdb8-09d37c7418bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "[14:23:11.784] [poptorch:cpp] [warning] Graph contains an unused input %2 : Long(1, 128, strides=[128, 1], requires_grad=0, device=cpu)\n",
      "[14:23:11.789] [poptorch:cpp] [warning] %2093 : Long(1, strides=[128], requires_grad=0, device=cpu) = aten::fill_(%2092, %908) # /tmp/ipykernel_1354260/2516398753.py:37:0: torch.int64 is not supported natively on IPU, loss of range/precision may occur. We will only warn on the first instance.\n",
      "Graph compilation:   0%|                                                                                                                                                                                     | 0/100 [00:00<?]2022-08-30T13:23:20.087469Z popart:devicex 1354260.1354260 W: The `debug.retainDebugInformation` engine option was implicitly set to `true`. The default will change to `false` in a future release. Set it to `true` explicitly if you want to query debug information (for example, by calling `Session::getReport`).\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:10<00:00]2022-08-30T13:23:29.278005Z popart:devicex 1354260.1354260 W: The `debug.retainDebugInformation` engine option was implicitly set to `true`. The default will change to `false` in a future release. Set it to `true` explicitly if you want to query debug information (for example, by calling `Session::getReport`).\n",
      "\n",
      "Compiled/Loaded model in 41.295423086732626 secs\n",
      "***** Running training *****\n",
      "  Num examples = 18666\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Device Iterations = 1\n",
      "  Replication Factor = 4\n",
      "  Gradient Accumulation steps = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Total optimization steps = 720\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjincheng\u001b[0m (\u001b[33msw-apps\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/localdata/jincheng/optimum-graphcore/notebooks/wandb/run-20220830_142346-2mfk28dz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.sourcevertex.net/sw-apps/huggingface/runs/2mfk28dz\" target=\"_blank\">mymodel-wikitext2</a></strong> to <a href=\"https://wandb.sourcevertex.net/sw-apps/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053d915beb854acaaeea40649ad6423f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.982, 'learning_rate': 2.7777777777777783e-06, 'epoch': 0.14}\n",
      "{'loss': 11.1641, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.28}\n",
      "{'loss': 10.3117, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.42}\n",
      "{'loss': 9.8141, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.56}\n",
      "{'loss': 9.3187, 'learning_rate': 1.388888888888889e-05, 'epoch': 0.69}\n",
      "{'loss': 9.2125, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.83}\n",
      "{'loss': 9.0023, 'learning_rate': 1.9444444444444445e-05, 'epoch': 0.97}\n",
      "{'loss': 8.8445, 'learning_rate': 1.9753086419753087e-05, 'epoch': 1.11}\n",
      "{'loss': 8.7188, 'learning_rate': 1.9444444444444445e-05, 'epoch': 1.25}\n",
      "{'loss': 8.5695, 'learning_rate': 1.9135802469135804e-05, 'epoch': 1.39}\n",
      "{'loss': 8.4547, 'learning_rate': 1.8827160493827163e-05, 'epoch': 1.53}\n",
      "{'loss': 8.3477, 'learning_rate': 1.851851851851852e-05, 'epoch': 1.67}\n",
      "{'loss': 8.3656, 'learning_rate': 1.820987654320988e-05, 'epoch': 1.81}\n",
      "{'loss': 8.2984, 'learning_rate': 1.7901234567901236e-05, 'epoch': 1.94}\n",
      "{'loss': 8.0422, 'learning_rate': 1.7592592592592595e-05, 'epoch': 2.08}\n",
      "{'loss': 7.9789, 'learning_rate': 1.728395061728395e-05, 'epoch': 2.22}\n",
      "{'loss': 7.9387, 'learning_rate': 1.697530864197531e-05, 'epoch': 2.36}\n",
      "{'loss': 7.8363, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.5}\n",
      "{'loss': 7.7453, 'learning_rate': 1.6358024691358026e-05, 'epoch': 2.64}\n",
      "{'loss': 7.6473, 'learning_rate': 1.6049382716049385e-05, 'epoch': 2.78}\n",
      "{'loss': 7.7379, 'learning_rate': 1.5740740740740744e-05, 'epoch': 2.92}\n",
      "{'loss': 7.5656, 'learning_rate': 1.54320987654321e-05, 'epoch': 3.06}\n",
      "{'loss': 7.4281, 'learning_rate': 1.5123456790123458e-05, 'epoch': 3.19}\n",
      "{'loss': 7.4172, 'learning_rate': 1.4814814814814815e-05, 'epoch': 3.33}\n",
      "{'loss': 7.5273, 'learning_rate': 1.4506172839506174e-05, 'epoch': 3.47}\n",
      "{'loss': 7.498, 'learning_rate': 1.4197530864197532e-05, 'epoch': 3.61}\n",
      "{'loss': 7.2391, 'learning_rate': 1.388888888888889e-05, 'epoch': 3.75}\n",
      "{'loss': 7.4004, 'learning_rate': 1.3580246913580248e-05, 'epoch': 3.89}\n",
      "{'loss': 7.3559, 'learning_rate': 1.3271604938271605e-05, 'epoch': 4.03}\n",
      "{'loss': 7.1488, 'learning_rate': 1.2962962962962964e-05, 'epoch': 4.17}\n",
      "{'loss': 7.4223, 'learning_rate': 1.2654320987654323e-05, 'epoch': 4.31}\n",
      "{'loss': 7.1734, 'learning_rate': 1.234567901234568e-05, 'epoch': 4.44}\n",
      "{'loss': 7.1344, 'learning_rate': 1.2037037037037039e-05, 'epoch': 4.58}\n",
      "{'loss': 7.1863, 'learning_rate': 1.1728395061728398e-05, 'epoch': 4.72}\n",
      "{'loss': 7.168, 'learning_rate': 1.1419753086419753e-05, 'epoch': 4.86}\n",
      "{'loss': 7.2223, 'learning_rate': 1.1111111111111113e-05, 'epoch': 5.0}\n",
      "{'loss': 7.177, 'learning_rate': 1.0802469135802469e-05, 'epoch': 5.14}\n",
      "{'loss': 7.2199, 'learning_rate': 1.0493827160493827e-05, 'epoch': 5.28}\n",
      "{'loss': 7.1207, 'learning_rate': 1.0185185185185186e-05, 'epoch': 5.42}\n",
      "{'loss': 7.2059, 'learning_rate': 9.876543209876543e-06, 'epoch': 5.56}\n",
      "{'loss': 7.2367, 'learning_rate': 9.567901234567902e-06, 'epoch': 5.69}\n",
      "{'loss': 7.0414, 'learning_rate': 9.25925925925926e-06, 'epoch': 5.83}\n",
      "{'loss': 7.0773, 'learning_rate': 8.950617283950618e-06, 'epoch': 5.97}\n",
      "{'loss': 7.1223, 'learning_rate': 8.641975308641975e-06, 'epoch': 6.11}\n",
      "{'loss': 7.0941, 'learning_rate': 8.333333333333334e-06, 'epoch': 6.25}\n",
      "{'loss': 7.1629, 'learning_rate': 8.024691358024692e-06, 'epoch': 6.39}\n",
      "{'loss': 7.1328, 'learning_rate': 7.71604938271605e-06, 'epoch': 6.53}\n",
      "{'loss': 7.0012, 'learning_rate': 7.4074074074074075e-06, 'epoch': 6.67}\n",
      "{'loss': 7.1191, 'learning_rate': 7.098765432098766e-06, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mymodel-wikitext2/checkpoint-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.948, 'learning_rate': 6.790123456790124e-06, 'epoch': 6.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Configuration saved in mymodel-wikitext2/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0687, 'learning_rate': 6.481481481481482e-06, 'epoch': 7.08}\n",
      "{'loss': 7.0906, 'learning_rate': 6.17283950617284e-06, 'epoch': 7.22}\n",
      "{'loss': 6.9422, 'learning_rate': 5.864197530864199e-06, 'epoch': 7.36}\n",
      "{'loss': 6.9574, 'learning_rate': 5.555555555555557e-06, 'epoch': 7.5}\n",
      "{'loss': 7.1527, 'learning_rate': 5.246913580246914e-06, 'epoch': 7.64}\n",
      "{'loss': 7.0496, 'learning_rate': 4.938271604938272e-06, 'epoch': 7.78}\n",
      "{'loss': 7.1391, 'learning_rate': 4.62962962962963e-06, 'epoch': 7.92}\n",
      "{'loss': 6.9145, 'learning_rate': 4.3209876543209875e-06, 'epoch': 8.06}\n",
      "{'loss': 7.0172, 'learning_rate': 4.012345679012346e-06, 'epoch': 8.19}\n",
      "{'loss': 6.9281, 'learning_rate': 3.7037037037037037e-06, 'epoch': 8.33}\n",
      "{'loss': 7.0871, 'learning_rate': 3.395061728395062e-06, 'epoch': 8.47}\n",
      "{'loss': 7.0355, 'learning_rate': 3.08641975308642e-06, 'epoch': 8.61}\n",
      "{'loss': 7.1281, 'learning_rate': 2.7777777777777783e-06, 'epoch': 8.75}\n",
      "{'loss': 6.8859, 'learning_rate': 2.469135802469136e-06, 'epoch': 8.89}\n",
      "{'loss': 6.909, 'learning_rate': 2.1604938271604937e-06, 'epoch': 9.03}\n",
      "{'loss': 7.0398, 'learning_rate': 1.8518518518518519e-06, 'epoch': 9.17}\n",
      "{'loss': 6.9648, 'learning_rate': 1.54320987654321e-06, 'epoch': 9.31}\n",
      "{'loss': 7.0578, 'learning_rate': 1.234567901234568e-06, 'epoch': 9.44}\n",
      "{'loss': 7.052, 'learning_rate': 9.259259259259259e-07, 'epoch': 9.58}\n",
      "{'loss': 7.0355, 'learning_rate': 6.17283950617284e-07, 'epoch': 9.72}\n",
      "{'loss': 6.973, 'learning_rate': 3.08641975308642e-07, 'epoch': 9.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.932, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 193.6912, 'train_samples_per_second': 951.618, 'train_steps_per_second': 3.717, 'train_loss': 7.6283148871527775, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=720, training_loss=7.6283148871527775, metrics={'train_runtime': 193.6912, 'train_samples_per_second': 951.618, 'train_steps_per_second': 3.717, 'train_loss': 7.6283148871527775, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3APq-vUc3l_R"
   },
   "source": [
    "Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "diKZnB1I3l_R",
    "outputId": "9b3ac725-0117-4830-f380-a555ee57c8cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "[14:27:23.384] [poptorch:cpp] [warning] Graph contains an unused input %attention_mask : Long(1, 128, strides=[128, 1], requires_grad=0, device=cpu)\n",
      "Graph compilation:   0%|                                                                                                                                                                                     | 0/100 [00:00<?]2022-08-30T13:27:31.187763Z popart:devicex 1354260.1354260 W: The `debug.retainDebugInformation` engine option was implicitly set to `true`. The default will change to `false` in a future release. Set it to `true` explicitly if you want to query debug information (for example, by calling `Session::getReport`).\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00]2022-08-30T13:27:34.431783Z popart:devicex 1354260.1354260 W: The `debug.retainDebugInformation` engine option was implicitly set to `true`. The default will change to `false` in a future release. Set it to `true` explicitly if you want to query debug information (for example, by calling `Session::getReport`).\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00]\n",
      "Compiled/Loaded model in 28.401313013397157 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1931\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fe4cc04b8248cd93d956f2237b1e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1083.86\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity is still quite high since for this demo we trained on a small dataset for a small number of epochs. For real LM training, you  would need a larger dataset and more epochs.\n",
    "\n",
    "If you want to resume training from a checkpoint, you could do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from mymodel-wikitext2/checkpoint-500).\n",
      "***** Running training *****\n",
      "  Num examples = 18666\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Device Iterations = 1\n",
      "  Replication Factor = 4\n",
      "  Gradient Accumulation steps = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Total optimization steps = 720\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 6\n",
      "  Continuing training from global step 500\n",
      "  Will skip the first 6 epochs then the first 68 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53605f8f6fa4d13aafb6a047855303c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0582, 'learning_rate': 6.481481481481482e-06, 'epoch': 7.08}\n",
      "{'loss': 7.0871, 'learning_rate': 6.17283950617284e-06, 'epoch': 7.22}\n",
      "{'loss': 6.9402, 'learning_rate': 5.864197530864199e-06, 'epoch': 7.36}\n",
      "{'loss': 6.968, 'learning_rate': 5.555555555555557e-06, 'epoch': 7.5}\n",
      "{'loss': 7.1574, 'learning_rate': 5.246913580246914e-06, 'epoch': 7.64}\n",
      "{'loss': 7.0508, 'learning_rate': 4.938271604938272e-06, 'epoch': 7.78}\n",
      "{'loss': 7.1469, 'learning_rate': 4.62962962962963e-06, 'epoch': 7.92}\n",
      "{'loss': 6.9172, 'learning_rate': 4.3209876543209875e-06, 'epoch': 8.06}\n",
      "{'loss': 7.023, 'learning_rate': 4.012345679012346e-06, 'epoch': 8.19}\n",
      "{'loss': 6.9328, 'learning_rate': 3.7037037037037037e-06, 'epoch': 8.33}\n",
      "{'loss': 7.0883, 'learning_rate': 3.395061728395062e-06, 'epoch': 8.47}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint='mymodel-wikitext2/checkpoint-500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Train a language model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
