{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan-T5 is (probably) all you need\n",
    "\n",
    "We believe that GPT-3 scale models are not necessary to perform many NLP workloads in production today. Many of these workloads simply do not need the impressive generative capability that these models offer. Here are a few of the issues with using (sometimes) excessive models:\n",
    "- Costs your business more, because these models must be executed on additional GPUs which is reflected in the bill from your cloud compute provider\n",
    "- The machines required to run these models A100 (H100) are in short supply and high demand, increasing the cost further\n",
    "- Has a greater environmental impact due to the high power requirements to both operate and cool these additional GPUs\n",
    "\n",
    "In December 2022 Google published [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) in which they perform extensive finetuning on a broad collection of tasks on a variety of models (PaLM, T5, U-PaLM). Part of this publication was the release of Flan-T5 checkpoints, \"which achieve strong few-shot performance even compared to much larger models\".\n",
    "\n",
    "In this notebook we will demonstrate how you can use Flan-T5 on readily available IPU-POD4s (Flan-T5-L) and IPU-POD16s (Flan-T5-XL) for common NLP workloads. We shall do this by considering the following questions:\n",
    "- How good is Flan-T5, really?\n",
    "- How do I run Flan-T5 on IPUs?\n",
    "- What can I use Flan-T5 for?\n",
    "- Why would I pay extra for Flan-T5-XL?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good is Flan-T5, really?\n",
    "\n",
    "Let's start by looking at some numbers from the paper:\n",
    "\n",
    "<img src=\"images/t5_vs_flan_t5.png\" style=\"width: 640px;\"/>\n",
    "\n",
    "> Part of table 5 from [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)\n",
    "\n",
    "These results are astounding, notice that:\n",
    "- Flan-T5 performs ~2x better than T5 in MMLU, BBH & MGSM\n",
    "- In TyDiQA we even see the emergence of new abilities\n",
    "- Flan-T5-Large is better than all previous variants of T5 (even XXL)\n",
    "\n",
    "This establishes Flan-T5 as an entirely different beast to the T5 you may know. Now let's see how Flan-T5 compares to other models in the MMLU benchmark:\n",
    "\n",
    "| Rank | Model | Average (%) | Parameters (Billions) |\n",
    "|------|-------|-------------|-----------------------|\n",
    "| 22 | GPT-3 (finetuned) | 53.9 | 175 |\n",
    "| 23 | GAL 120B (zero-shot) | 52.6 | 120 |\n",
    "| 24 | Flan-T5-XL | 52.4 | 3 |\n",
    "| 25 | Flan-PaLM 8B | 49.3 | 8 |\n",
    "| 30 | Flan-T5-XL (CoT) | 45.5 | 3 |\n",
    "| 31 | Flan-T5-L | 45.1 | 0.8 |\n",
    "| 33 | GPT-3 (few-shot, k=5) | 43.9 | 175 |\n",
    "| 35 | Flan-PaLM 8B (CoT) | 41.3 | 8 |\n",
    "| 36 | Flan-T5-L (CoT) | 40.5 | 0.8 |\n",
    "| 38 | LLaMA 7B (few-shot, k=5) | 35.1 | 7 |\n",
    "\n",
    "> Part of the MMLU leaderboard from [Papers With Code](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) (CoT = Chain of Thought)\n",
    "\n",
    "This table shows that:\n",
    "- Flan-T5-L/XL (0.8B and 3B respectively) is punching orders of magnitute above its weight class appearing alongside titans like GPT-3 (175B), Galactica (120B)\n",
    "- GPT-3 must be finetuned for MMLU to rank at the top of this comparison, but this benchmark was held out for evaluation with Flan-T5\n",
    "- Flan-T5 also outperforms smaller versions of more recent LLMs like PaLM and LLaMA (while also being multiple times smaller)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I run Flan-T5 on IPUs?\n",
    "\n",
    "Since the Flan-T5 checkpoints are available on HuggingFace, you can use Graphcore's HuggingFace integration ([ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore)) to easily run Flan-T5 with standard inference pipeline.\n",
    "\n",
    "If you already have an existing HuggingFace based application that you'd like to try on IPUs, then it's as simple as:\n",
    "\n",
    "```diff\n",
    "->>> from transformers import pipeline\n",
    "+>>> from optimum.graphcore import pipeline\n",
    "\n",
    "->>> text_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "+>>> text_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", ipu_config=\"Graphcore/t5-large-ipu\")\n",
    ">>> text_generator(\"Please solve the following equation: x^2 - 9 = 0\")\n",
    "[{'generated_text': '3'}]\n",
    "```\n",
    "\n",
    "Now let's define a text generator of our own to use in the rest of this notebook. First, let's make sure your environment has the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore) available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optimum-graphcore>=0.6.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location of the cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "executable_cache_dir=os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"./exe_cache/\")\n",
    "num_available_ipus=int(os.getenv(\"NUM_AVAILABLE_IPU\") or os.getenv(\"GRAPHCORE_POD_TYPE\", \"pod\")[3:] or 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the `pipeline` from `optimum.graphcore` and create our Flan-T5 pipeline for the appropriate number of IPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/harrym/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from optimum.graphcore import pipeline\n",
    "\n",
    "size = {4: \"large\", 16: \"xl\"}\n",
    "# TODO remove when configs are uploaded to HF\n",
    "cfg = {\n",
    "    \"large\": {\n",
    "            \"layers_per_ipu\": [12, 12, 12, 12],\n",
    "            \"matmul_proportion\": [0.05, 0.05, 0.2, 0.2],\n",
    "    },\n",
    "    \"xl\": {\n",
    "            \"layers_per_ipu\": [2, 4, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3],\n",
    "            \"matmul_proportion\": [0.1, 0.05, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.05, 0.05, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],  # fmt: skip\n",
    "    },\n",
    "}\n",
    "from optimum.graphcore import IPUConfig\n",
    "ipu_config = IPUConfig(\n",
    "    layers_per_ipu=cfg[size[num_available_ipus]][\"layers_per_ipu\"],\n",
    "    matmul_proportion=cfg[size[num_available_ipus]][\"matmul_proportion\"],\n",
    "    executable_cache_dir=executable_cache_dir,\n",
    "    # replication_factor=n_ipus // len(cfg[size][\"layers_per_ipu\"]),\n",
    ")\n",
    "# end TODO\n",
    "flan_t5 = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=f\"google/flan-t5-{size[num_available_ipus]}\",\n",
    "    max_input_length=896,\n",
    "    ipu_config=ipu_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just for fun, let's ask it some random questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:20<00:00]\n",
      "Graph compilation:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [01:15<00:44]"
     ]
    }
   ],
   "source": [
    "\n",
    "questions = [\n",
    "    \"Solve the following equation for x: x^2 - 9 = 0\",\n",
    "    \"At what temperature does nitrogen freeze?\",\n",
    "    \"In order to reduce symptoms of asthma such as tightness in the chest, wheezing, and difficulty breathing, what do you recommend?\",\n",
    "    \"Which country is home to the tallest mountainin the world?\"\n",
    "]\n",
    "for out in flan_t5(questions):\n",
    "    print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can I use Flan-T5 for?\n",
    "\n",
    "As we mentoined earlier, Flan-T5 has been finetuned on hundreds of different tasks. So no matter what your task might be, it's worth seeing if Flan-T5 can meet your requirements. Here we will demonstrate at a few common ones:\n",
    "\n",
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis = (\n",
    "    \"Review: It gets too hot, the battery only can last 4 hours. Sentiment: Negative\\n\"\n",
    "    \"Review: Nice looking phone. Sentiment: Positive\\n\"\n",
    "    \"Review: Sometimes it freezes and you have to close all the open pages and then reopen where you were. Sentiment: Negative\\n\"\n",
    "    \"Review: Wasn't that impressed, went back to my old phone. Sentiment:\"\n",
    ")\n",
    "flan_t5(sentiment_analysis)[0][\"generated_text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Company]: Apple, [Founded]: 1976, [Founders]: Steve Jobs, Steve Wozniak\n"
     ]
    }
   ],
   "source": [
    "advanced_ner = \"\"\"Microsoft Corporation is a company that makes computer software and video games. Bill Gates and Paul Allen founded the company in 1975\n",
    "[Company]: Microsoft, [Founded]: 1975, [Founders]: Bill Gates, Paul Allen\n",
    "\n",
    "Amazon.com, Inc., known as Amazon , is an American online business and cloud computing company. It was founded on July 5, 1994 by Jeff Bezos\n",
    "[Company]: Amazon, [Founded]: 1994, [Founders]: Jeff Bezos\n",
    "\n",
    "Apple Inc. is a multinational company that makes personal computers, mobile devices, and software. Apple was started in 1976 by Steve Jobs and Steve Wozniak.\"\"\"\n",
    "print(flan_t5(advanced_ner)[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denver Broncos\n"
     ]
    }
   ],
   "source": [
    "context = 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24-10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'\n",
    "question = \"Which NFL team represented the AFC at Super Bowl 50?\"\n",
    "response = flan_t5(f\"{context} {question}\")\n",
    "# The correct answer is Denver Broncos\n",
    "print(response[0]['generated_text'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file documents\n"
     ]
    }
   ],
   "source": [
    "intent_classification = \"\"\"[Text]: I really need to get a gym membership, I'm exhausted.\n",
    "[Intent]: get gym membership\n",
    "\n",
    "[Text]: What do I need to make a carbonara?\n",
    "[Intent]: cook carbonara\n",
    "\n",
    "[Text]: I need all these documents sorted and filed by Monday.\n",
    "[Intent]:\"\"\"\n",
    "print(flan_t5([intent_classification])[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man has been arrested after a firearm was found in a property in Edinburgh.\n"
     ]
    }
   ],
   "source": [
    "summarization=\"\"\"\n",
    "Document: Firstsource Solutions said new staff will be based at its Cardiff Bay site which already employs about 800 people.\n",
    "The 300 new jobs include sales and customer service roles working in both inbound and outbound departments.\n",
    "The company's sales vice president Kathryn Chivers said: \"Firstsource Solutions is delighted to be able to continue to bring new employment to Cardiff.\"\n",
    "Summary: Hundreds of new jobs have been announced for a Cardiff call centre.\n",
    "\n",
    "Document: The visitors raced into a three-goal first-half lead at Hampden.\n",
    "Weatherson opened the scoring with an unstoppable 15th-minute free-kick, and he made it 2-0 in the 27th minute.\n",
    "Matt Flynn made it 3-0 six minutes later with a fine finish.\n",
    "Queen's pulled a consolation goal back in stoppage time through John Carter.\n",
    "Summary: Peter Weatherson netted a brace as Annan recorded only their second win in eight matches.\n",
    "\n",
    "Document: Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday.\n",
    "Detectives said three firearms, ammunition and a five-figure sum of money were recovered.\n",
    "A 26-year-old man who was arrested and charged appeared at Edinburgh Sheriff Court on Thursday.\n",
    "Summary:\n",
    "\"\"\"\n",
    "print(flan_t5(summarization)[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "document, mean pooling, mean pooling\n"
     ]
    }
   ],
   "source": [
    "text_classification_1 = \"\"\"Message: When the spaceship landed on Mars, the whole humanity was excited\n",
    "[Topic]: space\n",
    "\n",
    "Message: I love playing tennis and golf. I'm practicing twice a week.\n",
    "[Topic]: sport\n",
    "\n",
    "Message: Managing a team of sales people is a tough but rewarding job.\n",
    "[Topic]: business\n",
    "\n",
    "Message: I am trying to cook chicken with tomatoes.\n",
    "[Topic]:\"\"\"\n",
    "print(flan_t5(text_classification_1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_classification_2 = \"\"\"Information Retrieval (IR) is the process of obtaining resources relevant to the information need. For instance, a search query on a web search engine can be an information need. The search engine can return web pages that represent relevant resources.\n",
    "Keywords: information, search, resources\n",
    "\n",
    "David Robinson has been in Arizona for the last three months searching for his 24-year-old son, Daniel Robinson, who went missing after leaving a work site in the desert in his Jeep Renegade on June 23. \n",
    "Keywords: searching, missing, desert\n",
    "\n",
    "I believe that using a document about a topic that the readers know quite a bit about helps you understand if the resulting keyphrases are of quality.\n",
    "Keywords: document, understand, keyphrases\n",
    "\n",
    "Since transformer models have a token limit, you might run into some errors when inputting large documents. In that case, you could consider splitting up your document into paragraphs and mean pooling (taking the average of) the resulting vectors.\n",
    "Keywords:\"\"\"\n",
    "\n",
    "print(flan_t5(text_classification_2)[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why would I pay extra for Flan-T5-XL?\n",
    "\n",
    "As we saw earlier when looking at the results from the paper, Flan-T5-XL is roughly 40% better than Flan-T5-Large on average across its validation tasks. Therefore when deciding if Flan-T5-XL is worth it for you, ask yourself the following questions:\n",
    "- Does my data need greater linguistic understanding for the task to be performed?\n",
    "- Is my task too complicated for a model as small as Flan-T5-Large and too easy for a model as large as GPT-3?\n",
    "- Does my task require longer output sequences that Flan-T5-XL is needed to generate?\n",
    "\n",
    "For fun, let's now look at an example of a task where the answer to all of the above questions is yes. Let's say you have a customer service AI that you use to answer basic questions in order to reduce the workload of your customer service personnel. This needs:\n",
    "- Strong linguistic ability to both parse and generate meduim sized chunks of text\n",
    "- An LLM that is able to learn well from context, but doesn't have all of human history embedded in its parameters\n",
    "- The ability to produce multiple sentence responses, but not much longer than this\n",
    "\n",
    "Looking at the code below, we see some context about Graphcore provided in the input, as well as primer for a conversational response from the model. As you can see from the example conversation (read before executing the code block), Flan-T5-XL was able to understand the information provided in the context and provide useful and natural answers to the questions it was asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting session\n",
      "[Customer]: What is an IPU?\n",
      "[Virtual Assistant]: The Intelligence Processing Unit (IPU) is a computer chip that is used to process artificial intelligence.\n",
      "[Customer]: Who makes it?\n",
      "[Virtual Assistant]: Graphcore is the manufacturer of the IPU.\n",
      "[Customer]: Can I use them?\n",
      "[Virtual Assistant]: Yes, I'm sure you can.\n",
      "[Customer]: Where?\n",
      "[Virtual Assistant]: The IPU is available on Paperspace, Gcore and Graphcloud.\n",
      "Ending session\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, model, context) -> None:\n",
    "        self.model = model\n",
    "        self.initial_context = context\n",
    "        self.context = self.initial_context\n",
    "        self.user, self.persona = [x.split(\":\")[0] for x in context.split(\"\\n\")[-2:]]\n",
    "\n",
    "    def ask(self, question):\n",
    "        question += \".\" if question[-1] not in [\".\", \"?\", \"!\"] else \"\"\n",
    "        x = f\"{self.context}\\n{self.user}: {question}\\n{self.persona}: \"\n",
    "        # print(f\"\\n{x}\\n\")\n",
    "        y = self.model(x)\n",
    "        response = y[0][\"generated_text\"]\n",
    "        self.context = f\"{x}{response}\"\n",
    "        return response\n",
    "\n",
    "    def session(self):\n",
    "        print(\"Starting session\", flush=True)\n",
    "        prompt = input()\n",
    "        while prompt != \"\":\n",
    "            if prompt == \"reset\":\n",
    "                clear_output()\n",
    "                print(\"Starting session\", flush=True)\n",
    "                self.context = self.initial_context\n",
    "                prompt = input()\n",
    "            print(f\"{self.user.title()}: {prompt}\", flush=True)\n",
    "            answer = self.ask(prompt)\n",
    "            print(f\"{self.persona.title()}: {answer}\", flush=True)\n",
    "            prompt = input()\n",
    "        print(\"Ending session\", flush=True)\n",
    "\n",
    "\n",
    "context = f\"\"\"This is a conversation between a [customer] and a [virtual assistant].\n",
    "The [virtual assistant] works at Graphcore. Graphcore is:\n",
    "- Located in Bristol.\n",
    "- Invented the Intelligence Processing Unit (IPU). It is purpose built for AI.\n",
    "- The currently available models are: Classic IPU, Bow IPU, C600\n",
    "- IPUs are available on: Paperspace, Gcore and Graphcloud\n",
    "\n",
    "[virtual assistant]: Hello, welcome to Graphcore, how can I help you today?\n",
    "[customer]: I'd like to ask some questions about your company.\n",
    "[virtual assistant]: Ok, I can help you with that.\"\"\"\n",
    "chatbot = ChatBot(flan_t5, context)\n",
    "chatbot.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5.model.detachFromDevice()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.2.0+1277_poptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
