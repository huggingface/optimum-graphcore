{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e98a61",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n",
    "\n",
    "# Text Generation on IPUs using mT5 - Inference\n",
    "\n",
    "[mT5](https://huggingface.co/docs/transformers/model_doc/mt5) is the multilingual variant of [T5](https://huggingface.co/docs/transformers/model_doc/t5), an encoder-decoder transformer model. T5 leverages a unified text-to-text format that allows it to represent natural language problems such as summarization, question answering and text classification in the same way. For example, for the task of machine translation, the source language text input to the encoder can be represented as \"Translate X to Y: token1 token2 token3..\" where the translation prefix specifies the source and target languages. The T5 output from the decoder is then the translated text in the target language. Similarly, input text for text classification can be prefixed as \"Classify: token1 token2 ..\", and the output text is a string with token(s) corresponding to the classified text. The advantage of this representation is that a single model can target different tasks simply by using an appropriate task prefix with the input text. However, pre-trained checkpoints for T5 primarily support only English as the source text for any natural language task, so the majority of problems that T5 can solve are English to English tasks. \n",
    "\n",
    "mT5 uses the same architecture as T5 but the encoder is pre-trained on a dataset covering 101 languages. However, unlike T5, mT5's pre-training does not include any supervised training so mT5 pre-trained checkpoints have limited immediate use. To be performant on any downstream task like machine translation or sentiment analysis, mT5 needs to be fine-tuned.\n",
    "\n",
    "This notebook shows how to easily use fine-tuned checkpoints for mT5 that target tasks such as language identification, translation and summarisation using the Graphcore IPU for **inference**. \n",
    "\n",
    "Note: No fine-tuning is performed in this notebook. Run the [fine-tuning-mT5](fine-tuning-mt5.ipynb) notebook to learn more about how to fine-tune mT5 for different tasks that use different languages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "455d4b0f",
   "metadata": {},
   "source": [
    "|  Domain | Tasks | Model | Datasets | Workflow |   Number of IPUs   | Execution time |\n",
    "|---------|-------|-------|----------|----------|--------------|--------------|\n",
    "|   NLP   |  Text summarisation | mT5 | XL-Sum | Inference| recommended: 4 | 20Xmn (X1h20mn)   |\n",
    "\n",
    "[![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb3b35",
   "metadata": {},
   "source": [
    "\n",
    "## Environment setup\n",
    "\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
    "\n",
    "[![Run on Gradient](images/gradient-badge.svg)](https://console.paperspace.com/github/<runtime-repo>?machine=Free-IPU-POD4&container=<dockerhub-image>&file=<path-to-file-in-repo>)  \n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to do this. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeb3c3e7",
   "metadata": {},
   "source": [
    "## Dependencies and configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae2b0da6",
   "metadata": {},
   "source": [
    "Install the dependencies the notebook needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bfdfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mwizak%40graphcore.ai:****@artifactory.sourcevertex.net:443/api/pypi/pypi-virtual/simple, https://pypi.python.org/simple/\n",
      "Processing /nethome/mwizak/optimum-graphcore-fork\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: rouge in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: nltk in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (3.8.1)\n",
      "Collecting torch@ https://download.pytorch.org/whl/cpu/torch-1.13.1%2Bcpu-cp38-cp38-linux_x86_64.whl\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-1.13.1%2Bcpu-cp38-cp38-linux_x86_64.whl (199.1 MB)\n",
      "Requirement already satisfied: datasets in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.6.0.dev0) (2.11.0)\n",
      "Requirement already satisfied: optimum==1.6.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.6.0.dev0) (1.6.1)\n",
      "Requirement already satisfied: scipy in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.6.0.dev0) (1.10.1)\n",
      "Requirement already satisfied: diffusers[torch]==0.12.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.6.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: tokenizers in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.6.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: transformers==4.25.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.6.0.dev0) (4.25.1)\n",
      "Requirement already satisfied: pillow in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.6.0.dev0) (9.5.0)\n",
      "Requirement already satisfied: sentencepiece in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.6.0.dev0) (0.1.97)\n",
      "Requirement already satisfied: requests in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (2023.3.23)\n",
      "Requirement already satisfied: numpy in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: importlib-metadata in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (6.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: filelock in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (3.10.7)\n",
      "Requirement already satisfied: accelerate>=0.11.0 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: coloredlogs in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum==1.6.1->optimum-graphcore==0.6.0.dev0) (15.0.1)\n",
      "Requirement already satisfied: sympy in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum==1.6.1->optimum-graphcore==0.6.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: packaging in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from optimum==1.6.1->optimum-graphcore==0.6.0.dev0) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: six in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: click in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from torch@ https://download.pytorch.org/whl/cpu/torch-1.13.1%2Bcpu-cp38-cp38-linux_x86_64.whl->optimum-graphcore==0.6.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.6.0.dev0) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.6.0.dev0) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.6.0.dev0) (3.8.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.6.0.dev0) (2023.3.0)\n",
      "Requirement already satisfied: multiprocess in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.6.0.dev0) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.6.0.dev0) (0.18.0)\n",
      "Requirement already satisfied: pandas in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.6.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.6.0.dev0) (11.0.0)\n",
      "Requirement already satisfied: psutil in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from accelerate>=0.11.0->diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (5.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (4.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (1.26.15)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (3.20.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from coloredlogs->optimum==1.6.1->optimum-graphcore==0.6.0.dev0) (10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from importlib-metadata->diffusers[torch]==0.12.1->optimum-graphcore==0.6.0.dev0) (3.15.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.6.0.dev0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.6.0.dev0) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.6.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages (from sympy->optimum==1.6.1->optimum-graphcore==0.6.0.dev0) (1.3.0)\n",
      "Building wheels for collected packages: optimum-graphcore\n",
      "  Building wheel for optimum-graphcore (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-graphcore: filename=optimum_graphcore-0.6.0.dev0-py3-none-any.whl size=193338 sha256=d4193cf03ef71daedc1032a06aeb8029b6a80eaae3d38f9952b54330dd2d5544\n",
      "  Stored in directory: /nethome/mwizak/.cache/pip/wheels/7a/fd/c0/7273911a88f48c4195b1d61febe5c3f2cb51901875a0cdb05a\n",
      "Successfully built optimum-graphcore\n",
      "Installing collected packages: optimum-graphcore\n",
      "  Attempting uninstall: optimum-graphcore\n",
      "    Found existing installation: optimum-graphcore 0.6.0.dev0\n",
      "    Uninstalling optimum-graphcore-0.6.0.dev0:\n",
      "      Successfully uninstalled optimum-graphcore-0.6.0.dev0\n",
      "Successfully installed optimum-graphcore-0.6.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install ../optimum-graphcore-fork/ rouge nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb9bff",
   "metadata": {},
   "source": [
    "To enable cached execution with this notebook, a path to a directory to store cached executables can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0462d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "executable_cache_dir=os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd56c834",
   "metadata": {},
   "source": [
    "## Running mT5 inference on the IPU\n",
    "\n",
    "This notebook is built to run with most mT5 model checkpoints from the [🤗 Model Hub](https://huggingface.co/models?search=mt5). [mT5](https://huggingface.co/docs/transformers/model_doc/mt5) has a selection of officially published model sizes ranging from small to XXL, where the size indicates the number of parameters in the model. The larger mT5 configurations offer a greater capacity of natural language understanding and can better support multiple tasks with a single model at the expense of requiring more memory and compute resource. However, smaller variants such as `mt5-small` and `mt5-base` still offer good language understanding.  \n",
    "\n",
    "For inference, `mt5-small` and `mt5-base` are able to run using 4 IPUs. Larger mT5 configurations will need more IPUs due to memory constraints. To start off, we specify a `mt5-base` checkpoint fine-tuned on [44 languages](https://huggingface.co/datasets/csebuetnlp/xlsum) for the task of text summarisation. Refer to the research article [XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages](https://aclanthology.org/2021.findings-acl.413/) for more information on the dataset. \n",
    "\n",
    "We define the model checkpoint so that we can use the 🤗 Transformers `pipeline` API to load the checkpoint from the 🤗 Hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe7a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"csebuetnlp/mT5_multilingual_XLSum\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22866f1b",
   "metadata": {},
   "source": [
    "In order to run on the IPU, an IPU configuration (see [here]() for parameter documentation) is required that specifies attributes and configuration parameters to compile and put the model on the device. \n",
    "\n",
    "`mt5-base` has 12 encoder layers and 12 decoder layers. The first IPU is reserved for the embedding layer and the remaining IPUs are reserved for the encoder and decoder layers as well as the language modelling head. The decoder layers are sharded on the last 2 IPUs due to memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef28060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.graphcore import IPUConfig\n",
    "\n",
    "ipu_config = IPUConfig(\n",
    "    inference_layers_per_ipu=[0, 12, 8, 4],\n",
    "    executable_cache_dir=executable_cache_dir,\n",
    "    embedding_serialization_factor=2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8aa5b251",
   "metadata": {},
   "source": [
    "Lastly, all we need to do is create a pipeline object by specifying the task, fine-tuned checkpoint and the IPU configuration. The task is set to `text2text-generation` due to the text-to-text format for T5 detailed previously. Below we use the `pipeline` interface from [🤗 Optimum Graphcore](https://github.com/huggingface/optimum-graphcore) which is similar to the 🤗 Transformers [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines) but integrates support for the IPU. For more information on pipelines, please see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec72646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/mwizak/venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from optimum.graphcore import pipeline\n",
    "pipe = pipeline(task=\"text2text-generation\", model=model_checkpoint, ipu_config=ipu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d634b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5Config {\n",
       "  \"_name_or_path\": \"csebuetnlp/mT5_multilingual_XLSum\",\n",
       "  \"architectures\": [\n",
       "    \"MT5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"gelu_new\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"length_penalty\": 0.6,\n",
       "  \"max_length\": 84,\n",
       "  \"model_type\": \"mt5\",\n",
       "  \"no_repeat_ngram_size\": 2,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"T5Tokenizer\",\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250112\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dba03d9",
   "metadata": {},
   "source": [
    "In order to generate text summarisation predictions, we can simply pass an input string from an English article to the constructed pipeline object as shown below. Note that upon receiving the first input, the encoder and decoder models need to be compiled. This should take a few minutes. After that, the compiled models can be reused without recompilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e227ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_text='Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.' \n",
      "num_tokens=188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00]\n",
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_text=[{'generated_text': \"The UK's vaccine agency has announced a crackdown on anti-virus vaccinations.\"}]\n"
     ]
    }
   ],
   "source": [
    "eng_text = \"\"\"Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.\"\"\"\n",
    "print(f\"{eng_text=}\", f\"\\nnum_tokens={len(eng_text.split())}\")\n",
    "summary_text = pipe(eng_text)\n",
    "print(f\"{summary_text=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7eddebb",
   "metadata": {},
   "source": [
    "The generated summary text is not a good representation of the article. On inspection, it can be found that both the input text and generated text are constrained to be no longer than 50 tokens. This value of 50 tokens is the default maximum length for the `text2text-generation` pipeline. However, the article we used has 188 tokens, resulting in the input text sent to the encoder being truncated to 50 tokens. As a result, the entire context of the article is not visible to the decoder. Parameters to control the length of the input and output sequence can be configured by specifying: \n",
    "\n",
    "- `max_input_length`: the maximum length of the input text\n",
    "- `max_length`: the maximum length of the generated text\n",
    "\n",
    "A selection of other parameters that can also affect the generated output are given below. For more information on these parameters and others please see the blog post on [how to generate text](https://huggingface.co/blog/how-to-generate).\n",
    "\n",
    "- `num_beams`\n",
    "- `do_sample`\n",
    "- `early_stopping`\n",
    "\n",
    "Aside from these parameters, there are also [model and task specific pipeline parameters](https://huggingface.co/docs/transformers/pipeline_tutorial) that can be customised. However these are not necessarily specific for execution on the IPU. Try to experiment with changing the various values to see the impact on the generated text.\n",
    "\n",
    "We now repeat the task of summarising the English article, but this time we override the default maximum input length and set `max_input_length=512`. We also specify `max_length=84`. The pipeline object is also reconstructed since the previous `pipe` object holds compiled models with sequence lengths limited to 50 instead. This is necessary because Graphcore's current PyTorch implementation only runs in static mode, so inputs are required to have static shapes. Therefore, changes to the number of inputs or their shapes requires recompilation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2641bf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:34<00:00]\n",
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:57<00:00]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text2text-generation\", model=model_checkpoint, ipu_config=ipu_config)\n",
    "max_input_length=512\n",
    "max_length=84\n",
    "summary_text = pipe(eng_text, max_length=max_length, max_input_length=max_input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b584ada",
   "metadata": {},
   "source": [
    "Now we see that the generated summary is a much better representation of the article text, given the longer context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae166713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_text=[{'generated_text': 'YouTube has banned thousands of videos spreading misinformation about Covid vaccines.'}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{summary_text=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37946185",
   "metadata": {},
   "source": [
    "## Approaches to inference using a dataset\n",
    "\n",
    "If we have to summarise a large dataset of articles in different languages, we can simply iterate over the samples, generating the summaries serially. The source dataset can be in any form you wish, for example a 🤗 [Dataset](https://huggingface.co/docs/datasets/index), a PyTorch [Dataset](https://pytorch.org/docs/stable/data.html), a pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) or even as `list[str]`, so long as we are able to retrieve `str` or `list[str]` samples from the dataset. \n",
    "\n",
    "We now demonstrate two trivial ways of obtaining outputs generated from a collection of samples. We use the [xlsum](https://huggingface.co/datasets/csebuetnlp/xlsum) dataset which consists of article-summary pairs from the BBC covering 45 langauges. For simplicity, we pick only the French subset from the dataset, but you can select other languages, for example Gujarati, Korean, Hausa, Hindi, Igbo and Indonesian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113e6108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xlsum (/nethome/mwizak/.cache/huggingface/datasets/csebuetnlp___xlsum/french/2.0.0/518ab0af76048660bcc2240ca6e8692a977c80e384ffb18fdddebaca6daebdce)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'summary', 'text'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "dataset = load_dataset(\"csebuetnlp/xlsum\", \"french\", split=\"test[:50]\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c236a82",
   "metadata": {},
   "source": [
    "To get a sense of what the data looks like, the following function will show some samples picked randomly from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b69511f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab0a14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les familles sont plongées dans la pauvreté par la pandémie, forçant les filles à se marier précocement (photo d'archive) Save the Children a déclaré que Covid-19 avait mis 2,5 millions de filles supplémentaires en danger de mariage précoce d'ici 2025. La pandémie augmente la pauvreté, obligeant les filles à quitter l'école et à travailler ou à se marier, a déclaré l'organisation caritative. Dans certaines régions d'Asie du Sud, d'Afrique et d'Amérique latine, les filles sont les plus vulnérables. L'organisation caritative britannique appelle les dirigeants du monde entier à s'engager à financer et à soutenir davantage les efforts visant à lutter contre le mariage des enfants et l'inégalité entre les sexes. A ne pas manquer sur BBC Afrique : \"Ces mariages violent les droits des filles et les exposent à un risque accru de dépression, de violence à vie, de handicap et même de mort\", a déclaré Karen Flanagan, conseillère en protection de l'enfance pour Save the Children. Selon elle, 78,6 millions de mariages d'enfants ont été empêchés au cours des 25 dernières années, mais les progrès pour mettre fin à cette pratique ont \"ralenti\". Le mois dernier, Girls Not Brides, un groupe qui fait campagne pour mettre fin aux mariages d'enfants, a déclaré à la BBC qu'il observait également cette tendance, provoquée par le ralentissement des économies et à la fermeture des écoles pendant la pandémie. Le Dr Faith Mwangi-Powell, directrice générale de Girls Not Brides, a déclaré que l'éducation \"fournit un filet de sécurité pour les filles\". Elle a ajouté qu'il fallait davantage de soutien financier, de suivi et d'engagement communautaire pour garantir que les filles puissent aller à l'école. Quelle est l'ampleur du problème ? Environ 12 millions de filles sont victimes de mariages précoces chaque année, selon l'organisation caritative. Mais son rapport indique que ce nombre devrait augmenter sensiblement au cours des cinq prochaines années, car les conséquences économiques de la pandémie se font sentir. Rien qu'en 2020, 500 000 autres filles risquent d'être forcées de se marier et jusqu'à un million d'autres devraient tomber enceintes, selon l'organisation caritative. Si aucune mesure n'est prise, il pourrait y avoir 61 millions de mariages d'enfants d'ici 2025, selon l'organisation caritative, mais cette estimation pourrait n'être que \"la partie émergée de l'iceberg\". Guinée : des ados contre le mariage précoce \"La pandémie pousse de plus en plus de familles dans la pauvreté, obligeant de nombreuses filles à travailler pour faire contribuer aux besoins de leur famille et à abandonner l'école - avec beaucoup moins de chances que les garçons d'y retourner un jour\", a déclaré Bill Chambers, le président et directeur général de l'organisation caritative. \"Un risque croissant de violence et d'exploitation sexuelle, combiné à une insécurité alimentaire et économique croissante, signifie également que de nombreux parents estiment n'avoir d'autre choix que de forcer leurs jeunes filles à épouser des hommes plus âgés\". En avril, les Nations unies ont déclaré qu'il pourrait y avoir jusqu'à 13 millions de mariages d'enfants supplémentaires dans le monde au cours de la prochaine décennie en raison de la pandémie. 'Les filles livrent leur corps à des hommes plus âgés' Pour son rapport, Save the Children s'est entretenu avec Esther, qui vit à Kinshasa, la capitale de la République démocratique du Congo. Des mesures de confinement visant à empêcher la propagation du Covid-19 dans sa communauté ont entraîné la fermeture d'écoles et de certains espaces publics. Pour l'instant, elle aide sa mère à s'occuper des poulets de la famille, mais l'impact économique de la pandémie se fait sentir - et en particulier chez les filles. \"De nombreux parents de mon quartier vendaient autrefois des marchandises au grand marché en plein air. Mais à cause du confinement, ils ne font plus rien. Les filles doivent se tourner vers des hommes plus âgés pour subvenir à leurs besoins\", a déclaré Esther. 'Je ne peux jamais compromettre mon éducation' L'organisation caritative a également parlé à Abena, conseillère de Save the Children en Éthiopie, âgée de 16 ans . Abena a travaillé avec les communautés locales pour empêcher que les filles soient contraintes d'épouser des hommes plus âgés. Malgré cela, les parents d'Abena voulaient toujours qu'elle se marie à 16 ans avec \"un homme éduqué et aisé\". Abena les a persuadés qu'elle devait poursuivre ses études. A lire aussi sur les mariages précoces : Ma réponse a été \"pas question\", a-t-elle dit. \"Je ne peux jamais compromettre mon éducation, et la demande de mariage elle-même est une violation des droits d'une fille tant qu'elle a moins de 18 ans\". Le Global Girlhood Report 2020 de l'organisation caritative, publié jeudi, analyse les effets du Covid-19 sur l'égalité des sexes dans le monde. Le rapport a également constaté que : A regarder : Pourquoi y a-t-il des mariages précoces ?</td>\n",
       "      <td>La pandémie de coronavirus pourrait entraîner une forte hausse des mariages d'enfants dans le monde, remettant en cause 25 ans de progrès dans la lutte contre cette pratique, a averti une organisation caritative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Les mesures de prévention continuent d’être appliquées en ce jour de rentrée au lycée BW Harris. Les professeurs devront continuer de prendre la température des élèves pour s’assurer qu'ils n’ont pas la maladie Fermer les écoles semble avoir été une mesure efficace parmi d’autres puisque l’épidémie semble reculer. En effet, le nombre de nouveaux cas d'Ebola chaque semaine est beaucoup plus faible aujourd'hui qu'il y a quelques mois. Isaac et Henry sont en dernière année. Henry est inquiet, il a peur d’attraper la maladie en classe et de repasser le virus à sa famille. Isaac, quant à lui, pense qu’il pourrait attraper la maladie des professeurs. Néanmoins, les deux garçons sont déterminés à retourner à l'école dès que possible. Le Liberia était le pays le plus touché avec près de 4 000 morts mais l’épidémie a également eu des conséquences dévastatrices pour ses voisins la Sierra Leone et la Guinée. Plus d'un million d'enfants n’ont pas pu aller en classe, les leçons ont été diffusées à la radio pour leur permettre de continuer à apprendre. Patrick à 12 ans. Il veut aussi reprendre les cours. Mais Ebola a tout changé. En allant s’inscrire, Patrick et sa mère ont découvert le nouveau règlement : pour limiter les risques d’infection, le nombre d’étudiant est désormais limité à 50 par classe. Le nombre limite ayant été atteint, Patrick n’a pas pu s’inscrire. Bien qu'il existe encore quelques cas d'Ebola au Liberia, de nombreux enfants sont maintenant désireux de retourner en classe. Mesure de prévention Les élèves devront se laver soigneusement les mains pour se désinfecter, et on devra prendre leur température pour s’assurer qu'ils n’ont pas la maladie. La maman et le papa d’Adama (14 ans) sont tous les deux morts du virus Ebola. Elle aussi est tombée malade mais elle a survécu. “Lorsque mon père est mort, ma mere est tombée malade. Quand ma mère est tombée malade, les gens ont dit qu’ils ne voulaient pas de moi à cause d’Ebola“. C’est la tante d’Adama qui s’occupe d’elle à présent. Elles vendent de l’eau dans des petits sacs plastique, ce qui leur rapporte peu d’argent. Adama aimerait aller à l’école. Mais sa tante pense que compte tenu la stigmatisation autour des survivants au virus, il est peut-être trop tôt pour que sa nièce retourne à l’école. Les écoles sont également invitées à limiter le nombre d'enfants par classe pour éviter la surpopulation. Certaines classes n’accueilleront que 50 élèves, ce qui, au Liberia, représente une classe de petite taille. Plus de 9000 personnes ont été tuées par l’épidémie en Afrique de l'Ouest, mais selon les ONG engagées dans la lutte contre la maladie, la situation s'améliore.</td>\n",
       "      <td>Des centaines d’élèves ont repris le chemin des classes alors que les écoles ont rouvert leurs portes cette semaine après une fermeture de six mois due à l’épidémie du virus Ebola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En soutien à Kaepernick, Rihanna ne chantera pas au Super Bowl Le quarterback de 30 ans a été le premier joueur à s'agenouiller lors de l'hymne national américain. Il a ainsi lancé un mouvement de boycott de l'hymne américain en 2016 pour dénoncer les violences policières visant la population noire. Lire aussi: Une source proche de la chanteuse aurait déclaré au magazine US Weekly que Rihanna \"n'est pas d'accord avec la position du NFL\". Le National Football League (NFL) a refusé de commenter les informations. Le groupe \"Maroon 5\" sera à l'affiche du spectacle qui aura lieu le 3 février à Atlanta, en Géorgie. Dénoncer les violences policières Kaepernick est sans équipe depuis qu'il a résilié son contrat avec les 49ers de San Francisco en mars 2017. D'autres joueurs avaient emboîté le pas à Colin Kaepernick, ce qui avait provoqué des critiques du président Donald Trump. Kaepernick est sans équipe depuis qu'il a résilié son contrat avec les 49ers de San Francisco en mars 2017. Il a déposé un grief contre les propriétaires d'équipes de la NFL, qui aurait conspiré pour ne pas l'engager à cause de ses protestations. Il est devenu le visage d'une nouvelle campagne publicitaire de Nike. Lire aussi :</td>\n",
       "      <td>Rihanna aurait refusé de jouer en première partie du spectacle du Super Bowl à la mi-temps afin de soutenir Colin Kaepernick.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La Ghouta orientale fait l'objet d'une offensive meurtrière du régime syrien. \"Sur ordre du président russe et dans le but d'éviter les pertes parmi les civils de la Ghouta orientale, une trêve humanitaire quotidienne sera instaurée à partir du 27 février de 09H00 à 14H00\", a indiqué le ministre russe de la Défense, Sergueï Choïgou. \"Sur ordre du président russe et dans le but d'éviter les pertes parmi les civils de la Ghouta orientale, une trêve humanitaire quotidienne sera instaurée à partir du 27 février de 09H00 à 14H00\", a indiqué le ministre russe de la Défense, Sergueï Choïgou. Le fief rebelle de la Ghouta orientale fait l'objet d'une offensive meurtrière du régime de Bachar al-Assad. Par ailleurs, la représentante de la diplomatie européenne Federica Mogherini a demandé lundi l'application immédiate de la trêve de 30 jours réclamée dimanche par le Conseil de sécurité de l'ONU. Cette trêve doit permettre l'acheminement de l'aide humanitaire et les évacuations médicales. L'accord d'Astana, signé le 4 mai 2017 par la Russie, l'Iran et la Turquie, prévoit la création de quatre zones de cessez-le-feu en Syrie. La région de la Goutha orientale figure dans ces zones.</td>\n",
       "      <td>Vladimir Poutine a ordonné à partir de mardi l'instauration d'une \"trêve humanitaire\" quotidienne dans la Ghouta orientale en Syrie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pas moins de 30 personnes avaient été appréhendées pour enlèvement et autres activités criminelles à travers le pays. Pas moins de 30 personnes avaient été appréhendées pour enlèvement et autres activités criminelles à travers le pays. Parmi eux se trouve un officier de l'armée nigériane accusé d'avoir dirigé un gang de kidnappeurs dans l'Etat d'Edo au sud du Nigeria. Plusieurs membres d'un groupe de kidnappeurs très actif ont été arrêtés durant le week-end à Lagos. Osinbajo condamne les menaces contre les Igbos \"Buhari va mieux\" Cameroun : un soldat tué dans un attentat suicide Cela fait suite à l'enlèvement de 6 écoliers à Lagos il y a deux semaines. Leurs ravisseurs demandaient près d'un million de dollars de rançon. 14 suspects ont un rapport avec ce que la police a décrit comme des \"activités terroristes\" au centre et au nord-est du Nigeria. Un complice du commandant de Boko haram a été arrêté à Damaturu dans l'état de Yobe tandis que deux insurgés présumés de Boko haram ont été appréhendés à Abuja. La police secrète a assuré que les personnes arrêtées sont sous le coup d'une enquête et seront présentées devant une cour prochainement.</td>\n",
       "      <td>La police secrète du Nigéria a annoncé l'arrestation de plusieurs malfaiteurs dont des kidnappeurs suspectés d'opérer le long de la route d'Abuja-Kaduna.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset.select_columns([\"text\", \"summary\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16835913",
   "metadata": {},
   "source": [
    "To obtain summaries of all the text in our dataset, we can pass a `list[str]` to our constructed pipeline object by accessing the `text` feature of our dataset. Note that internally the input `list[str]` is serially processed, so that only a single sample is sent to be processed on the IPU at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f72f888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 17min 40s, sys: 12.4 s, total: 1h 17min 53s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = pipe(dataset[\"text\"], max_input_length=max_input_length, max_length=max_length)\n",
    "predictions_dataset = dataset.add_column(\"generated_summary\", [p[\"generated_text\"] for p in predictions])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "235385c5",
   "metadata": {},
   "source": [
    "This is a selection of the generated summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e2743f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>generated_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yasuke représenté dans un livre pour enfants japonais par Kurusu Yoshio Il deviendra le premier homme né à l'étranger à obtenir le statut de guerrier samouraï. Son histoire a inspiré deux films produits par Hollywood. Connu sous le nom de Yasuke, cet homme était un guerrier qui a atteint le rang de samouraï sous le règne d'Oda Nobunaga, un puissant seigneur féodal japonais du XVIe siècle qui fut le premier des trois unificateurs du Japon. En 1579, son arrivée à Kyoto, la capitale à l'époque, avait provoqué une telle sensation que les gens se bousculaient pour l'apercevoir et certains moururent même écrasés dans le mouvement de foule, selon l'historien Lawrence Winkler. En moins d'un an, Yasuke avait rejoint les échelons supérieurs de la classe des guerriers japonais, les samouraïs. Peu de temps après, il parlait couramment le japonais et se battait aux côtés de Nobunaga. \"Sa taille était de 6 shaku 2 sun (environ 1,88m)... il était noir, et sa peau était comme du charbon de bois,\" un autre samouraï, Matsudaira Letada, le décrit dans son journal en 1579. La taille moyenne d'un Japonais en 1900 était de 1m57, ce qui fait que Yasuke aurait dominé la plupart des Japonais au XVIe siècle. Lire aussi A l'époque, les gens étaient généralement plus petits en raison d'une mauvaise alimentation. Fabrication d'un guerrier Il n'y a aucune trace de la date ou du pays de naissance de Yasuke. La plupart des historiens disent qu'il venait du Mozambique, mais certains ont suggéré d'autres pays comme l'Ethiopie ou le Nigeria. Ce que l'on sait, cependant, c'est que Yasuke est arrivé au Japon avec un jésuite italien du nom d'Alessandro Valignano lors d'une visite d'inspection, et n'apparaît dans l'histoire qu'entre 1579 et 1582. Certains experts disent qu'il était un esclave, mais c'est difficile à prouver. Floyd Webb et Deborah DeSnoo, cinéastes travaillant sur un documentaire à son sujet, croient que les affirmations selon lesquelles il était un esclave sont au mieux des spéculations. \"Il aurait été impossible pour Yasuke de se hisser au rang de samouraï en un an seulement sans un passé de guerrier \", dit Mme DeSnoo. Les samouraïs ont souvent commencé leur formation dès l'enfance. Amitié avec le seigneur de la guerre Yasuke a rencontré Nobunaga peu après son arrivée au Japon et il l'a intéressé, disent les cinéastes, en étant un talentueux débatteur. Yasuke parlait déjà un peu japonais et les deux hommes s'entendaient bien, selon l'universitaire Thomas Lockey, qui a écrit un livre sur Yasuke. Selon M. Lockey, Yasuke a diverti Nobunaga avec des contes d'Afrique et d'Inde, où M. Lockey pense que Yasuke avait passé quelque temps avant d'aller au Japon. M. Webb pense qu'en raison de sa maîtrise de la langue japonaise, Yasuke avait déjà un apriori favorable. Yasuke se bat aux côtés d'Oda Nobunaga dans Kurusu Yoshio, livre pour enfants Kuro-suke. \"Il n'était pas comme les Jésuites, qui avaient un programme religieux au Japon \", dit M. Webb. Selon certaines informations, Nobunaga aurait demandé à son neveu de donner une somme d'argent à Yasuke lors de leur toute première rencontre. L'écrivain franco-ivoirien Serge Bile était si intrigué par l'extraordinaire ascension de Yasuke qu'il a écrit un livre sur le guerrier. \"Cela fait partie du mystère qui entoure ce personnage. C'est pourquoi il me fascine\", a-t-il déclaré à la BBC. Le guerrier africain et le seigneur de guerre japonais avaient beaucoup en commun. Nobunaga était un grand fan des arts martiaux et passait beaucoup de temps à les pratiquer. C'était aussi un excentrique qui, selon M. Webb, s'habillait souvent à l'occidentale et recherchait la compagnie de personnes très disciplinées et intelligentes. Lire aussi \"Yasuke portait l'esprit guerrier\", dit M. Webb. Il comprenait le langage culturel du Japon et aimait danser et interpréter l'Utenzi - une forme historique de poésie narrative swahilie célébrant des actes héroïques, ajoute M. Webb. Cela suggère que Yasuke était peut-être originaire du Mozambique, comme certains historiens le croient, étant donné que le swahili est encore parlé dans certaines régions du nord du pays. De même, Nobunaga était un amoureux du théâtre Noh - une forme de drame musical classique japonais - et il est largement rapporté qu'il était un mécène des arts. Nobunaga s'est attaché à Yasuke et l'a traité comme un membre de sa famille. L'Africain faisait partie d'un groupe très sélectif de personnes autorisées à dîner avec lui. \"Nobunaga a fait l'éloge de la force et de la stature de Yasuke, décrivant sa puissance comme celle de 10 hommes,\" dit Mme DeSnoo. La légende continue de vivre L'écrivain franco-ivoirien Serge Bilé est depuis longtemps fasciné par Yasuke. Quand Nobunaga a conféré le rang de samouraï à Yasuke, l'idée d'un samouraï non-japonais n'avait jamais été entendue. Plus tard, d'autres étrangers obtiendront aussi le titre. En tant que premier samouraï né à l'étranger, Yasuke a livré d'importantes batailles aux côtés d'Oda Nobunaga. Il était également présent la nuit fatidique où un des généraux de Nobunaga, Akechi Mitsuhide, s'est retourné contre lui et a mis le feu au palais du seigneur de la guerre, piégeant Nobunaga dans une des pièces. Nobunaga a mis fin à sa vie en faisant un seppuku, un suicide rituel. Avant de se suicider, il a demandé à Yasuke de le décapiter et de porter sa tête et son épée à son fils, selon l'historien Thomas Lockley. C'était un signe de grande confiance. La légende de Yasuke prend fin peu après, en 1582. La chute de Nobunaga aux mains d'un général traître a entraîné l'exil du premier samouraï noir, peut-être de retour dans une mission jésuite à Kyoto. Lire aussi Bien que son destin et les dernières années de sa vie demeurent inconnus, Yasuke a vécu dans l'imagination de nombreux Japonais qui ont grandi avec le livre pour enfants primé Kuro-suke (kuro qui signifie \"noir\" en japonais) de Kurusu Yoshio. Le livre, qui dramatise la vie de Yasuke, se termine sur une note douce-amère : après que Nobunaga s'est suicidé, Kuro-suke (Yasuke) est conduit dans un temple où il rêve de ses parents en Afrique et pleure. Chadwick Boseman jouera Yasuke dans un prochain long métrage Le journal de l'industrie du divertissement Variety a rapporté en mai que l'acteur de Black Panther, Chadwick Boseman, jouera Yasuke dans un prochain long métrage. Ce sera le deuxième film hollywoodien en cours de développement sur la vie de Yasuke. En 2017, le studio hollywoodien Lionsgate a annoncé qu'il développait un film sur la vie du samouraï noir. Près de 500 ans plus tard, son parcours hors du commun continue d'émerveiller et d'inspirer les gens. Voir aussi Au Japon, la princesse Mako, la petite fille aînée de l'empereur Akihito va se marier.</td>\n",
       "      <td>Il y a près de 500 ans, un homme très grand de taille originaire d'Afrique est arrivé au Japon.</td>\n",
       "      <td>Le Japon est l'un des premiers hommes à avoir atteint le statut de guerrier samouraï.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Réputé généreux et discret, il symbolisait le savoir-faire local face aux multinationales du secteur des BTP La CSE, qu'il a fondé en 1970, est l'une des plus grandes entreprises de BTP (Bâtiment Travaux Publics) au Sénégal et en Afrique de l'ouest avec un chiffre d'affaires estimé en 2016 à 100 milliards de FCFA. Réputé généreux et discret, il symbolisait le savoir-faire local face aux multinationales du secteur. Le président sénégalais s'est dit peiné par sa disparition dans un hommage qu'il lui a rendu. \"Avec le rappel à Dieu de Aliou Sow, le Sénégal perd une figure emblématique de son secteur privé national. Créateur d'entreprises et philanthrope engagé, le fondateur de la CSE est un modèle de ténacité et de constance dans la quête de l'excellence. Je salue sa mémoire et présente à sa famille mes condoléances émues. Paix à son âme\", a déclaré Macky Sall. LIRE AUSSI Faire carrière au Sénégal Sénégal : des prisonniers boulangers Un photomontage de Macky Sall crée la polémique Au Sénégal, la voie de dégagement nord (VDN), l'hôtel Radisson Blu, à Dakar, l'aéroport international Blaise-Diagne de Diass (AIBD), la route Saint-Louis-Ndioum dans le nord du pays, le pont de Kolda en Casamance porte la signature de la CSE. La CSE est présente dans sept pays d'Afrique de l'ouest (Burkina Faso, Mali, Sierra Leone, Guinée, Niger, Gambie, Liberia) et en Afrique Centrale (Cameroun). Au Sénégal, la voie de dégagement nord (VDN), l'hôtel Radisson Blu, à Dakar, l'aéroport international Blaise-Diagne de Diass (AIBD), la route Saint-Louis-Ndioum dans le nord du pays, le pont de Kolda en Casamance porte la signature de la CSE. Aliou Sow est le père de Yérim Sow, un autre milliardaire , PDG du groupe Teyliom.</td>\n",
       "      <td>Aliou Ardo Sow, le fondateur de la Compagnie sahélienne d'entreprises (CSE), est décédé le 22 août à Paris à 83 ans.</td>\n",
       "      <td>Le président sénégalais Macky Sall a annoncé samedi la mort de son fondateur, l'ex-président de la Confédération des Nations unies pour les travaux publics (CSE).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Guillaume Soro, le président de l'Assemblée Nationale s'était gardé d'intervenir officiellement sur cette affaire de cache d'armes. Il a été envoyé en prison après son audition dans le cadre de la découverte en mai dernier d'une cache d'armes lors d'une mutinerie à Bouaké, dans le centre du pays. ''Une information judiciaire a été ouverte contre lui pour complot contre l'autorité de l'Etat'', selon le procureur. Dimanche, ses camarades de l'Alliance des Forces Nouvelles avaient publié un communiqué dans lequel ils se réservent le droit de ''lancer des mots d'ordre afin de préserver les libertés des Ivoiriens acquises de haute lutte'', selon leur porte-parole Sekongo Félicien. Plus de 6 tonnes de diverses armes de guerre et des munitions ont été découvertes dans une propriété de Souleymane Kamarate à Bouaké. Guillaume Soro, le président de l'Assemblée Nationale s'était gardé d'intervenir officiellement sur cette affaire de cache d'armes.</td>\n",
       "      <td>Souleymane Kamarate, chef du protocole du président de l'Assemblée Nationale, Guillaume Soro, a été écroué lundi à Abidjan.</td>\n",
       "      <td>Le président de l'Assemblée Nationale ivoirienne, Souleymane Kamarate, a été placé en prison pour complot contre le gouvernement.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L'étude a été réalisée par Fifpro et l'Université de Manchester. Cette étude qui sera dévoilée lundi prochain lors d'une conférence à Amsterdam souligne les écarts de rémunérations entre le foot féminin et masculin notamment au niveau des primes. \"Nos recherches montrent combien il est dur même pour une joueuse évoluant dans une sélection nationale de faire carrière dans le football\", souligne le secrétaire général de la FifPro Theo van Seggelen, cité dans un communiqué. L'étude a été réalisée par Fifpro et l'Université de Manchester. Il se base sur le témoignage de près de 3300 joueuses évoluant dans 33 pays, dont les Etats-Unis, l'Allemagne, la France, l'Angleterre ou la Suède. \"Les résultats de cette étude livrent un puissant témoignage sur les difficultés qu'éprouvent les joueuses aujourd'hui\", renchérit Caroline Jonsson, responsable de la branche féminine de la FifPro. Lire aussi: Serena Williams contre l’inégalité salariale Sierra Leone: bientôt une compétition de foot féminin JO 2016 : le football féminin pour commencer Nigéria : les footballeuses sans primes Des carrières pro plus courtes Selon les premiers chiffres dévoilés mardi, 87% des joueuses interrogées déclarent qu'elles pourraient réduire la durée de leur carrière footballistique. Cette étude révèle que des centaines de joueuses de haut niveau décident de mettre un terme à leur carrière plus que tôt que prévu \"afin de poursuivre une carrière plus viable ou fonder une famille\". Selon les premiers chiffres dévoilés mardi, 87% des joueuses interrogées déclarent qu'elles pourraient réduire la durée de leur carrière footballistique. L'une des causes de ce phénomène est la faible rémunération du football féminin. Ainsi, 66% des joueuses évoluant dans une équipe nationale se déclarent non-satisfaites par le niveau des primes, tandis que 35% d'entre elles ne sont tout simplement pas payées lorsqu'elles représentent leurs pays. A titre de comparaison, l'ensemble des primes versées lors de l'Euro-2016 masculin à 24 équipes atteignait 301 millions d'euros, tandis que pour l'Euro-2017 féminin à 16 équipes ce montant total devrait atteindre 8 millions d'euros, selon les chiffres de l'UEFA, organisateur des compétitions européennes.</td>\n",
       "      <td>La moitié des footballeuses ne sont pas payées par leurs clubs, selon les premiers chiffres d'une étude la Fifpro, le syndicat mondial des footballeurs.</td>\n",
       "      <td>Les centaines de joueuses de haut niveau décident de mettre un terme à leur carrière footballistique plus que tôt que prévu, selon une étude de la FifPro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les initiatives de paix au Soudan du Sud ont toutes échoué Le président du Soudan du Sud Salva Kiir et l'ancien chef rebelle Riek Machar ont convenu de former un gouvernement d'unité avant l'expiration de la date limite du 22 février. L'accord vise à mettre fin à un conflit meurtrier de six ans qui a fait plus 400 000 morts. Certaines questions restent en suspens, notamment celles relatives au partage du pouvoir et à l'intégration des combattants. Les deux parties ont accepté de traiter les autres questions ultérieurement. L'accord a été annoncé quelques heures après la publication par l'ONU d'un rapport accablant accusant les deux parties d'avoir délibérément affamé les civils pendant leur lutte pour le pouvoir.Lire aussi: Rapport accablant de l'ONU sur le Soudan du Sud Quels sont les enjeux de cet d'accord? On espère que l'accord mettra fin à la guerre civile dévastatrice qui sévit au Soudan du Sud depuis six ans. Ce conflit a tué quelque 400 000 personnes et déplacé environ un tiers des 12 millions d'habitants. Le président Kiir a exprimé son désir de voir la période transitoire de trois ans aboutir au retour des réfugiés et des déplacés. Si l'accord se concrétise, il pourrait annoncer un nouveau départ dans le plus jeune pays du monde. Lire aussi: Au Soudan du Sud, les rebelles rejettent la fédération de 10 Etats Pourquoi ces combats? Le Soudan du Sud est devenu un État indépendant du Soudan en 2011, marquant la fin d'une longue guerre civile. Mais il n'a pas fallu longtemps pour que la promesse de paix s'effondre. Deux ans seulement après l'indépendance, le pays est retombé dans un conflit violent en décembre 2013, après que le président Kiir a limogé Machar, alors vice-président. Why is South Sudan starving? Le président Kiir avait accusé M. Machar de préparer un coup d'État pour le renverser, ce que ce dernier a nié. La guerre a des origines politiques et ethniques avec des implications sur les relations de pouvoir. Les Dinka et les Nuer, les deux plus grands groupes ethniques du Sud-Soudan, auxquels appartiennent les deux dirigeants, ont été accusés de s'être pris pour cible pendant la guerre, des atrocités ayant été commises par toutes les parties. Pourquoi a-t-il été si difficile de parvenir à un accord de paix ? Les parties n'avaient pas pu ou voulu s'entendre sur les modalités de formation d'un gouvernement de transition, conformément à l'accord de paix revitalisé de 2018. L'accord devait être finalisé en mai 2019 mais a été reporté à deux reprises, la dernière échéance étant le 22 février. Riek Machar n'est plus à Juba de façon permanente depuis 2016 Le conflit a précipité le pays dans une profonde crise humanitaire. Malgré la situation, il a été difficile pour les parties de parvenir et de maintenir un accord de paix qui pourrait stabiliser le pays. Les deux principaux dirigeants se méfient l'un de l'autre et il n'y a pas eu de relations de travail cordiales depuis que le président Kiir a limogé M. Machar en 2013. Riek Machar n'est jamais retourné définitivement à Juba, craignant pour sa sécurité - depuis qu'il a fui le pays lorsque ses forces étaient engagées dans des affrontements avec les troupes gouvernementales lors de l'effondrement de l'accord de paix de 2016. Lire aussi: Salva Kiir et Riek Machar s'engagent à former un gouvernement A quoi ressemble la vie au Soudan du Sud ? Le Fonds monétaire international (FMI) classe le pays comme le plus pauvre du monde, en termes de PIB par habitant. Une grande partie du pays ne dispose pas d'infrastructures. Le pays n'a que 300 kilomètres de routes pavées, sur une superficie de plus de 600 000 kilomètres carrés. La plupart des régions du pays situées en dehors des centres urbains n'ont ni électricité ni eau courante. World's youngest country 2011Gained independence 2013Civil war started 4.3mPeople forced from their homes 12mPeople in the country 82%Of people live on less than $1 a day 65%Of the population unable to read and write Le Soudan du Sud a également l'un des taux d'alphabétisation les plus faibles au monde, 34,5 %, selon l'Unesco (2018). L'Unicef, l'organisation des Nations unies pour l'enfance, estime que 70 % (environ 2,2 millions) des enfants ne sont pas scolarisés - ce qui met en péril leur avenir et celui de leur pays. Cela représente l'un des taux les plus élevés d'enfants non scolarisés dans le monde. Au niveau mondial, le Sud-Soudan se classe au quatrième rang des indices de développement humain les plus bas, malgré son énorme potentiel en ressources naturelles, telles que les terres agricoles fertiles, l'or, les diamants et le pétrole. En 2019, plus de la moitié de la population avait besoin d'une aide humanitaire, avec des niveaux extrêmes d'insécurité alimentaire dans tout le pays, selon la Banque mondiale. Le pays dépend presque exclusivement des revenus du pétrole et il y a très peu d'investissements dans d'autres secteurs comme l'agriculture et les infrastructures. L'accord garantit-il une paix durable ? Il n'y a pas de garanties que cet accord favorisera une paix durable dans le pays. Les accords précédents ont été largement annoncés pour s'effondrer peu de temps après. Plus de dix accords et cessez-le-feu ont été conclus depuis 2013. Les deux dirigeant ont affiché leur incapacité à maintenir un quelconque accord, y compris sur le partage du pouvoir. Peter Adwok Nyaba, un activiste et ancien ministre du Soudan du Sud, déclare dans un avis de 2019 que l'accord ne traite pas complètement les éléments conflictuels du nationalisme ethnique, des luttes de pouvoir et de la faiblesse des institutions de gouvernance, qui, selon lui, restent persistants malgré l'accord. \"C'est un cercle vicieux typique : pauvreté-conflit-paix-absence de développement, puis conflit\", dit-il.</td>\n",
       "      <td>L'ancien chef rebelle Riek Machar a prêté serment ce samedi en tant que premier vice-président.</td>\n",
       "      <td>Les deux parties ont signé un accord pour mettre fin à la guerre civile qui sévit au Soudan du Sud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(predictions_dataset.select_columns([\"text\", \"summary\", \"generated_summary\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "131458cb",
   "metadata": {},
   "source": [
    "An alternative to passing an in-memory `list[str]` to our pipeline object is to use a generator object that allows us to load samples from our dataset on the fly. This is favourable to the in-memory approach especially in cases when we are not able to allocate the entire dataset in main memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146fe84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 µs, sys: 3 µs, total: 28 µs\n",
      "Wall time: 56.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def data(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        yield dataset[i][\"text\"]\n",
    "   \n",
    "# predictions = []\n",
    "# for out in pipe(data(dataset), max_input_length=max_input_length, max_length=max_length):\n",
    "#     predictions.append(out[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cbadfe0",
   "metadata": {},
   "source": [
    "## Batched inference on the IPU\n",
    "\n",
    "As mentioned above, by default, a collection of samples provided to pipeline objects are processed serially. This is because the performance characterisation of batching depends on the specific problem being solved, as shown in the 🤗 documentation for [pipeline batching](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching). In practice, the decision to enable batching on the IPU or any other device should be based on your own experiments.\n",
    "\n",
    "To enable batching on the IPU, we simply need to use the `batch_size` parameter when passing a collection of samples to our pipeline object. Again, since the models attached to our pipeline object are compiled to process a single sample on the IPU, we redefine the pipeline object below so that recompilation takes the `batch_size` parameter into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae614e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:50<00:00]\n",
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:33<00:00]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text2text-generation\", model=model_checkpoint, ipu_config=ipu_config)\n",
    "predictions = []\n",
    "for out in pipe(data(dataset), max_input_length=max_input_length, max_length=max_length, batch_size=2):\n",
    "    predictions.append(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f82e57",
   "metadata": {},
   "source": [
    "Below we see that sending 2 samples to the IPU is faster than sending a single sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb84613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 1min 12s, sys: 0 ns, total: 1h 1min 12s\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = []\n",
    "for out in pipe(data(dataset), max_input_length=max_input_length, max_length=max_length, batch_size=2):\n",
    "    predictions.append(out[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0f1d32f",
   "metadata": {},
   "source": [
    "For `mt5-base` inference, batch sizes greater than 2 are unable to fit. To fit larger batch sizes, we can try to change the different maximum length parameters as well as `num_beams`. However, this introduces a trade-off between obtaining good text summaries with device throughput. By using more IPUs and sharding the encoder and decoder layers among them, we can fit larger batch sizes. If you have access to a POD16 machine on Paperspace, try using: `inference_layers_per_ipu=[0, 4, 4, 4, 2, 5, 5, 0]` and  `ipus_per_replica=8` with a new IPU configuration to try batch sizes greater than 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8893053e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated how to easily run inference workloads for mT5 given a model checkpoint from the Hugging Face Hub on the Graphcore IPU. Apart from specifying IPU configurations, we have shown that the interface to run inference is exactly the same as the default [🤗 pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial). \n",
    "\n",
    "For other examples on how to do X on the IPU, see [resource](somewhere). Have a question? Please contact us on our [Graphcore community channel](https://www.graphcore.ai/join-community)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
