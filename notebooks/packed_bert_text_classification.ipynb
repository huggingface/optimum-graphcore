{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "First of all, make sure your environment has installed the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/optimum-graphcore.git;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make sure all the packages required for text classification are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn;\n",
    "! pip install matplotlib;\n",
    "! pip install tokenizers==0.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the versions of Transformers and Optimum Graphcore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import optimum.graphcore\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(optimum.graphcore.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning BERT on a text classification task using packing\n",
    "\n",
    "This notebook is an alternative for [Fine-tuning BERT on a text classification task](text_classification.ipynb) showing how to implement packing for BERT step by step and use if for fine-tuning on `GLUE/sst2` text classification. This includes packing the dataset and adapting an existing BERT model. Packing consists in concatenating several input sequences into one to increase the computational efficiency. More details about packing can be found in the [blog post](https://www.graphcore.ai/posts/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing) and the original [paper](https://arxiv.org/abs/2107.02027)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model to a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/).\n",
    "\n",
    "![Widget inference on a text classification task](images/text_classification.png)\n",
    "\n",
    "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a  dataset containing sentences labeled grammatically correct or not.\n",
    "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
    "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
    "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
    "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
    "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
    "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
    "- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
    "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)\n",
    "\n",
    "We will see how to easily load the dataset for each one of those tasks and use packed BERT to fine-tune a model on it. Each task is named by its acronym, with `mnli-mm` standing for the mismatched version of MNLI (so same training set as `mnli` but different validation and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZbiBDuGIrId"
   },
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this Packed bert demo, we will cover (single-label) sequence classification on `sst2` dataset. But `task` can be changed to run the other `GLUE` tasks . However, training hyperparameters may need some tuning for these other tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "In this notebook, we are using both data parallelism and pipeline parallelism (see this [tutorial](https://github.com/graphcore/tutorials/tree/master/tutorials/pytorch/tut2_efficient_data_loading) for more). Therefore the global batch size, which is the actual number of samples used for the weight update, is determined with three factors:\n",
    "- global batch size = micro_batch_size * gradient accumulation steps * replication factor\n",
    "\n",
    "and replication factor is determined by `pod_type`, which will be used as a key to select the replication factor from a dictionary defined in the IPU config file. For example, the dictionary in the IPU config file [Graphcore/roberta-base-ipu](https://huggingface.co/Graphcore/roberta-base-ipu/blob/main/ipu_config.json) looks like this:\n",
    "- \"replication_factor\": {\"pod4\": 1, \"pod8\": 2, \"pod16\": 4, \"pod32\": 8, \"pod64\": 16, \"default\": 1}\n",
    "\n",
    "Depending on you model and the pod machine you are using, you might need to adjust these three batch-size-related arguments.\n",
    "\n",
    "By default this notebook is configured to run on 4 IPUs.\n",
    "\n",
    "Finally, `max_seq_length` is the length we are going to pad the sentences to, so it should not be larger than the maximum length of the model. Set those seven parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the small size of the sequences in `sst2`, we can reduce the model input size to `max_seq_length = 256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "task = \"sst2\"\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "ipu_config_name = \"Graphcore/bert-base-uncased\"\n",
    "micro_batch_size = 2\n",
    "gradient_accumulation_steps = 32\n",
    "pod_type = \"pod4\"\n",
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKx2zKs5IrIq"
   },
   "source": [
    "Apart from `mnli-mm` being a special code, we can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o4rUteaIrI_",
    "outputId": "18038ef5-554c-45c5-e00a-133b02ec10f1"
   },
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "You can call its `compute` method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XN1Rq0aIrJC",
    "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOCrQwPoIrJG"
   },
   "source": [
    "Note that `load_metric` has loaded the proper metric associated to your task, which is:\n",
    "\n",
    "- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
    "- for MNLI (matched or mismatched): Accuracy\n",
    "- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
    "- for QNLI: Accuracy\n",
    "- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
    "- for RTE: Accuracy\n",
    "- for SST-2: Accuracy\n",
    "- for STS-B: [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Spearman's_Rank_Correlation_Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
    "- for WNLI: Accuracy\n",
    "\n",
    "so the metric object only computes the one(s) needed for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyGdtK9oIrJM"
   },
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbqtC4MrIrJO"
   },
   "source": [
    "We can double check it does work on our current dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19GG646uIrJO",
    "outputId": "0cb4a520-817e-4f92-8de8-bb45df367657"
   },
   "outputs": [],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the three arguments.`truncation=True` will ensure that an input longer than maximum length will be truncated to the maximum length. `max_length=max_seq_length` sets the maximum length of a sequence.\n",
    "\n",
    "**Note: since we will use packing later, we don't want to perform any padding in the tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "# no padding for packing\n",
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, max_length=max_seq_length)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
   },
   "outputs": [],
   "source": [
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "len(encoded_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Packing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement packing, we need to pack our dataset first. Each new element will be a \"pack\" containing at most `max_seq_per_pack` sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_per_pack = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to pack efficiently, we will use an histogram-based algorithm (SPFHP) presented in the [blog post](https://www.graphcore.ai/posts/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing) https://github.com/graphcore/tutorials/tree/master/blogs_code/packedBERT. First we need to generate the histogram of the sequences lengths in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_histogram(unpadded_input_ids, max_seq_len):\n",
    "    dataset_seq_lens:list = np.array([len(seq) for seq in unpadded_input_ids])\n",
    "    histogram = np.zeros(max_seq_len, dtype=np.int64)\n",
    "    seq_lens, counts = np.unique(dataset_seq_lens, return_counts=True)\n",
    "    histogram[seq_lens - 1] = counts\n",
    "\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "\n",
    "train_dataset = encoded_dataset['train']\n",
    "val_dataset = encoded_dataset[validation_key]\n",
    "\n",
    "train_hist = generate_histogram(train_dataset['input_ids'], max_seq_length )\n",
    "val_hist = generate_histogram(val_dataset['input_ids'], max_seq_length )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_hist, bins = [k for k in range(0,max_seq_length,10)]) \n",
    "plt.title(\"sequences length histogram\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the `Shortest pack first histogram packing` algorithm to generate a packing strategy from the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy import optimize, stats\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def add_pack(pack, count, tmp, final, limit, offset, max_sequence_length=512):\n",
    "    \"\"\"Filter out packs that reached maximum length or number of components.\"\"\"\n",
    "    if len(pack) == limit or offset == 0:\n",
    "        final[offset].append((count, pack))\n",
    "    else:\n",
    "        tmp[offset].append((count, pack))\n",
    "\n",
    "\n",
    "#^SPFHP - Shortest pack first histogram packing\n",
    "def SPFHP(histogram, max_sequence_length, max_sequences_per_pack):\n",
    "    \"\"\"Shortest-pack-first histogram-packing.\"\"\"\n",
    "    start = time.time()\n",
    "    reversed_histogram = np.flip(histogram)\n",
    "    # Initialize main strategy data dictionary.\n",
    "    # The key indicates how many tokens are left for full length.\n",
    "    # The value is a list of tuples, consisting of counts and respective packs.\n",
    "    # A pack is a (sorted) list of sequence length values that get concatenated.\n",
    "    tmp_strategies_per_length = defaultdict(list)\n",
    "    strategies_per_length = defaultdict(list)\n",
    "    # Index i indicates here, how much space is left, due to reversed histogram\n",
    "    for i in range(max_sequence_length):\n",
    "        n_sequences_to_bin = reversed_histogram[i]\n",
    "        length_to_bin = max_sequence_length - i\n",
    "        offset = i + 1  # largest possible offset\n",
    "        while n_sequences_to_bin > 0:\n",
    "            if (length_to_bin + offset) in tmp_strategies_per_length:\n",
    "                # extract shortest pack that will get modified\n",
    "                n_sequences_to_pack, pack = tmp_strategies_per_length[\n",
    "                    length_to_bin + offset].pop()\n",
    "                new_pack = pack + [length_to_bin]\n",
    "                count = min(n_sequences_to_pack, n_sequences_to_bin)\n",
    "                if n_sequences_to_pack > n_sequences_to_bin:\n",
    "                    # old pack gets reduced\n",
    "                    n_sequences_to_pack -= n_sequences_to_bin\n",
    "                    tmp_strategies_per_length[length_to_bin + offset].append(\n",
    "                        (n_sequences_to_pack, pack))\n",
    "                    n_sequences_to_bin = 0\n",
    "                else:\n",
    "                    n_sequences_to_bin -= n_sequences_to_pack\n",
    "                add_pack(new_pack, count,\n",
    "                         tmp_strategies_per_length, strategies_per_length,\n",
    "                         max_sequences_per_pack, offset)\n",
    "                # clean up to speed up main key search\n",
    "                if not tmp_strategies_per_length[length_to_bin + offset]:\n",
    "                    tmp_strategies_per_length.pop(length_to_bin + offset)\n",
    "            else:\n",
    "                offset -= 1\n",
    "            # Does not fit anywhere. Create new pack.\n",
    "            if offset < 0:\n",
    "                add_pack([length_to_bin], n_sequences_to_bin,\n",
    "                         tmp_strategies_per_length, strategies_per_length,\n",
    "                         max_sequences_per_pack, i)\n",
    "                n_sequences_to_bin = 0\n",
    "    # merge all strategies\n",
    "    for key in tmp_strategies_per_length:\n",
    "        strategies_per_length[key].extend(tmp_strategies_per_length[key])\n",
    "    # flatten strategies dictionary\n",
    "    strategy_set = []\n",
    "    strategy_repeat_count = []\n",
    "    for key in strategies_per_length:\n",
    "        for count, pack in strategies_per_length[key]:\n",
    "            pack.reverse()\n",
    "            strategy_set.append(pack)\n",
    "            strategy_repeat_count.append(count)\n",
    "\n",
    "    # Summarize efficiency of solution\n",
    "    duration = time.time() - start\n",
    "    sequence_lengths = np.arange(1, max_sequence_length + 1)\n",
    "    strategy_repeat_count = np.array(strategy_repeat_count)\n",
    "    n_strategies = len(strategy_set)\n",
    "    old_number_of_samples = histogram.sum()\n",
    "    new_number_of_samples = strategy_repeat_count.sum()\n",
    "    sequences = sum([count*len(pack) for count, pack in\n",
    "                     zip(strategy_repeat_count, strategy_set)])\n",
    "    total_tokens = max_sequence_length * new_number_of_samples\n",
    "    empty_tokens = sum([count*(max_sequence_length-sum(pack)) for count, pack\n",
    "                        in zip(strategy_repeat_count, strategy_set)])\n",
    "    efficiency = 100 - empty_tokens / total_tokens * 100\n",
    "    speedup_upper_bound = 1.0 / (1 - (histogram*(1 - sequence_lengths / max_sequence_length)).sum() / old_number_of_samples)\n",
    "    packing_factor = sequences/sum(strategy_repeat_count)\n",
    "    \n",
    "    print(f\"Packing efficiency (fraction of real tokens): {efficiency:3.4f}\\n\",\n",
    "          f\"Speed-up theoretical limit: {speedup_upper_bound:3.4f}\\n\",\n",
    "          f\"Achieved speed-up over un-packed dataset: {old_number_of_samples/new_number_of_samples:3.5f}\\n\",\n",
    "          f\"Runtime: Packed {old_number_of_samples} sequences in {duration:3.3f} seconds\\n\",\n",
    "          f\"Average packing factor: {packing_factor}\")\n",
    "    \n",
    "\n",
    "    return strategy_set, np.array(strategy_repeat_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`strategy_set` is a list of lists containing the sequences lenghts we can pack together.\n",
    "\n",
    "`strategy_repeat_count` gives the corresponding number of time we can create each pack of `strategy_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_strategy = SPFHP(train_hist, max_seq_length, max_seq_per_pack)\n",
    "val_strategy = SPFHP(val_hist, max_seq_length, max_seq_per_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the actual packed dataset object. \n",
    "We pick the sequences and pack them based on their length and following the strategy we just generated. Once they are packed, we also need to pad the sequences to the `max_seq_lentgh` to maintain a constant input size.\n",
    "\n",
    "Notes:\n",
    "- A specific `attention_mask` is generated: It contains a unique index for each sequence of the pack and `0` for the remaining padding tokens.\n",
    "    - Example of 3 sequences: `attention_mask = [1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,0,...,0,1,2,3]`\n",
    "\n",
    "\n",
    "- The [CLS] tokens of each sequence are moved at the end of the pack.\n",
    "    - For instance: `[CLS,a,b,c] + [CLS, d,e,f] + [CLS, g,h,i] -> [a,b,c,d,e,f,g,h,i,...,CLS,CLS,CLS]`\n",
    "    \n",
    "\n",
    "- The `position_ids` of a pack contains the concatenated `position_ids` of each sequences \n",
    "    - For instance given 3 sequences: `[0,1,2,3,4] + [0,1,2,3] + [0,1,2] -> [1,2,3,4,1,2,3,1,2,...,0,0,0]` (note: the CLS tokens position id '0' are also moved the end of the pack)\n",
    "    \n",
    "- `labels` and `token_type_ids` are also packed to correspond the `input_ids` pack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_dataset_from_strategy(data, strategy_set, strategy_repeat_count, max_seq_len, max_seq_per_pack):\n",
    "    total_num_packs:int = np.sum(strategy_repeat_count)\n",
    "        \n",
    "\n",
    "    # Sort the sequences by length\n",
    "    dataset_seq_lens = np.array([len(seq) for seq in data['input_ids']])\n",
    "    len_sorted_seq_idxs = np.argsort(dataset_seq_lens)\n",
    "    len_sorted_seq_lens = dataset_seq_lens[len_sorted_seq_idxs]\n",
    "    sorted_seqs = np.stack((len_sorted_seq_lens, len_sorted_seq_idxs))\n",
    "\n",
    "\n",
    "    # Get the data from the tokenised dataset\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = data['label']\n",
    "    \n",
    "    # Prepare the manually padded constant sized data\n",
    "    packed_input_ids = np.zeros((total_num_packs, max_seq_len), dtype=int)\n",
    "    packed_attention_mask = np.zeros((total_num_packs, max_seq_len), dtype=int)\n",
    "    packed_token_type_ids = np.zeros((total_num_packs, max_seq_len), dtype=int)\n",
    "    packed_position_ids = np.zeros((total_num_packs, max_seq_len), dtype=int)\n",
    "    packed_labels = -100 * np.ones((total_num_packs, max_seq_per_pack), dtype=int)\n",
    "    \n",
    "    # Pack the data using the developed strategies\n",
    "    pack_index = 0\n",
    "    for i in range(len(strategy_repeat_count)):\n",
    "        strategy = strategy_set[i]\n",
    "        # This is the offset we apply to the start positions to account for the positional change of the logits when unmasking the pack to extract a set of logits for each sequence in the pack\n",
    "        for _ in range(strategy_repeat_count[i]):\n",
    "\n",
    "            '''Key terms in loop:\n",
    "\n",
    "            * sorted_seqs: (shape [2, dataset])\n",
    "                - index 0: sorted lengths of each sequence in dataset\n",
    "                    -- e.g. sorted_seqs[0,12] gives the length of the sequence at dataset position at index: sorted_seqs[1,12]\n",
    "                - index 1: index of corresponding lengths in the dataset\n",
    "                    -- e.g. dataset[sorted_seqs[1,12]] returns dataset sequence at index: sorted_seqs[1,12]\n",
    "\n",
    "            * ref_inds: (shape [strategy_set])\n",
    "                - the indices of the [length, dataset index] pair in sorted_seqs (this is used to remove/clear sorted_seqs as data is packed).\n",
    "                    -- e.g sorted_seqs[0, ref_inds] = -1 will nullify the sequence length at positions in [array] ref_inds such that they cannot be called to pull data from those indices again.\n",
    "\n",
    "            * inds: (shape [strategy_set])\n",
    "                - the indices in the actual dataset, called using the indices of sorted_seqs retrieved from ref_inds.\n",
    "                    --e.g. > inds = sorted_seqs[1, ref_inds]\n",
    "                           > packed data = concatenate(dataset[inds])\n",
    "            '''\n",
    "\n",
    "            ref_inds = []\n",
    "            for x in strategy:\n",
    "                ref_ind = np.argwhere(sorted_seqs[0] == x)[-1]\n",
    "                sorted_seqs[0, ref_ind] = -1\n",
    "                ref_inds.append(ref_ind)\n",
    "\n",
    "            inds = sorted_seqs[1, ref_inds].ravel()\n",
    "\n",
    "            # Exclude the CLS tokens to put them at the end later\n",
    "            input_id_pack = list(itertools.chain(*[input_ids[x][1:] for x in inds]))\n",
    "            attention_mask_pack = list(itertools.chain(*[itertools.repeat(n+1, len(attention_mask[v])-1) for n,v in enumerate(inds)]))\n",
    "            token_type_ids_pack = list(itertools.chain(*[token_type_ids[x][1:] for x in inds]))\n",
    "            position_ids_pack = list(itertools.chain(*[range(1, len(attention_mask[v])) for n,v in enumerate(inds)]))\n",
    "\n",
    "            # Create the equivalent tokenised packed dataset\n",
    "            packed_input_ids[pack_index, :len(input_id_pack)] = input_id_pack\n",
    "            packed_attention_mask[pack_index, :len(attention_mask_pack)] = attention_mask_pack\n",
    "            packed_token_type_ids[pack_index, :len(token_type_ids_pack)] = token_type_ids_pack\n",
    "            packed_position_ids[pack_index, :len(position_ids_pack)] = position_ids_pack\n",
    "            labels_pack = [labels[x] for x in inds]\n",
    "            packed_labels[pack_index, :len(labels_pack)] = labels_pack\n",
    "\n",
    "            # Now add the CLS tokens and their masks at the end of the pack\n",
    "            packed_input_ids[pack_index, -max_seq_per_pack:] = [input_ids[0][0] for _ in range(max_seq_per_pack)]\n",
    "            packed_attention_mask[pack_index, -max_seq_per_pack:] = list(range(1, max_seq_per_pack+1))\n",
    "\n",
    "            pack_index += 1\n",
    "            \n",
    "    new_dataset = Dataset.from_dict({ \"input_ids\": packed_input_ids,\n",
    "                                      \"attention_mask\": packed_attention_mask,\n",
    "                                      \"token_type_ids\": packed_token_type_ids,\n",
    "                                      \"position_ids\": packed_position_ids,\n",
    "                                      \"labels\": packed_labels\n",
    "                                })\n",
    "    new_dataset.set_format(type='torch', columns=new_dataset.features)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_train_dataset = create_dataset_from_strategy(train_dataset, train_strategy[0], train_strategy[1], max_seq_length, max_seq_per_pack)\n",
    "packed_val_dataset = create_dataset_from_strategy(val_dataset, val_strategy[0], val_strategy[1], max_seq_length, max_seq_per_pack)\n",
    "\n",
    "print(packed_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one sample of the new `packed_train_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_train_dataset[3020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. The number of labels will be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, default_data_collator\n",
    "from optimum.graphcore import IPUConfig, IPUTrainer, IPUTrainingArguments\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Packed BERT\n",
    "\n",
    "A few model modifications are required to make packing work with BERT.\n",
    "We will extend the existing class `BertForSequenceClassification`.\n",
    "\n",
    "First let's load a default BERT configuration using `AutoConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "config.max_position_embeddings = max_seq_length\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packing sequences increases the number of elements per batch.\n",
    "In order to reuse the classifications heads from `transformers` library, we need a special pooler. Instead of pooling the hidden states of a single sequence, it's pooling multiple ones (given the maxium number of sequences in the pack) and ordering them along the batch dimension. So the output size of the pooler is: `[batch-size x max_sequences_per_pack, hidden_size]`\n",
    "\n",
    "From the Loss point-of-view , everything will appear as if the batch-size was larger (`batch-size x max_sequences_per_pack`).\n",
    "When the number of sequences in the pack is lower than `max_sequences_per_pack`, padding is ignored by using the default `ignore_index` (-100) of the loss as a special labels (this was already done in the dataset preprocessing, cf: *Packing the dataset*).\n",
    "\n",
    "![pooling](images/pooling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PackedBertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.max_sequences_per_pack = config.max_sequences_per_pack\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden states corresponding\n",
    "        # to the last max_sequences_per_pack tokens. Note that the [CLS] tokens\n",
    "        # are always located at the end of the pack. When the actual number of\n",
    "        # sequences is lower than max_sequences_per_pack, we still slice out\n",
    "        # the last max_sequences_per_pack tokens, but we will not use all of\n",
    "        # them during loss calculation.\n",
    "        sh = hidden_states.shape\n",
    "        last_tokens_tensors = hidden_states[:, -self.max_sequences_per_pack:]\n",
    "        last_reshape = last_tokens_tensors.reshape(sh[0]*self.max_sequences_per_pack, sh[2])\n",
    "        # output size: [bs x max_sequences_per_pack, hidden_size]\n",
    "        output = self.dense(last_reshape)\n",
    "        output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention mask\n",
    "The attention mask should be used in a specific way in packed-BERT.\n",
    "We will create a 2D attention mask like in the following example.\n",
    "By doing so, the cross-attention will treat separately each sequence of the pack (and it will also ignore the padding).\n",
    "![attn-mask](images/attention-mask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better intuition here is an example showing how to transform the 1D attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 : Flat attention mask genreated by the dataset. Each sequence has a different index. 0 is padding.\n",
    "attention_mask = torch.tensor([[1,1,2,2,3,3,3,4,4,4,4,0,0,0,0,1,2,3,4]])\n",
    "# 2: Generate the boolean 2D attention mask\n",
    "attention_mask = attention_mask[:, None, :].repeat(1, attention_mask.shape[1], 1)\n",
    "attention_mask = (attention_mask == attention_mask.transpose(1, 2)) * (attention_mask != 0)\n",
    "# Notice that the mask is always False for the padding tokens.\n",
    "print(attention_mask.to(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's integrate this idea to the input of packed BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inheriting from `BertPipelineMixin` , the `paralellize()` method is already implemented for the BERT body. We overloaded it to also place the classifier on the last IPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "from optimum.graphcore.models.bert.modeling_bert import BertPipelineMixin\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "class PackedBertForSequenceClassification(BertForSequenceClassification, BertPipelineMixin):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config.max_sequences_per_pack = max_seq_per_pack\n",
    "        self.bert.pooler = PackedBertPooler(config)\n",
    "        \n",
    "    def parallelize(self):\n",
    "            super().parallelize()\n",
    "            last_ipu = self.ipu_config.ipus_per_replica - 1\n",
    "            self.classifier = poptorch.BeginBlock(self.classifier, \"Classifier Output\", ipu_id=last_ipu)\n",
    "            return self\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, labels=None):\n",
    "        \n",
    "        seq_len = input_ids.shape[1]\n",
    "        attention_mask = attention_mask[:, None, :].repeat(1, seq_len, 1)\n",
    "        attention_mask = (attention_mask == attention_mask.transpose(1, 2)) * (attention_mask != 0)\n",
    "        \n",
    "        output = super().forward(input_ids = input_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 token_type_ids=token_type_ids,\n",
    "                                 position_ids=position_ids,\n",
    "                                 labels=labels)\n",
    "\n",
    "        # For validation: output should keep the same batch dimension as the original input\n",
    "        if not self.training:\n",
    "            output.logits = output.logits.reshape([-1,max_seq_per_pack, num_labels])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PackedBertForSequenceClassification(config).from_pretrained(\"bert-base-uncased\", num_labels=num_labels).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "The warning is telling us we are throwing away some weights and randomly initializing some other. This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first test the model on CPU and observe that the output logits have now the size [batch_size x max_seq_per_pack, 2] = [12, 2] with this notebook default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(packed_train_dataset,\n",
    "                             batch_size=micro_batch_size,\n",
    "                             shuffle=True,\n",
    "                             drop_last=True,\n",
    "                             collate_fn=default_data_collator)\n",
    "data = iter(loader).next()\n",
    "outputs = model(**data)\n",
    "print(\"logits: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare the model for IPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set the model in half precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "We need to define the `IPUConfig`, which is a class that specifies attributes and configuration parameters to compile and put the model on the device. We initialize it with one config name or path, which we set earlier. Then we use it to set the mode attribute `model.ipu_config` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config = IPUConfig.from_pretrained(\n",
    "    ipu_config_name,\n",
    "    executable_cache_dir = \"/tmp/exe_cache/\",\n",
    "    replication_factor=1,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    device_iterations = 32,\n",
    "    inference_replication_factor=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation, we need to define a function to compute the metrics from the predictions, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B). To ignore the `-100` labels from uncomplete packs, we use a boolean mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "#     Remove the padding labels\n",
    "    mask = (labels != -100)\n",
    "    labels = labels[mask]\n",
    "    predictions = predictions[mask]\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = IPUTrainingArguments(\n",
    "    \"/tmp/\"+f\"{model_name}-finetuned-{task}\",\n",
    "    learning_rate=0.00009,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0,\n",
    "    metric_for_best_model=metric_name,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_mode=\"async_rebatched\",\n",
    "    logging_steps=1,\n",
    "    pod_type=pod_type,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    push_to_hub=False,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    model,\n",
    "    ipu_config,\n",
    "    args,\n",
    "    train_dataset=packed_train_dataset,\n",
    "    eval_dataset=packed_val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***About the performances:*** `IPUTrainer` doesn't take into account that we have packed data samples when computing the speed metrics. So the actual throughput estimation can be obtained by multiplying the `samples_per_second` by the average packing factor of the dataset. (These were obtained in the `packing_algorithm` section: `5.15` for `sst2` training set and `5.77` for validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard).\n",
    "\n",
    "You can now upload the result of the training to the Hub, just execute this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"sgugger/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, the packing factor does affect the convergence the same way as bigger batch size would do. However, for inference, we are free to use a bigger packing factor to speed it up.\n",
    "Let's try it on `sst2` with `max_seq_per_pack = 12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_per_pack = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have enough examples, we will reuse the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "inference_dataset = encoded_dataset['train'] # train set again, to have enough examples\n",
    "infer_strategy = SPFHP(train_hist, max_seq_length, max_seq_per_pack)\n",
    "packed_dataset = create_dataset_from_strategy(train_dataset, infer_strategy[0], infer_strategy[1], max_seq_length, max_seq_per_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the average packing factor `6.7` is not close to the maximum now (12), this is still an imporvement compared to the previous `5.7`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also modify the configuration of the model for inference. For speed up, we can us a single IPU and 4 replicas by changing `layers_per_ipu` , `inference_replication_factor` and `ipus_per_replica` and also use a larger `batch-size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config.layers_per_ipu = [12]\n",
    "ipu_config.inference_device_iterations = 32\n",
    "ipu_config.inference_replication_factor = 4\n",
    "ipu_config.ipus_per_replica = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PackedBertForSequenceClassification(config).from_pretrained(\"bert-base-uncased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = IPUTrainingArguments(\n",
    "    \"/tmp/\"+f\"{model_name}-finetuned-{task}-fast-inference\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_mode=\"async_rebatched\",\n",
    "    dataloader_drop_last=True,\n",
    "    logging_steps=10,\n",
    "    pod_type=pod_type\n",
    ")\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    model,\n",
    "    ipu_config,\n",
    "    args,\n",
    "    eval_dataset=packed_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, to get a correct throughput estimation we need to multiply `eval_samples_per_second` by the average packing factor (6.72)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Text Classification on GLUE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
