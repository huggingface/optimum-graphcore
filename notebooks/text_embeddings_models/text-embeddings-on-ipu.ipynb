{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13f588db-977d-487c-aae5-1f6a76ff5376",
   "metadata": {},
   "source": [
    "# General-purpose text embeddings on the IPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "781dcd25-64d4-4380-b401-3478d41074a7",
   "metadata": {},
   "source": [
    "This notebook describes how to use supported embeddings models to generate SOTA text embeddings on the IPU. You can use the:\n",
    "* [E5 model](https://arxiv.org/pdf/2212.03533.pdf) (Emb**E**ddings from bidir**E**ctional **E**ncoder r**E**presentations) to generate text embeddings on the IPU.\n",
    "* [Sentence Transformers MPNet Base V2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) An embeddings model based on the MPNet base model.\n",
    "* [Sentence-T5](https://arxiv.org/abs/2108.08877) runs on a pretrained T5 model encoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d591cc4-38ca-47ae-a52f-165b04deb0a8",
   "metadata": {},
   "source": [
    "First, install the requirements for running this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a8017-830e-45fd-9a14-76661bd61424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install sentence-transformers\n",
    "! pip install --find-links https://download.pytorch.org/whl/cpu/torch_stable.html torch==2.0.1+cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2caff0d-c9dd-426b-a62f-08e53ff1c201",
   "metadata": {},
   "source": [
    "Import the required modules for the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb86be2-6ba9-48dc-a948-6009dcf617e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import poptorch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "885c52bb-63fc-4c3d-9ee0-40ae4a1c02f5",
   "metadata": {},
   "source": [
    "We need to instantiate some global parameters that will be used to run the models.\n",
    "\n",
    "The **micro batch size** (number of batches to process in parallel) is set to a smaller value of 2 due to its greater effect on device memory. \n",
    "\n",
    "We use on-IPU loops (**device iterations**) which iterate over a number of batches sequentially (where the iteration takes place on-device in one dataloader call), to extend the batch size for more throughput benefit (this is more efficient than loading smaller batches on the host a large number of times). \n",
    "\n",
    "Data parallelism is controlled by the **replication factor**, i.e. how many devices the batch sizes are replicated over. This value is set to `None` by default as it will be automatically determined by the `pod_type` of the machine being used. By default, the model itself requires 1 IPU to run, and if running on a IPU POD4 (4 IPU) machine, the replication factor is set to 4. Similarly, if running on an IPU POD16, it is set to 16. This can be overidden with a different value if needed, i.e., if `replication_factor=N` the model will be replicated over `N` IPUs as long as `N * n_ipu (number of IPUs a single instance of the model uses) <= total available IPUs`.\n",
    "\n",
    "The total effective batch size for inference is calculated by:\n",
    "```\n",
    "effective_batch_size = replication_factor * device_iterations * micro_batch_size\n",
    "```\n",
    "\n",
    "The model itself, through model pipelining, can also be run over 4 IPUs (by setting `model_ipu` to 4), in which case the replication factor will be adjusted accordingly. The reason we might want to spread the model over more IPUs is to reduce the memory consumption of the model over a single machine (e.g., with 4 IPUs, we compute far fewer layers per IPU, while with 1 IPU, all model layers are on a single IPU) allowing for higher batch sizes to be used. This is particularly beneficial on an IPU POD16 machine, as the 4-IPU pipelined version of the model can be run at a higher effective batch size (with higher micro batch size) and achieve even higher overall batched throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e503c3df-45ce-4613-ac7c-3b38a48ed40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"\")\n",
    "\n",
    "n_ipu = os.getenv(\"NUM_AVAILABLE_IPU\", 4)\n",
    "model_ipu = 1\n",
    "micro_batch_size = 2\n",
    "device_iterations = 256\n",
    "replication_factor = None\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4f21a35-67c8-41b9-a8bd-eaa35f04c054",
   "metadata": {},
   "source": [
    "To run embeddings models, we will set up a generic IPU embeddings class which loads the pretrained model onto the IPU and runs the embedding pooling and normalisation stages in the forward pass along with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ebea39-a859-4737-8687-b10ef5916046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "from typing import Optional, List\n",
    "\n",
    "from transformers import AutoModel\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "from optimum.graphcore import IPUConfig\n",
    "\n",
    "logger = logging.getLogger(\"e5\")\n",
    "\n",
    "class IPUEmbeddingsModel(torch.nn.Module):\n",
    "    def __init__(self, model, ipu_config: IPUConfig, fp16=True):\n",
    "        super().__init__()\n",
    "        self.encoder = to_pipelined(model, ipu_config)\n",
    "        self.encoder = self.encoder.parallelize()\n",
    "        if fp16: self.encoder = self.encoder.half()\n",
    "    \n",
    "    def pool(self, last_hidden_states: torch.Tensor, attention_mask: torch.Tensor, pool_type: str) -> torch.Tensor:\n",
    "             \n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    \n",
    "        if pool_type == \"avg\":\n",
    "            emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "        elif pool_type == \"cls\":\n",
    "            emb = last_hidden[:, 0]\n",
    "        else:\n",
    "            raise ValueError(f\"pool_type {pool_type} not supported\")\n",
    "\n",
    "        return emb\n",
    "    \n",
    "    def forward(self, pool_type: str ='avg', **kwargs) -> torch.Tensor:\n",
    "        outputs = self.encoder(**kwargs)\n",
    "        \n",
    "        embeds = self.pool(outputs.last_hidden_state, kwargs[\"attention_mask\"], pool_type=pool_type)\n",
    "        embeds = torch.nn.functional.normalize(embeds, p=2, dim=-1)\n",
    "\n",
    "        return embeds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e031ce8-5f31-4ba5-8cc6-dce38963b08f",
   "metadata": {},
   "source": [
    "Before setting up and running each of the models, lets create a simple `infer` function which handles loading the batches from a dataloader and generating the embeddings from any model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1891ce31-b349-4393-93dc-588f09a1a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def infer(model, dataloader):\n",
    "    encoded_embeds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_dict in tqdm(dataloader, desc='encoding'):\n",
    "            lat = time.time()\n",
    "            outputs = model(**batch_dict)\n",
    "            lat = time.time() - lat\n",
    "            \n",
    "            encoded_embeds.append(outputs)\n",
    "            print(f\"batch len: {len(batch_dict['input_ids'])} | batch latency: {lat}s | per_sample: {lat/len(batch_dict['input_ids'])}s | throughput: {len(batch_dict['input_ids'])/lat} samples/s\")\n",
    "    \n",
    "    return torch.cat(encoded_embeds, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c14b1389-0bec-4b79-9d8e-feb31b386a35",
   "metadata": {},
   "source": [
    "## Embeddings with E5-Large"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbaf200e-2375-4d58-aa41-d64daf8017ad",
   "metadata": {},
   "source": [
    "First, use Transformers' `AutoConfig` to load the model config for the E5 large model. E5 uses a bidirectional encoder, essentially the encoder stage of a BERT model, to generate the trained embeddings. The config will define the architecture of the model, such as the number of encoder layers and size of the hidden dimension within the model.\n",
    "\n",
    "We also need to tokenize the dataset, for this we define a custom transform function which applies the pre-trained tokenization for each model to the dataset. We will call this function when loading the function, to avoid loading multiple tokenized datasets at the same time.\n",
    "\n",
    "We define some IPU specific configurations to get the most out of the model, the `get_ipu_config` function will set up the IPU config according to the model config, taking into consideration the defined number of IPUs for model parallelism, the number of IPUs available and batching configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81185f5b-6887-415f-b82f-2170fe22d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_ipu_config\n",
    "from transformers import AutoConfig, AutoTokenizer, BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e140bc-d2e2-4da3-a53d-3532094554fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/optimum/graphcore/ipu_configuration.py:403: UserWarning: The \"enable_half_first_order_momentum\" parameter is deprecated\n",
      "  warnings.warn('The \"enable_half_first_order_momentum\" parameter is deprecated')\n"
     ]
    }
   ],
   "source": [
    "e5_model_name = 'intfloat/e5-large'\n",
    "e5_tokenizer = AutoTokenizer.from_pretrained(e5_model_name)\n",
    "e5_model_config = AutoConfig.from_pretrained(e5_model_name)\n",
    "e5_model = AutoModel.from_pretrained(e5_model_name, config=e5_model_config)\n",
    "\n",
    "def e5_transform_func(example) -> BatchEncoding:\n",
    "    return e5_tokenizer(\n",
    "        example['text'],\n",
    "        max_length=e5_model_config.max_position_embeddings,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "e5_ipu_config = get_ipu_config(\n",
    "    e5_model_config, n_ipu, model_ipu, device_iterations, replication_factor, random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "302901d1-65c9-4634-bf09-d5aa20edddce",
   "metadata": {},
   "source": [
    "## Embeddings with All-MPNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e64f65d-ab11-4c06-a999-049bdd1d80ae",
   "metadata": {},
   "source": [
    "For MPNet, we do the same for the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d2754d-c2ec-48a0-a0bf-1021914237e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "mpnet_tokenizer = AutoTokenizer.from_pretrained(mpnet_model_name)\n",
    "mpnet_model_config = AutoConfig.from_pretrained(mpnet_model_name)\n",
    "mpnet_model = AutoModel.from_pretrained(mpnet_model_name, config=mpnet_model_config)\n",
    "\n",
    "def mpnet_transform_func(example) -> BatchEncoding:\n",
    "    return mpnet_tokenizer(\n",
    "        example['text'],\n",
    "        max_length=mpnet_model_config.max_position_embeddings,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "mpnet_ipu_config = get_ipu_config(\n",
    "    mpnet_model_config, n_ipu, model_ipu, device_iterations, replication_factor, random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a48b9f43-c49a-41e4-b2c1-17b0307110fb",
   "metadata": {},
   "source": [
    "## Embeddings with Sentence-T5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20c34ab0-f865-4052-b772-1234d66e74f2",
   "metadata": {},
   "source": [
    "Note that for T5, we need to use the `T5EncoderModel` instead of `AutoModel`. We must manually specify the encoder as T5 is an encoder-decoder model, and we don't want to load the decoder for embeddings generation. \n",
    "\n",
    "Transformers `AutoModel` supports a 1-to-1 mapping of architecture definitions to model types, and it will by default load the `T5Model` class. We can override this by directly importing and loading the pretrained model using `T5EncoderModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a23a16-2eee-42b0-9ff3-4444591e61e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`replicated_tensor_sharding` is not used when `replication_factor=1`\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.t5.modeling_t5 import T5EncoderModel\n",
    "\n",
    "t5_model_name = 'sentence-transformers/sentence-t5-base'\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "t5_model_config = AutoConfig.from_pretrained(t5_model_name)\n",
    "t5_model = T5EncoderModel.from_pretrained(t5_model_name, config=t5_model_config)\n",
    "\n",
    "def t5_transform_func(example) -> BatchEncoding:\n",
    "    return t5_tokenizer(\n",
    "        example['text'],\n",
    "        max_length=t5_model_config.n_positions,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "t5_ipu_config = get_ipu_config(\n",
    "    t5_model_config, n_ipu, model_ipu, device_iterations, replication_factor, random_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f66faa5-43eb-4e26-9347-f039baf63411",
   "metadata": {},
   "source": [
    "## Creating the embeddings model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12b40f13-ccc2-423f-b2f3-b23867dae7d7",
   "metadata": {},
   "source": [
    "We'll wrap this behaviour into a simple function so we can iteratively run all three models and initialise the `poptorch.Dataloader` to create an IPU-ready batched dataloader. We pass an arbitrary call to the model using the first batch to ensure we have compiled the model executable (or loaded the already compiled executable).\n",
    "\n",
    "The function goes through the model and dataset setup for a given model:\n",
    "1. Initialise the `IPUEmbeddingsModel` class with the loaded model and IPU config.\n",
    "2. Convert the IPU config into an IPU options object and pass this to a `poptorch.inferenceModel` wrapper to prepare the model for the IPU.\n",
    "3. Initialise the [`poptorch.Dataloader`](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html) to batch the data according to the IPU options.\n",
    "4. Run the model once with a batch to compile or load the compiled executable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9d71a3-7b02-4a0e-8056-aa34f8777d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator as data_collator\n",
    "\n",
    "def create_model(model, ipu_config, dataset):\n",
    "    model = IPUEmbeddingsModel(model, ipu_config)\n",
    "\n",
    "    ipu_options = ipu_config.to_options(for_inference=True)\n",
    "    model = poptorch.inferenceModel(model, ipu_options)\n",
    "\n",
    "    dataloader = poptorch.DataLoader(\n",
    "        ipu_options,\n",
    "        dataset['train'],\n",
    "        batch_size=micro_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        num_workers=2,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    model(**next(iter(dataloader)))\n",
    "    return model, dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ebc664-eba6-4ee9-97e1-c75e024d5bf1",
   "metadata": {},
   "source": [
    "Lets load a dataset we'll use to try out the models. Using the Hugging Face `datasets` library we can load a pre-existing dataset from the Hugging Face Hub. In this case, lets use the `rotten_tomatoes` film review dataset. Later in the notebook, we will use this dataset to perform create a basic semantic search functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "416c6965-a064-4808-b804-3cac3b8b0e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3dc81335404744b670c9b27af3ff77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7174614c-befa-4822-960d-8c8146f2c6e8",
   "metadata": {},
   "source": [
    "## Run the E5 model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dacec9c0-1a24-4db7-938b-34fd28864a2c",
   "metadata": {},
   "source": [
    "The dataset first needs to be tokenized using the pre-trained tokenizer for each model, we can use the `map()` method to tokenize each of the inputs of the dataset using the model-specific transform function. Then we can convert the Hugging Face Arrow format dataset to a Pytorch ready dataset with `set_format` which converts the tokenized inputs into tensors.\n",
    "\n",
    "To run the model, Simply call the `infer` function we created earlier to generate embeddings for the full dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9d4bb7-cfec-4251-8fe4-1d51c9873b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-68d5b7a8dc262139.arrow\n",
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-38a2073251df1e1f.arrow\n",
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-38d4e65763a34107.arrow\n",
      "Graph compilation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45d0a6cacdb42e184debce7752aa52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoding:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch len: 2048 | batch latency: 2.38167405128479s | per_sample: 0.0011629267828539014s | throughput: 859.8993631791092 samples/s\n",
      "batch len: 2048 | batch latency: 2.387167453765869s | per_sample: 0.0011656091082841158s | throughput: 857.9205437679637 samples/s\n",
      "batch len: 2048 | batch latency: 2.371131420135498s | per_sample: 0.0011577790137380362s | throughput: 863.7226863970986 samples/s\n",
      "batch len: 2048 | batch latency: 2.381141424179077s | per_sample: 0.00116266671102494s | throughput: 860.0917103048884 samples/s\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(e5_transform_func, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "model, dataloader = create_model(e5_model, e5_ipu_config, tokenized_dataset)\n",
    "\n",
    "e5_data_embeddings = infer(model, dataloader)\n",
    "\n",
    "model.detachFromDevice()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "287cc882-a5f9-4506-badb-341563805c52",
   "metadata": {},
   "source": [
    "## Run the All-MPNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71988253-9683-426c-9808-a88d5b13ed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-1a69839fde2119f9.arrow\n",
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-5a87b4ded56026b1.arrow\n",
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-381d045f41dd6c09.arrow\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(mpnet_transform_func, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m tokenized_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m model, dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmpnet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmpnet_ipu_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m mpnet_data_embeddings \u001b[38;5;241m=\u001b[39m infer(model, dataloader)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mdetachFromDevice()\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model, ipu_config, dataset)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(model, ipu_config, dataset):\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mIPUEmbeddingsModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mipu_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     ipu_options \u001b[38;5;241m=\u001b[39m ipu_config\u001b[38;5;241m.\u001b[39mto_options(for_inference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m poptorch\u001b[38;5;241m.\u001b[39minferenceModel(model, ipu_options)\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mIPUEmbeddingsModel.__init__\u001b[0;34m(self, model, ipu_config, fp16)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, ipu_config: IPUConfig, fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mto_pipelined\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mipu_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mparallelize()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fp16: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mhalf()\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/optimum/graphcore/modeling_utils.py:58\u001b[0m, in \u001b[0;36mto_pipelined\u001b[0;34m(model, ipu_config, force)\u001b[0m\n\u001b[1;32m     56\u001b[0m pipelined_cls \u001b[38;5;241m=\u001b[39m _PRETRAINED_TO_PIPELINED_REGISTRY\u001b[38;5;241m.\u001b[39mget(model_cls, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipelined_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipelined_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_transformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mipu_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# If the user defined his/her own model and already subclassed from PipelineMixin. I.e., the model is already pipelined.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, PipelineMixin):\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/optimum/graphcore/modeling_utils.py:93\u001b[0m, in \u001b[0;36mPipelineMixin.from_transformers\u001b[0;34m(cls, model, ipu_config)\u001b[0m\n\u001b[1;32m     91\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     92\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model\u001b[38;5;241m.\u001b[39mgeneration_config)\n\u001b[0;32m---> 93\u001b[0m pipelined_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m pipelined_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m generation_config\n\u001b[1;32m     95\u001b[0m pipelined_model\u001b[38;5;241m.\u001b[39mload_state_dict(model\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/optimum/graphcore/models/mpnet/modeling_mpnet.py:110\u001b[0m, in \u001b[0;36mPipelinedMPNetModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/transformers/models/mpnet/modeling_mpnet.py:493\u001b[0m, in \u001b[0;36mMPNetModel.__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;241m=\u001b[39m MPNetPooler(config) \u001b[38;5;28;01mif\u001b[39;00m add_pooling_layer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/transformers/modeling_utils.py:1090\u001b[0m, in \u001b[0;36mPreTrainedModel.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost_init\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;124;03m    A method executed at the end of each Transformer model initialization, to execute code that needs the model's\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124;03m    modules properly initialized (such as weight initialization).\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1090\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_compatibility_gradient_checkpointing()\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/transformers/modeling_utils.py:1581\u001b[0m, in \u001b[0;36mPreTrainedModel.init_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpruned_heads)\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_weights:\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;66;03m# Initialize weights\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;66;03m# Tie weights should be skipped when not initializing all weights\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;66;03m# since from_pretrained(...) calls tie weights anyways\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/torch/nn/modules/module.py:884\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 884\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/torch/nn/modules/module.py:884\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 884\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.apply at line 884 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/torch/nn/modules/module.py:884\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 884\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/torch/nn/modules/module.py:885\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    884\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m--> 885\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/transformers/modeling_utils.py:1242\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hf_initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m module\u001b[38;5;241m.\u001b[39m_is_hf_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/transformers/models/mpnet/modeling_mpnet.py:63\u001b[0m, in \u001b[0;36mMPNetPreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the weights\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Slightly different from the TF version which uses truncated_normal for initialization\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# cf https://github.com/pytorch/pytorch/pull/5617\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializer_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         module\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(mpnet_transform_func, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "model, dataloader = create_model(mpnet_model, mpnet_ipu_config, tokenized_dataset)\n",
    "\n",
    "mpnet_data_embeddings = infer(model, dataloader)\n",
    "\n",
    "model.detachFromDevice()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6fee156-4846-4acf-82f8-b443ef149319",
   "metadata": {},
   "source": [
    "## Run the Sentence-T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4eff08-cd23-4ec8-aa91-9406c0ffcbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-ec008d65d286c61e.arrow\n",
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-c91446b275feccba.arrow\n",
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-d524898cfa86fa16.arrow\n",
      "Graph compilation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b0ed90b8cc47deb5fe8a446be25d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoding:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch len: 2048 | batch latency: 1.2537388801574707s | per_sample: 0.00061217718757689s | throughput: 1633.5139895659688 samples/s\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(t5_transform_func, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "model, dataloader = create_model(t5_model, t5_ipu_config, tokenized_dataset)\n",
    "\n",
    "t5_data_embeddings = infer(model, dataloader)\n",
    "\n",
    "model.detachFromDevice()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "353baa3c-39ce-4e6f-9500-d3cc3831c608",
   "metadata": {},
   "source": [
    "The embeddings for a single sequence represent low-dimensional numerical representations of the word-level and sentence-level context for each token. These pre-trained embeddings can be used in applications like embedding retrieval for recommender systems, or semantic search for query-matching using cosine-similarity. Both of these use cases take advantage of the generated embeddings space, by performing a relative comparison of the user input sequence embeddings using some proximity metric.\n",
    "\n",
    "We'll use the open source `sentence_transformers` library which provides utilities for embeddings tasks to perform semantic search on a user query to retrieve the most similar sequences from the dataset to the query. This is a helpful utility for making, for example, more responsive FAQs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cd0341f-1ddf-4ddd-a224-663ca6c5d290",
   "metadata": {},
   "source": [
    "## Semantic search with generated embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10e889bd-47b4-4fc6-805f-deffe287b717",
   "metadata": {},
   "source": [
    "Using the `rotten_tomatoes` dataset, lets create a simple similarity search engine using `sentence_transformers` semantic search function, which uses cosine similarity to retrieve close-proximity sentences from a given set of embeddings to a given query. We have already generated embeddings for the dataset, so the next step is to do the same with a given query and perform the search.\n",
    "\n",
    "First, to process the query, we need to tokenize it and convert it to a single-batch input for the model. This has been wrapped into a simple function which tokenizes and prepares a dictionary of model inputs (`input_ids`, `attention_mask`, etc.,) to which we just need to pass a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efe05fe-8e30-4d17-ade7-e92df76fd6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_query(query: str):\n",
    "    t_query = mpnet_tokenizer(\n",
    "            query,\n",
    "            max_length=mpnet_model_config.max_position_embeddings,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    return {k: torch.as_tensor([t_query[k]]) for k in t_query}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "001da157-186d-484e-abd8-b3373063deba",
   "metadata": {},
   "source": [
    "Next, to perform inference with a single input (i.e., effective batch size of 1) we re-instantiate the model (for this example, lets use the All-MPNet model) by setting all device batching, replication and micro batch-size to 1 and re-compile the model. The change in batch size necessitates a recompilation, since the input shape to the model has been changed. We will follow the steps to initiate the model outlined earlier in the notebook, with the only change being setting the `get_ipu_config` function to have all batching turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81edc13-236e-4577-bbf4-01904cc09777",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_infer_ipu_config = get_ipu_config(\n",
    "    mpnet_model_config, n_ipu, model_ipu=1, device_iterations=1, replication_factor=1, random_seed=random_seed)\n",
    "\n",
    "model = IPUEmbeddingsModel(mpnet_model, mpnet_infer_ipu_config)\n",
    "model = poptorch.inferenceModel(model, mpnet_infer_ipu_config.to_options(for_inference=True))\n",
    "\n",
    "o=model(**prepare_query(\"Running once to compile\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f1a9655-f6c4-438b-845d-0bf9c954e8dd",
   "metadata": {},
   "source": [
    "Finally, we can use the model to embed a single query, and perform a semantic search across the full dataset embeddings to retrieve highly relevant reviews to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb8f74-3bc0-41a6-afc0-0d87d3cfd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "query = \"Strongly disliked this action movie\"\n",
    "\n",
    "query_embeddings = model(**prepare_query(query))\n",
    "hits = semantic_search(query_embeddings.float(), mpnet_data_embeddings.float(), top_k=10)\n",
    "\n",
    "print(f\"\\n SEARCH QUERY: {query}\")\n",
    "for n, res in enumerate(hits[0]):\n",
    "    print(f\"\\n Result (rank {n+1}) | Score: {res['score']} | Text: {dataset['train']['text'][res['corpus_id']]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4e3d6-640e-451c-b176-21b803bb7ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.detachFromDevice()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
