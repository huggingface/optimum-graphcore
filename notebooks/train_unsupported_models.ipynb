{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "First of all, make sure your environment has installed the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install optimum[graphcore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the versions of Transformers and Optimum Graphcore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import optimum.graphcore\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(optimum.graphcore.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3KD3WXU3l-O"
   },
   "source": [
    "# Train a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAscNNUD3l-P"
   },
   "source": [
    "In this notebook, we'll see how to train a model that is not supported by Optimum Graphcore and not even in [ðŸ¤— Transformers](https://github.com/huggingface/transformers) on a language modeling task.\n",
    "\n",
    "We will see how to easily load and preprocess the dataset for each one of those tasks, and how to use the `IPUTrainer` API to train a model on it.\n",
    "\n",
    "This notebooks assumes you have trained a tokenizer on the corpus you are using, see the [How to train a tokenizer](https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb) notebook ([open in colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r_n9OWV3l-Q"
   },
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kswRMhPc3l-Q"
   },
   "source": [
    "For each of those tasks, we will use the [Wikitext 2]() dataset as an example. You can load it very easily with the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n2ZRs1cL3l-R",
    "outputId": "11151c56-be90-4d11-e7df-db85e745ca5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/localdata/jincheng/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b04aa438a04b21a30cbcbed565df78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEA1ju653l-p"
   },
   "source": [
    "## Causal Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-WGBCO343l-q"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"gpt2\"\n",
    "tokenizer_checkpoint = \"gpt2\"\n",
    "\n",
    "ipu_config_name = \"Graphcore/gpt2-small-ipu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5io6fY_d3l-u"
   },
   "source": [
    "To tokenize all our texts with the same vocabulary that was used when training the model, we have to download a pretrained tokenizer. This is all done by the `AutoTokenizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iAYlS40Z3l-v"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpOiBrJ13l-y"
   },
   "source": [
    "We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that call the tokenizer on our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lS2m25YM3l-z"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9xVAa3s3l-2"
   },
   "source": [
    "Then we apply it to all the splits in our `datasets` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NVAO0H8u3l-3",
    "outputId": "30d88b8a-e353-4e13-f709-8e5e06ef747b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7fc9e1662ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4186bd2dfad041e4bd6156013ee1cb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85acb5312594302a4c442b66966a346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4269f2462bb43bd82e8009cae811c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a68e270e6345ac8b7e3dee5054db23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44ac36582f74beb9f61daff7157de65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741a43432cd64f4794e26ddd60c5d002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd6875ef7a146d4a4a9c4f1649aeffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9794808069964847b1ba9b0624135cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6e8f416e5d424c8233aa2afcad3874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453088be62574c96870891523e9cc641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a0071381954076896a25d301e592c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80742c186896462097c80638ef54a633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obvgcXda3l--"
   },
   "source": [
    "Then we grab the maximum length our model was pretrained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DVHs5aCA3l-_"
   },
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpNfGiMw3l_A"
   },
   "source": [
    "Then we write the preprocessing function that will group our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iaAJy5Hu3l_B"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGJWXtNv3l_C"
   },
   "source": [
    "Again we apply it to all the splits in our `datasets` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gXUSfBrq3l_C",
    "outputId": "34e55885-3d8f-4f05-cbdb-706ce56a25f8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c608b419173947779124553b8d17749b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a57b0aea4d450a9ed536faf276f3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e4b772201c4914b2c0632850152f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bb567291fa43f29f7cf58a0aa05514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6f4f53d0a04003bd74a57f5fac8687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c04b78da9f54a138070143e68e071fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c741d17cabd4d1da4006dd06b05ce0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fdd18a60954e88ba8fe0bbe1a6143d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cfa70797a84edf925104ed969f707f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cf0815c7704171a668114257560c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d2afd2293549998560246f86f68d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a09f927982d49d3acbc8c9650e0fcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let define a customized model. You might notice that this is just a customized version of GPT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, block_size, vocab_size, d_model, nhead, dim_feedforward, nlayers, dropout=0.1, embd_pdrop=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embeddings = nn.Embedding(block_size, d_model)\n",
    "        self.drop = nn.Dropout(embd_pdrop)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, nlayers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.tie_weights(self.lm_head, self.word_embeddings)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def tie_weights(self, output_embeddings, input_embeddings):\n",
    "        output_embeddings.weight = input_embeddings.weight\n",
    "        output_embeddings.bias.data = nn.functional.pad(\n",
    "            output_embeddings.bias.data,\n",
    "            (\n",
    "                0,\n",
    "                output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
    "            ),\n",
    "            \"constant\",\n",
    "            0,\n",
    "        )\n",
    "        output_embeddings.out_features = input_embeddings.num_embeddings\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, -10000.0).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.word_embeddings.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.position_embeddings.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.lm_head.bias)\n",
    "        nn.init.uniform_(self.lm_head.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        device = input_ids.device\n",
    "        input_shape = input_ids.size()\n",
    "\n",
    "        mask = self._generate_square_subsequent_mask(self.block_size).to(device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_ids = torch.arange(0, input_shape[-1], dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        hidden_states = self.transformer_encoder(hidden_states, mask)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n. Use roll() + ignore_index instead of slicing for better efficiency on IPUs.\n",
    "            labels = torch.roll(labels, -1, 1)\n",
    "            # By default the ignore_index of CrossEntropyLoss is -100\n",
    "            labels[:, -1] = -100\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        output = (lm_logits,)\n",
    "        return (loss,) if loss is not None else output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then subclass the model to inherit from `PipelineMixin`. Then the model will have the `parallelize` and `deparallelize` methods. Here we override the `parallelize` method to customize the optimization. Note that if the model is simple and no customized optimization is needed for the model, there is no need to override the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "from optimum.graphcore.modeling_utils import PipelineMixin, get_layer_ipu, recomputation_checkpoint, register\n",
    "from optimum.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class IPUTransformerModel(TransformerModel, PipelineMixin):\n",
    "    def parallelize(self):\n",
    "        super().parallelize()\n",
    "        logger.info(\"---------- Device Allocation -----------\")\n",
    "        logger.info(\"Embedding  --> IPU 0\")\n",
    "        self.word_embeddings = poptorch.BeginBlock(self.word_embeddings, \"word_embeddings\", ipu_id=0)\n",
    "        self.position_embeddings = poptorch.BeginBlock(self.position_embeddings, \"position_embeddings\", ipu_id=0)\n",
    "\n",
    "        layer_ipu = get_layer_ipu(self.ipu_config.layers_per_ipu)\n",
    "        for index, layer in enumerate(self.transformer_encoder.layers):\n",
    "            if self.ipu_config.recompute_checkpoint_every_layer:\n",
    "                # Put checkpoints on every encoder layer\n",
    "                h = recomputation_checkpoint(layer)\n",
    "                self._hooks.append(h)\n",
    "            ipu = layer_ipu[index]\n",
    "            logger.info(f\"Encoder {index:<2} --> IPU {ipu}\")\n",
    "            self.transformer_encoder.layers[index] = poptorch.BeginBlock(layer, f\"Encoder{index}\", ipu_id=ipu)\n",
    "\n",
    "        logger.info(f\"Head       --> IPU 0\")\n",
    "        logger.info(\"---------------------------------------\")\n",
    "        self.lm_head = poptorch.BeginBlock(self.lm_head, \"lm_head\", ipu_id=0)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IPUTransformerModel(\n",
    "    block_size=block_size,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=768,\n",
    "    nhead=12,\n",
    "    dim_feedforward=768,\n",
    "    nlayers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEmeQ7Xm3l_H"
   },
   "source": [
    "To instantiate an `IPUTrainer`, we first define the `IPUConfig`, which is a class that specifies attributes and configuration parameters to compile and put the model on the device. We initialize it with one config name or path, which we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.graphcore import IPUConfig, IPUTrainer, IPUTrainingArguments\n",
    "\n",
    "ipu_config = IPUConfig.from_pretrained(\"../test_trainer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing we need to define is the `IPUTrainingArguments`, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YbSwEhQ63l_L"
   },
   "outputs": [],
   "source": [
    "micro_batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "pod_type = \"pod16\"\n",
    "\n",
    "training_args = IPUTrainingArguments(\n",
    "    \"mymodel-wikitext2\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    per_device_eval_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    pod_type=pod_type,\n",
    "    num_train_epochs=1,\n",
    "    loss_scaling=16384,\n",
    "    warmup_ratio=0.1,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=64,\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZRbT9ui3l_N"
   },
   "source": [
    "Finally, we pass along all of those to the `IPUTrainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OEuqwIra3l_N",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding IPU config: gradient_accumulation_steps=16\n",
      "Cloning https://huggingface.co/Jinchen/gpt2-wikitext2 into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9775a82a69a6464388de97cc67553bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 15.2k/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654ee803ca634cdca07e71883d653085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 2.55k/2.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9885ce0c2b44e53a77c9ffd3958243a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  39%|###9      | 1.00k/2.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51cc42a0c324be0aa731cc5574fd52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = IPUTrainer(\n",
    "    model=model,\n",
    "    ipu_config=ipu_config,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Vvz34Td3l_O"
   },
   "source": [
    "And we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NyZvu_MF3l_P",
    "outputId": "b69d0931-7f1f-4f2d-fdb8-09d37c7418bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "[16:39:53.411] [poptorch:cpp] [warning] Graph contains an unused input %2 : Long(1, 128, strides=[128, 1], requires_grad=0, device=cpu)\n",
      "[16:39:53.417] [poptorch:cpp] [warning] %2093 : Long(1, strides=[128], requires_grad=0, device=cpu) = aten::fill_(%2092, %908) # /tmp/ipykernel_1576165/3863633953.py:68:0: torch.int64 is not supported natively on IPU, loss of range/precision may occur. We will only warn on the first instance.\n",
      "Graph compilation:   0%|                                                                                                                     | 0/100 [00:00<?]2022-08-19T15:40:00.333998Z popart:devicex 1576165.1576165 W: The `debug.retainDebugInformation` engine option was implicitly set to `true`. The default will change to `false` in a future release. Set it to `true` explicitly if you want to query debug information (for example, by calling `Session::getReport`).\n",
      "Graph compilation:   3%|â–ˆâ–ˆâ–ˆâ–                                                                                                             | 3/100 [00:21<11:46]2022-08-19T15:40:21.161204Z popart:devicex 1576165.1576165 W: The `debug.retainDebugInformation` engine option was implicitly set to `true`. The default will change to `false` in a future release. Set it to `true` explicitly if you want to query debug information (for example, by calling `Session::getReport`).\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:41<00:00]\n",
      "2022-08-19T15:46:43.213375Z popart:session 1576165.1576165 W: Remember that if you would like to run the model using the model runtime then you have to create your own buffer and callback in your model runtime application for rngStateTensor.\n",
      "Compiled/Loaded model in 432.60084040509537 secs\n",
      "***** Running training *****\n",
      "  Num examples = 18666\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Device Iterations = 1\n",
      "  Replication Factor = 4\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Total optimization steps = 2910\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjincheng\u001b[0m (\u001b[33msw-apps\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/localdata/jincheng/optimum-graphcore/notebooks/wandb/run-20220819_164701-2ffvsi73</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.sourcevertex.net/sw-apps/huggingface/runs/2ffvsi73\" target=\"_blank\">gpt2-wikitext2</a></strong> to <a href=\"https://wandb.sourcevertex.net/sw-apps/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc95e1aa6d84a3c8821637452fec066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.0398, 'learning_rate': 6.872852233676977e-07, 'epoch': 0.03}\n",
      "{'loss': 11.9625, 'learning_rate': 1.3745704467353954e-06, 'epoch': 0.07}\n",
      "{'loss': 11.7398, 'learning_rate': 2.061855670103093e-06, 'epoch': 0.1}\n",
      "{'loss': 11.4867, 'learning_rate': 2.7491408934707907e-06, 'epoch': 0.14}\n",
      "{'loss': 11.1125, 'learning_rate': 3.436426116838488e-06, 'epoch': 0.17}\n",
      "{'loss': 10.7312, 'learning_rate': 4.123711340206186e-06, 'epoch': 0.21}\n",
      "{'loss': 10.4102, 'learning_rate': 4.810996563573884e-06, 'epoch': 0.24}\n",
      "{'loss': 10.0617, 'learning_rate': 5.4982817869415815e-06, 'epoch': 0.27}\n",
      "{'loss': 9.7906, 'learning_rate': 6.185567010309279e-06, 'epoch': 0.31}\n",
      "{'loss': 9.6422, 'learning_rate': 6.872852233676976e-06, 'epoch': 0.34}\n",
      "{'loss': 9.3812, 'learning_rate': 7.560137457044674e-06, 'epoch': 0.38}\n",
      "{'loss': 9.3125, 'learning_rate': 8.247422680412371e-06, 'epoch': 0.41}\n",
      "{'loss': 9.1977, 'learning_rate': 8.93470790378007e-06, 'epoch': 0.45}\n",
      "{'loss': 9.1625, 'learning_rate': 9.621993127147768e-06, 'epoch': 0.48}\n",
      "{'loss': 8.8609, 'learning_rate': 1.0309278350515464e-05, 'epoch': 0.52}\n",
      "{'loss': 8.9258, 'learning_rate': 1.0996563573883163e-05, 'epoch': 0.55}\n",
      "{'loss': 8.757, 'learning_rate': 1.168384879725086e-05, 'epoch': 0.58}\n",
      "{'loss': 8.8773, 'learning_rate': 1.2371134020618558e-05, 'epoch': 0.62}\n",
      "{'loss': 8.7711, 'learning_rate': 1.3058419243986255e-05, 'epoch': 0.65}\n",
      "{'loss': 8.6141, 'learning_rate': 1.3745704467353953e-05, 'epoch': 0.69}\n",
      "{'loss': 8.6094, 'learning_rate': 1.443298969072165e-05, 'epoch': 0.72}\n",
      "{'loss': 8.4234, 'learning_rate': 1.5120274914089348e-05, 'epoch': 0.76}\n",
      "{'loss': 8.4328, 'learning_rate': 1.5807560137457047e-05, 'epoch': 0.79}\n",
      "{'loss': 8.2602, 'learning_rate': 1.6494845360824743e-05, 'epoch': 0.82}\n",
      "{'loss': 8.2445, 'learning_rate': 1.7182130584192442e-05, 'epoch': 0.86}\n",
      "{'loss': 8.2371, 'learning_rate': 1.786941580756014e-05, 'epoch': 0.89}\n",
      "{'loss': 8.1977, 'learning_rate': 1.8556701030927837e-05, 'epoch': 0.93}\n",
      "{'loss': 8.0367, 'learning_rate': 1.9243986254295536e-05, 'epoch': 0.96}\n",
      "{'loss': 8.0852, 'learning_rate': 1.9931271477663232e-05, 'epoch': 1.0}\n",
      "{'loss': 8.0012, 'learning_rate': 1.9931271477663232e-05, 'epoch': 1.03}\n",
      "{'loss': 7.9566, 'learning_rate': 1.9854906452844598e-05, 'epoch': 1.07}\n",
      "{'loss': 7.8391, 'learning_rate': 1.9778541428025964e-05, 'epoch': 1.1}\n",
      "{'loss': 7.7621, 'learning_rate': 1.9702176403207334e-05, 'epoch': 1.13}\n",
      "{'loss': 7.7453, 'learning_rate': 1.96258113783887e-05, 'epoch': 1.17}\n",
      "{'loss': 7.7848, 'learning_rate': 1.9549446353570067e-05, 'epoch': 1.2}\n",
      "{'loss': 7.7492, 'learning_rate': 1.9473081328751433e-05, 'epoch': 1.24}\n",
      "{'loss': 7.6223, 'learning_rate': 1.93967163039328e-05, 'epoch': 1.27}\n",
      "{'loss': 7.7055, 'learning_rate': 1.9320351279114166e-05, 'epoch': 1.31}\n",
      "{'loss': 7.6371, 'learning_rate': 1.9243986254295536e-05, 'epoch': 1.34}\n",
      "{'loss': 7.5066, 'learning_rate': 1.91676212294769e-05, 'epoch': 1.37}\n",
      "{'loss': 7.4699, 'learning_rate': 1.909125620465827e-05, 'epoch': 1.41}\n",
      "{'loss': 7.4246, 'learning_rate': 1.9014891179839635e-05, 'epoch': 1.44}\n",
      "{'loss': 7.4832, 'learning_rate': 1.8938526155021e-05, 'epoch': 1.48}\n",
      "{'loss': 7.416, 'learning_rate': 1.886216113020237e-05, 'epoch': 1.51}\n",
      "{'loss': 7.3664, 'learning_rate': 1.8785796105383734e-05, 'epoch': 1.55}\n",
      "{'loss': 7.2488, 'learning_rate': 1.8709431080565104e-05, 'epoch': 1.58}\n",
      "{'loss': 7.3598, 'learning_rate': 1.863306605574647e-05, 'epoch': 1.62}\n",
      "{'loss': 7.3793, 'learning_rate': 1.8556701030927837e-05, 'epoch': 1.65}\n",
      "{'loss': 7.3535, 'learning_rate': 1.8480336006109203e-05, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt2-wikitext2/checkpoint-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2898, 'learning_rate': 1.840397098129057e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Configuration saved in gpt2-wikitext2/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2383, 'learning_rate': 1.8327605956471936e-05, 'epoch': 1.75}\n",
      "{'loss': 7.3582, 'learning_rate': 1.8251240931653306e-05, 'epoch': 1.79}\n",
      "{'loss': 7.2828, 'learning_rate': 1.817487590683467e-05, 'epoch': 1.82}\n",
      "{'loss': 7.2621, 'learning_rate': 1.809851088201604e-05, 'epoch': 1.86}\n",
      "{'loss': 7.1852, 'learning_rate': 1.8022145857197405e-05, 'epoch': 1.89}\n",
      "{'loss': 7.1805, 'learning_rate': 1.794578083237877e-05, 'epoch': 1.92}\n",
      "{'loss': 7.1793, 'learning_rate': 1.786941580756014e-05, 'epoch': 1.96}\n",
      "{'loss': 7.159, 'learning_rate': 1.7793050782741504e-05, 'epoch': 1.99}\n",
      "{'loss': 7.0969, 'learning_rate': 1.7716685757922874e-05, 'epoch': 2.03}\n",
      "{'loss': 7.2242, 'learning_rate': 1.764032073310424e-05, 'epoch': 2.06}\n",
      "{'loss': 7.1527, 'learning_rate': 1.7563955708285607e-05, 'epoch': 2.1}\n",
      "{'loss': 7.1473, 'learning_rate': 1.7487590683466973e-05, 'epoch': 2.13}\n",
      "{'loss': 7.2023, 'learning_rate': 1.741122565864834e-05, 'epoch': 2.16}\n",
      "{'loss': 6.9977, 'learning_rate': 1.7334860633829706e-05, 'epoch': 2.2}\n",
      "{'loss': 7.2086, 'learning_rate': 1.7258495609011075e-05, 'epoch': 2.23}\n",
      "{'loss': 7.0895, 'learning_rate': 1.7182130584192442e-05, 'epoch': 2.27}\n",
      "{'loss': 7.0812, 'learning_rate': 1.7105765559373808e-05, 'epoch': 2.3}\n",
      "{'loss': 7.1805, 'learning_rate': 1.7029400534555175e-05, 'epoch': 2.34}\n",
      "{'loss': 7.1738, 'learning_rate': 1.695303550973654e-05, 'epoch': 2.37}\n",
      "{'loss': 7.025, 'learning_rate': 1.687667048491791e-05, 'epoch': 2.41}\n",
      "{'loss': 7.1867, 'learning_rate': 1.6800305460099274e-05, 'epoch': 2.44}\n",
      "{'loss': 7.0707, 'learning_rate': 1.6723940435280644e-05, 'epoch': 2.47}\n",
      "{'loss': 6.9273, 'learning_rate': 1.664757541046201e-05, 'epoch': 2.51}\n",
      "{'loss': 7.0746, 'learning_rate': 1.6571210385643376e-05, 'epoch': 2.54}\n",
      "{'loss': 7.0207, 'learning_rate': 1.6494845360824743e-05, 'epoch': 2.58}\n",
      "{'loss': 6.8789, 'learning_rate': 1.641848033600611e-05, 'epoch': 2.61}\n",
      "{'loss': 6.9855, 'learning_rate': 1.6342115311187475e-05, 'epoch': 2.65}\n",
      "{'loss': 7.0879, 'learning_rate': 1.6265750286368845e-05, 'epoch': 2.68}\n",
      "{'loss': 7.0949, 'learning_rate': 1.618938526155021e-05, 'epoch': 2.71}\n",
      "{'loss': 6.998, 'learning_rate': 1.6113020236731578e-05, 'epoch': 2.75}\n",
      "{'loss': 7.118, 'learning_rate': 1.6036655211912944e-05, 'epoch': 2.78}\n",
      "{'loss': 7.0156, 'learning_rate': 1.596029018709431e-05, 'epoch': 2.82}\n",
      "{'loss': 6.9887, 'learning_rate': 1.588392516227568e-05, 'epoch': 2.85}\n",
      "{'loss': 7.1281, 'learning_rate': 1.5807560137457047e-05, 'epoch': 2.89}\n",
      "{'loss': 7.0348, 'learning_rate': 1.5731195112638413e-05, 'epoch': 2.92}\n",
      "{'loss': 6.9441, 'learning_rate': 1.565483008781978e-05, 'epoch': 2.96}\n",
      "{'loss': 7.0527, 'learning_rate': 1.5578465063001146e-05, 'epoch': 2.99}\n",
      "{'loss': 6.8582, 'learning_rate': 1.5502100038182512e-05, 'epoch': 3.02}\n",
      "{'loss': 6.9324, 'learning_rate': 1.5425735013363882e-05, 'epoch': 3.06}\n",
      "{'loss': 7.043, 'learning_rate': 1.534936998854525e-05, 'epoch': 3.09}\n",
      "{'loss': 7.0289, 'learning_rate': 1.5273004963726615e-05, 'epoch': 3.13}\n",
      "{'loss': 7.0426, 'learning_rate': 1.5196639938907981e-05, 'epoch': 3.16}\n",
      "{'loss': 6.9062, 'learning_rate': 1.5120274914089348e-05, 'epoch': 3.2}\n",
      "{'loss': 7.0949, 'learning_rate': 1.5043909889270716e-05, 'epoch': 3.23}\n",
      "{'loss': 6.9691, 'learning_rate': 1.4967544864452082e-05, 'epoch': 3.26}\n",
      "{'loss': 6.9148, 'learning_rate': 1.489117983963345e-05, 'epoch': 3.3}\n",
      "{'loss': 7.0082, 'learning_rate': 1.4814814814814815e-05, 'epoch': 3.33}\n",
      "{'loss': 6.943, 'learning_rate': 1.4738449789996183e-05, 'epoch': 3.37}\n",
      "{'loss': 6.9879, 'learning_rate': 1.466208476517755e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt2-wikitext2/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8582, 'learning_rate': 1.4585719740358917e-05, 'epoch': 3.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in gpt2-wikitext2/checkpoint-1000/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0332, 'learning_rate': 1.4509354715540286e-05, 'epoch': 3.47}\n",
      "{'loss': 6.9055, 'learning_rate': 1.443298969072165e-05, 'epoch': 3.51}\n",
      "{'loss': 7.0617, 'learning_rate': 1.4356624665903018e-05, 'epoch': 3.54}\n",
      "{'loss': 6.977, 'learning_rate': 1.4280259641084385e-05, 'epoch': 3.57}\n",
      "{'loss': 6.9223, 'learning_rate': 1.4203894616265753e-05, 'epoch': 3.61}\n",
      "{'loss': 6.918, 'learning_rate': 1.4127529591447117e-05, 'epoch': 3.64}\n",
      "{'loss': 6.9617, 'learning_rate': 1.4051164566628486e-05, 'epoch': 3.68}\n",
      "{'loss': 6.8781, 'learning_rate': 1.3974799541809852e-05, 'epoch': 3.71}\n",
      "{'loss': 6.95, 'learning_rate': 1.389843451699122e-05, 'epoch': 3.75}\n",
      "{'loss': 6.8121, 'learning_rate': 1.3822069492172585e-05, 'epoch': 3.78}\n",
      "{'loss': 6.9035, 'learning_rate': 1.3745704467353953e-05, 'epoch': 3.81}\n",
      "{'loss': 6.759, 'learning_rate': 1.3669339442535321e-05, 'epoch': 3.85}\n",
      "{'loss': 6.9887, 'learning_rate': 1.3592974417716687e-05, 'epoch': 3.88}\n",
      "{'loss': 6.909, 'learning_rate': 1.3516609392898055e-05, 'epoch': 3.92}\n",
      "{'loss': 6.9313, 'learning_rate': 1.344024436807942e-05, 'epoch': 3.95}\n",
      "{'loss': 6.9352, 'learning_rate': 1.3363879343260788e-05, 'epoch': 3.99}\n",
      "{'loss': 6.7941, 'learning_rate': 1.3287514318442154e-05, 'epoch': 4.02}\n",
      "{'loss': 6.8512, 'learning_rate': 1.3211149293623523e-05, 'epoch': 4.05}\n",
      "{'loss': 6.8223, 'learning_rate': 1.3134784268804887e-05, 'epoch': 4.09}\n",
      "{'loss': 6.7922, 'learning_rate': 1.3058419243986255e-05, 'epoch': 4.12}\n",
      "{'loss': 6.8809, 'learning_rate': 1.2982054219167622e-05, 'epoch': 4.16}\n",
      "{'loss': 6.8133, 'learning_rate': 1.290568919434899e-05, 'epoch': 4.19}\n",
      "{'loss': 6.7645, 'learning_rate': 1.2829324169530358e-05, 'epoch': 4.23}\n",
      "{'loss': 6.7648, 'learning_rate': 1.2752959144711723e-05, 'epoch': 4.26}\n",
      "{'loss': 6.7375, 'learning_rate': 1.267659411989309e-05, 'epoch': 4.3}\n",
      "{'loss': 6.8934, 'learning_rate': 1.2600229095074457e-05, 'epoch': 4.33}\n",
      "{'loss': 6.8066, 'learning_rate': 1.2523864070255825e-05, 'epoch': 4.36}\n",
      "{'loss': 6.9898, 'learning_rate': 1.244749904543719e-05, 'epoch': 4.4}\n",
      "{'loss': 6.8023, 'learning_rate': 1.2371134020618558e-05, 'epoch': 4.43}\n",
      "{'loss': 7.0219, 'learning_rate': 1.2294768995799924e-05, 'epoch': 4.47}\n",
      "{'loss': 6.784, 'learning_rate': 1.2218403970981292e-05, 'epoch': 4.5}\n",
      "{'loss': 7.016, 'learning_rate': 1.2142038946162657e-05, 'epoch': 4.54}\n",
      "{'loss': 6.9934, 'learning_rate': 1.2065673921344025e-05, 'epoch': 4.57}\n",
      "{'loss': 6.8273, 'learning_rate': 1.1989308896525393e-05, 'epoch': 4.6}\n",
      "{'loss': 6.8219, 'learning_rate': 1.191294387170676e-05, 'epoch': 4.64}\n",
      "{'loss': 6.7496, 'learning_rate': 1.1836578846888128e-05, 'epoch': 4.67}\n",
      "{'loss': 6.8488, 'learning_rate': 1.1760213822069492e-05, 'epoch': 4.71}\n",
      "{'loss': 6.875, 'learning_rate': 1.168384879725086e-05, 'epoch': 4.74}\n",
      "{'loss': 6.7758, 'learning_rate': 1.1607483772432227e-05, 'epoch': 4.78}\n",
      "{'loss': 6.9055, 'learning_rate': 1.1531118747613595e-05, 'epoch': 4.81}\n",
      "{'loss': 6.9246, 'learning_rate': 1.145475372279496e-05, 'epoch': 4.85}\n",
      "{'loss': 6.8133, 'learning_rate': 1.1378388697976328e-05, 'epoch': 4.88}\n",
      "{'loss': 6.7656, 'learning_rate': 1.1302023673157694e-05, 'epoch': 4.91}\n",
      "{'loss': 6.857, 'learning_rate': 1.1225658648339062e-05, 'epoch': 4.95}\n",
      "{'loss': 6.9336, 'learning_rate': 1.114929362352043e-05, 'epoch': 4.98}\n",
      "{'loss': 6.909, 'learning_rate': 1.1072928598701795e-05, 'epoch': 5.02}\n",
      "{'loss': 6.8254, 'learning_rate': 1.0996563573883163e-05, 'epoch': 5.05}\n",
      "{'loss': 6.8883, 'learning_rate': 1.092019854906453e-05, 'epoch': 5.09}\n",
      "{'loss': 6.759, 'learning_rate': 1.0843833524245897e-05, 'epoch': 5.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt2-wikitext2/checkpoint-1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8297, 'learning_rate': 1.0767468499427262e-05, 'epoch': 5.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Configuration saved in gpt2-wikitext2/checkpoint-1500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8754, 'learning_rate': 1.069110347460863e-05, 'epoch': 5.19}\n",
      "{'loss': 6.9203, 'learning_rate': 1.0614738449789997e-05, 'epoch': 5.22}\n",
      "{'loss': 6.8203, 'learning_rate': 1.0538373424971365e-05, 'epoch': 5.26}\n",
      "{'loss': 6.7996, 'learning_rate': 1.046200840015273e-05, 'epoch': 5.29}\n",
      "{'loss': 6.7547, 'learning_rate': 1.0385643375334097e-05, 'epoch': 5.33}\n",
      "{'loss': 6.9426, 'learning_rate': 1.0309278350515464e-05, 'epoch': 5.36}\n",
      "{'loss': 6.791, 'learning_rate': 1.0232913325696832e-05, 'epoch': 5.4}\n",
      "{'loss': 6.8102, 'learning_rate': 1.01565483008782e-05, 'epoch': 5.43}\n",
      "{'loss': 6.7168, 'learning_rate': 1.0080183276059565e-05, 'epoch': 5.46}\n",
      "{'loss': 6.8051, 'learning_rate': 1.0003818251240933e-05, 'epoch': 5.5}\n",
      "{'loss': 6.8586, 'learning_rate': 9.927453226422299e-06, 'epoch': 5.53}\n",
      "{'loss': 6.7195, 'learning_rate': 9.851088201603667e-06, 'epoch': 5.57}\n",
      "{'loss': 6.8895, 'learning_rate': 9.774723176785034e-06, 'epoch': 5.6}\n",
      "{'loss': 6.7758, 'learning_rate': 9.6983581519664e-06, 'epoch': 5.64}\n",
      "{'loss': 6.7734, 'learning_rate': 9.621993127147768e-06, 'epoch': 5.67}\n",
      "{'loss': 6.7687, 'learning_rate': 9.545628102329134e-06, 'epoch': 5.7}\n",
      "{'loss': 6.8598, 'learning_rate': 9.4692630775105e-06, 'epoch': 5.74}\n",
      "{'loss': 6.8063, 'learning_rate': 9.392898052691867e-06, 'epoch': 5.77}\n",
      "{'loss': 6.8633, 'learning_rate': 9.316533027873235e-06, 'epoch': 5.81}\n",
      "{'loss': 6.7492, 'learning_rate': 9.240168003054602e-06, 'epoch': 5.84}\n",
      "{'loss': 6.8281, 'learning_rate': 9.163802978235968e-06, 'epoch': 5.88}\n",
      "{'loss': 6.8777, 'learning_rate': 9.087437953417334e-06, 'epoch': 5.91}\n",
      "{'loss': 6.7406, 'learning_rate': 9.011072928598702e-06, 'epoch': 5.95}\n",
      "{'loss': 6.8281, 'learning_rate': 8.93470790378007e-06, 'epoch': 5.98}\n",
      "{'loss': 6.7598, 'learning_rate': 8.858342878961437e-06, 'epoch': 6.01}\n",
      "{'loss': 6.7641, 'learning_rate': 8.781977854142803e-06, 'epoch': 6.05}\n",
      "{'loss': 6.7094, 'learning_rate': 8.70561282932417e-06, 'epoch': 6.08}\n",
      "{'loss': 6.6957, 'learning_rate': 8.629247804505538e-06, 'epoch': 6.12}\n",
      "{'loss': 6.6664, 'learning_rate': 8.552882779686904e-06, 'epoch': 6.15}\n",
      "{'loss': 6.7055, 'learning_rate': 8.47651775486827e-06, 'epoch': 6.19}\n",
      "{'loss': 6.7586, 'learning_rate': 8.400152730049637e-06, 'epoch': 6.22}\n",
      "{'loss': 6.8234, 'learning_rate': 8.323787705231005e-06, 'epoch': 6.25}\n",
      "{'loss': 6.7715, 'learning_rate': 8.247422680412371e-06, 'epoch': 6.29}\n",
      "{'loss': 6.5941, 'learning_rate': 8.171057655593738e-06, 'epoch': 6.32}\n",
      "{'loss': 6.648, 'learning_rate': 8.094692630775106e-06, 'epoch': 6.36}\n",
      "{'loss': 6.818, 'learning_rate': 8.018327605956472e-06, 'epoch': 6.39}\n",
      "{'loss': 6.7562, 'learning_rate': 7.94196258113784e-06, 'epoch': 6.43}\n",
      "{'loss': 6.6937, 'learning_rate': 7.865597556319207e-06, 'epoch': 6.46}\n",
      "{'loss': 6.9453, 'learning_rate': 7.789232531500573e-06, 'epoch': 6.49}\n",
      "{'loss': 6.7867, 'learning_rate': 7.712867506681941e-06, 'epoch': 6.53}\n",
      "{'loss': 6.6496, 'learning_rate': 7.636502481863307e-06, 'epoch': 6.56}\n",
      "{'loss': 6.8047, 'learning_rate': 7.560137457044674e-06, 'epoch': 6.6}\n",
      "{'loss': 6.8648, 'learning_rate': 7.483772432226041e-06, 'epoch': 6.63}\n",
      "{'loss': 6.7238, 'learning_rate': 7.4074074074074075e-06, 'epoch': 6.67}\n",
      "{'loss': 6.6633, 'learning_rate': 7.331042382588775e-06, 'epoch': 6.7}\n",
      "{'loss': 6.7953, 'learning_rate': 7.254677357770143e-06, 'epoch': 6.74}\n",
      "{'loss': 6.7879, 'learning_rate': 7.178312332951509e-06, 'epoch': 6.77}\n",
      "{'loss': 6.634, 'learning_rate': 7.101947308132876e-06, 'epoch': 6.8}\n",
      "{'loss': 6.848, 'learning_rate': 7.025582283314243e-06, 'epoch': 6.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt2-wikitext2/checkpoint-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7289, 'learning_rate': 6.94921725849561e-06, 'epoch': 6.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Configuration saved in gpt2-wikitext2/checkpoint-2000/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7098, 'learning_rate': 6.872852233676976e-06, 'epoch': 6.91}\n",
      "{'loss': 6.7488, 'learning_rate': 6.796487208858344e-06, 'epoch': 6.94}\n",
      "{'loss': 6.5922, 'learning_rate': 6.72012218403971e-06, 'epoch': 6.98}\n",
      "{'loss': 6.6934, 'learning_rate': 6.643757159221077e-06, 'epoch': 7.01}\n",
      "{'loss': 6.6152, 'learning_rate': 6.567392134402444e-06, 'epoch': 7.04}\n",
      "{'loss': 6.6137, 'learning_rate': 6.491027109583811e-06, 'epoch': 7.08}\n",
      "{'loss': 6.7324, 'learning_rate': 6.414662084765179e-06, 'epoch': 7.11}\n",
      "{'loss': 6.7504, 'learning_rate': 6.338297059946545e-06, 'epoch': 7.15}\n",
      "{'loss': 6.6051, 'learning_rate': 6.2619320351279125e-06, 'epoch': 7.18}\n",
      "{'loss': 6.5969, 'learning_rate': 6.185567010309279e-06, 'epoch': 7.22}\n",
      "{'loss': 6.5906, 'learning_rate': 6.109201985490646e-06, 'epoch': 7.25}\n",
      "{'loss': 6.8258, 'learning_rate': 6.0328369606720125e-06, 'epoch': 7.29}\n",
      "{'loss': 6.623, 'learning_rate': 5.95647193585338e-06, 'epoch': 7.32}\n",
      "{'loss': 6.6352, 'learning_rate': 5.880106911034746e-06, 'epoch': 7.35}\n",
      "{'loss': 6.7188, 'learning_rate': 5.803741886216113e-06, 'epoch': 7.39}\n",
      "{'loss': 6.6176, 'learning_rate': 5.72737686139748e-06, 'epoch': 7.42}\n",
      "{'loss': 6.6629, 'learning_rate': 5.651011836578847e-06, 'epoch': 7.46}\n",
      "{'loss': 6.7027, 'learning_rate': 5.574646811760215e-06, 'epoch': 7.49}\n",
      "{'loss': 6.773, 'learning_rate': 5.4982817869415815e-06, 'epoch': 7.53}\n",
      "{'loss': 6.5941, 'learning_rate': 5.421916762122949e-06, 'epoch': 7.56}\n",
      "{'loss': 6.5707, 'learning_rate': 5.345551737304315e-06, 'epoch': 7.59}\n",
      "{'loss': 6.7676, 'learning_rate': 5.269186712485682e-06, 'epoch': 7.63}\n",
      "{'loss': 6.6953, 'learning_rate': 5.192821687667049e-06, 'epoch': 7.66}\n",
      "{'loss': 6.7609, 'learning_rate': 5.116456662848416e-06, 'epoch': 7.7}\n",
      "{'loss': 6.6711, 'learning_rate': 5.040091638029782e-06, 'epoch': 7.73}\n",
      "{'loss': 6.6578, 'learning_rate': 4.9637266132111495e-06, 'epoch': 7.77}\n",
      "{'loss': 6.5727, 'learning_rate': 4.887361588392517e-06, 'epoch': 7.8}\n",
      "{'loss': 6.7254, 'learning_rate': 4.810996563573884e-06, 'epoch': 7.84}\n",
      "{'loss': 6.7586, 'learning_rate': 4.73463153875525e-06, 'epoch': 7.87}\n",
      "{'loss': 6.6605, 'learning_rate': 4.658266513936618e-06, 'epoch': 7.9}\n",
      "{'loss': 6.5895, 'learning_rate': 4.581901489117984e-06, 'epoch': 7.94}\n",
      "{'loss': 6.6445, 'learning_rate': 4.505536464299351e-06, 'epoch': 7.97}\n",
      "{'loss': 6.6426, 'learning_rate': 4.4291714394807184e-06, 'epoch': 8.01}\n",
      "{'loss': 6.6848, 'learning_rate': 4.352806414662085e-06, 'epoch': 8.04}\n",
      "{'loss': 6.6953, 'learning_rate': 4.276441389843452e-06, 'epoch': 8.08}\n",
      "{'loss': 6.5496, 'learning_rate': 4.2000763650248184e-06, 'epoch': 8.11}\n",
      "{'loss': 6.6098, 'learning_rate': 4.123711340206186e-06, 'epoch': 8.14}\n",
      "{'loss': 6.6695, 'learning_rate': 4.047346315387553e-06, 'epoch': 8.18}\n",
      "{'loss': 6.7062, 'learning_rate': 3.97098129056892e-06, 'epoch': 8.21}\n",
      "{'loss': 6.5582, 'learning_rate': 3.8946162657502865e-06, 'epoch': 8.25}\n",
      "{'loss': 6.7672, 'learning_rate': 3.818251240931654e-06, 'epoch': 8.28}\n",
      "{'loss': 6.5398, 'learning_rate': 3.7418862161130205e-06, 'epoch': 8.32}\n",
      "{'loss': 6.6645, 'learning_rate': 3.6655211912943874e-06, 'epoch': 8.35}\n",
      "{'loss': 6.5805, 'learning_rate': 3.5891561664757546e-06, 'epoch': 8.38}\n",
      "{'loss': 6.6176, 'learning_rate': 3.5127911416571214e-06, 'epoch': 8.42}\n",
      "{'loss': 6.5629, 'learning_rate': 3.436426116838488e-06, 'epoch': 8.45}\n",
      "{'loss': 6.6906, 'learning_rate': 3.360061092019855e-06, 'epoch': 8.49}\n",
      "{'loss': 6.675, 'learning_rate': 3.283696067201222e-06, 'epoch': 8.52}\n",
      "{'loss': 6.5949, 'learning_rate': 3.2073310423825895e-06, 'epoch': 8.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt2-wikitext2/checkpoint-2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6824, 'learning_rate': 3.1309660175639563e-06, 'epoch': 8.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Configuration saved in gpt2-wikitext2/checkpoint-2500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.607, 'learning_rate': 3.054600992745323e-06, 'epoch': 8.63}\n",
      "{'loss': 6.6441, 'learning_rate': 2.97823596792669e-06, 'epoch': 8.66}\n",
      "{'loss': 6.5324, 'learning_rate': 2.9018709431080567e-06, 'epoch': 8.69}\n",
      "{'loss': 6.5539, 'learning_rate': 2.8255059182894235e-06, 'epoch': 8.73}\n",
      "{'loss': 6.616, 'learning_rate': 2.7491408934707907e-06, 'epoch': 8.76}\n",
      "{'loss': 6.6035, 'learning_rate': 2.6727758686521575e-06, 'epoch': 8.8}\n",
      "{'loss': 6.6469, 'learning_rate': 2.5964108438335243e-06, 'epoch': 8.83}\n",
      "{'loss': 6.7117, 'learning_rate': 2.520045819014891e-06, 'epoch': 8.87}\n",
      "{'loss': 6.7086, 'learning_rate': 2.4436807941962584e-06, 'epoch': 8.9}\n",
      "{'loss': 6.7434, 'learning_rate': 2.367315769377625e-06, 'epoch': 8.93}\n",
      "{'loss': 6.6223, 'learning_rate': 2.290950744558992e-06, 'epoch': 8.97}\n",
      "{'loss': 6.7184, 'learning_rate': 2.2145857197403592e-06, 'epoch': 9.0}\n",
      "{'loss': 6.5652, 'learning_rate': 2.138220694921726e-06, 'epoch': 9.04}\n",
      "{'loss': 6.5812, 'learning_rate': 2.061855670103093e-06, 'epoch': 9.07}\n",
      "{'loss': 6.6457, 'learning_rate': 1.98549064528446e-06, 'epoch': 9.11}\n",
      "{'loss': 6.6777, 'learning_rate': 1.909125620465827e-06, 'epoch': 9.14}\n",
      "{'loss': 6.602, 'learning_rate': 1.8327605956471937e-06, 'epoch': 9.18}\n",
      "{'loss': 6.6887, 'learning_rate': 1.7563955708285607e-06, 'epoch': 9.21}\n",
      "{'loss': 6.6848, 'learning_rate': 1.6800305460099275e-06, 'epoch': 9.24}\n",
      "{'loss': 6.5523, 'learning_rate': 1.6036655211912947e-06, 'epoch': 9.28}\n",
      "{'loss': 6.491, 'learning_rate': 1.5273004963726615e-06, 'epoch': 9.31}\n",
      "{'loss': 6.6348, 'learning_rate': 1.4509354715540283e-06, 'epoch': 9.35}\n",
      "{'loss': 6.6824, 'learning_rate': 1.3745704467353954e-06, 'epoch': 9.38}\n",
      "{'loss': 6.6289, 'learning_rate': 1.2982054219167622e-06, 'epoch': 9.42}\n",
      "{'loss': 6.7504, 'learning_rate': 1.2218403970981292e-06, 'epoch': 9.45}\n",
      "{'loss': 6.6484, 'learning_rate': 1.145475372279496e-06, 'epoch': 9.48}\n",
      "{'loss': 6.7527, 'learning_rate': 1.069110347460863e-06, 'epoch': 9.52}\n",
      "{'loss': 6.634, 'learning_rate': 9.9274532264223e-07, 'epoch': 9.55}\n",
      "{'loss': 6.6059, 'learning_rate': 9.163802978235968e-07, 'epoch': 9.59}\n",
      "{'loss': 6.559, 'learning_rate': 8.400152730049638e-07, 'epoch': 9.62}\n",
      "{'loss': 6.6773, 'learning_rate': 7.636502481863308e-07, 'epoch': 9.66}\n",
      "{'loss': 6.7395, 'learning_rate': 6.872852233676977e-07, 'epoch': 9.69}\n",
      "{'loss': 6.85, 'learning_rate': 6.109201985490646e-07, 'epoch': 9.73}\n",
      "{'loss': 6.5852, 'learning_rate': 5.345551737304315e-07, 'epoch': 9.76}\n",
      "{'loss': 6.6648, 'learning_rate': 4.581901489117984e-07, 'epoch': 9.79}\n",
      "{'loss': 6.6766, 'learning_rate': 3.818251240931654e-07, 'epoch': 9.83}\n",
      "{'loss': 6.7258, 'learning_rate': 3.054600992745323e-07, 'epoch': 9.86}\n",
      "{'loss': 6.641, 'learning_rate': 2.290950744558992e-07, 'epoch': 9.9}\n",
      "{'loss': 6.6258, 'learning_rate': 1.5273004963726615e-07, 'epoch': 9.93}\n",
      "{'loss': 6.6266, 'learning_rate': 7.636502481863307e-08, 'epoch': 9.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.634, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 841.0864, 'train_samples_per_second': 221.428, 'train_steps_per_second': 3.46, 'train_loss': 7.128000161082475, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2910, training_loss=7.128000161082475, metrics={'train_runtime': 841.0864, 'train_samples_per_second': 221.428, 'train_steps_per_second': 3.46, 'train_loss': 7.128000161082475, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3APq-vUc3l_R"
   },
   "source": [
    "Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "diKZnB1I3l_R",
    "outputId": "9b3ac725-0117-4830-f380-a555ee57c8cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "[17:02:02.942] [poptorch:cpp] [warning] Graph contains an unused input %attention_mask : Long(1, 128, strides=[128, 1], requires_grad=0, device=cpu)\n",
      "Graph compilation:   3%|â–ˆâ–ˆâ–ˆâ–                                                                                                             | 3/100 [00:03<02:07]2022-08-19T16:02:10.584383Z popart:devicex 1576165.1576165 W: The `debug.retainDebugInformation` engine option was implicitly set to `true`. The default will change to `false` in a future release. Set it to `true` explicitly if you want to query debug information (for example, by calling `Session::getReport`).\n",
      "2022-08-19T16:02:13.576535Z popart:devicex 1576165.1576165 W: The `debug.retainDebugInformation` engine option was implicitly set to `true`. The default will change to `false` in a future release. Set it to `true` explicitly if you want to query debug information (for example, by calling `Session::getReport`).\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:51<00:00]\n",
      "Compiled/Loaded model in 138.4036930827424 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1931\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facde5a978ca4e56b78752f41b9422d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 780.67\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity is still quite high since for this demo we trained on a small dataset for a small number of epochs. For a real LM training, you  would need a larger dataset and more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Train a language model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
