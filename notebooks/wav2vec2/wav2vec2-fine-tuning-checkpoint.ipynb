{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb00796b",
   "metadata": {},
   "source": [
    "# wav2vec 2.0 Fine-Tuning on IPU\n",
    "\n",
    "This notebook will demonstrate how to fine-tune a pre-trained wav2vec 2.0 model with PyTorch on the Graphcore IPU-POD16 system. We will use a \"wav2vec2-base\" model and fine-tune for a CTC downstream task using LibriSpeech.\n",
    "\n",
    "We will show how to use a wav2vec 2.0 model written in PyTorch from the ü§ó`transformers` library from HuggingFace and paralllize it easily using the ü§ó`optimum-graphcore` library.\n",
    "\n",
    "### Background\n",
    "\n",
    "ASR (Automatic Speech Recognition), the task of transcribing audio automatically, has historically required large amounts of labelled data. Additionally, these systems had predominantly used fixed feature extraction methods which do not learn from the raw signal, e.g., STFT, or Mel-Frequency. Research conducted by Facebook AI (now Meta AI) demonstrates a ‚Äúframework for self-supervised learning for speech representations‚Äù. In other words, a pre-training phase and architecture which can learn feature representations, and their relationships, by leveraging large amounts of unlabelled, raw audio data.  \n",
    "\n",
    "There are two phases to training: pre-training on unlabelled data, and fine-tuning on a down-stream task. In the original literature the model is fine-tuned for CTC (connectionist temporal classification), which is an ASR task. The consistent modules between pre-training and fine-tuning are what you‚Äôd expect to see in a CTC system; it has feature extraction, and an encoder. But, unlike many models of the past, the feature extraction is a convolutional neural network, which makes it trainable. Following that there is a BERT-style encoder where a large convolutional block is used before the first layers, rather than using sinusoidal positional encoding.  \n",
    "\n",
    "### Environment\n",
    "\n",
    "Requirements:\n",
    "- A Poplar SDK environment enabled (see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) guide for your IPU system)\n",
    "- Python packages installed with `python -m pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add16b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8733e4e",
   "metadata": {},
   "source": [
    "To run this Jupyter notebook on a remote IPU machine:\n",
    "1. Enable a Poplar SDK environment \n",
    "(see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) \n",
    " guide for your IPU system) and install required packages with `python -m pip install -r requirements.txt`\n",
    "2. In the same environment, install the Jupyter notebook server: `python -m pip install notebook`\n",
    "3. Launch a Jupyter Server on a specific port: `jupyter-notebook --no-browser --port <port number>`\n",
    "4. Connect via SSH to your remote machine, forwarding your chosen port:\n",
    "`ssh -NL <port number>:localhost:<port number> <your username>@<remote machine>`\n",
    "\n",
    "For more details about this process, or if you need troubleshooting, \n",
    "see our [guide on using IPUs from Jupyter notebooks](../../standard_tools/using_jupyter/README.md).\"\n",
    "\n",
    "### Graphcore Hugging Face models\n",
    "Hugging Face provides convenient access to pre-trained transformer models. The partnership between Hugging Face and Graphcore allows us to run these models on the IPU.\n",
    "\n",
    "Hugging Face models ported to the IPU can be found on the Graphcore organisation page on Hugging Face. \n",
    "\n",
    "### Utility imports\n",
    "We start by importing the utilities that will be used later in the tutorial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3dae5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, load_metric\n",
    "\n",
    "import transformers\n",
    "from optimum.graphcore import IPUConfig, IPUTrainer\n",
    "from optimum.graphcore import IPUTrainingArguments\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForCTC,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Wav2Vec2Processor,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3baf3ee",
   "metadata": {},
   "source": [
    "## Preparing the LibriSpeech Dataset\n",
    "\n",
    "The ü§ó`datasets` library from HuggingFace can be used to conventiuently load the LibriSpeech dataset, as well as provide easy to use tool to process the data.\n",
    "\n",
    "First we are going to create a `DatasetDict` dictionary to handle our data, and then load the LibriSpeech splits for training and validation. For this notebook we will use `train.100` which is 100 hours of clean training data. Section C of the appendix in the [paper](https://arxiv.org/abs/2006.11477) suggests that fine-tuning a `Base` model can yield 6.1% WER without an additional language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72874ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "Reusing dataset librispeech_asr (/home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "raw_datasets[\"train\"] = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.100\")\n",
    "raw_datasets[\"eval\"] = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abca91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 28539\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ac40f",
   "metadata": {},
   "source": [
    "### Text Normalisation\n",
    "\n",
    "Using the package `map` function, any special characters are removed from the transcription. The resultant transcript is then lower-cased. These two processes means that the model will not have to learn punctuation and capatilsation, although it may have the ability to do so, this is much easier for the model to do.\n",
    "\n",
    "There are other situations where text normalisation may be used like converting digits into their text counterpart. This is not performed in this script as LibriSpeech already has the text counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28065436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-8c8acfcd6e3fb31e.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2519815fd64082dc.arrow\n"
     ]
    }
   ],
   "source": [
    "chars_to_ignore_regex = \"\".join([\",\",\"?\",\".\",\"!\",\"-\",\"\\;\",\"\\:\",\"\\\"\",\"‚Äú\",\"%\",\"‚Äò\",\"‚Äù\",\"ÔøΩ\"])\n",
    "text_column_name = \"text\"\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    if chars_to_ignore_regex is not None:\n",
    "        batch[\"target_text\"] = re.sub(chars_to_ignore_regex, \"\", batch[text_column_name]).lower() + \" \"\n",
    "    else:\n",
    "        batch[\"target_text\"] = batch[text_column_name].lower() + \" \"\n",
    "    return batch\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    remove_special_characters,\n",
    "    remove_columns=[text_column_name],\n",
    "    desc=\"remove special characters from datasets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c580de",
   "metadata": {},
   "source": [
    "### Create Vocabulary and Tokenizer\n",
    "\n",
    "We now create a vocabulary from the dataset. This will find all the unique characters from all the normalised text in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "342e7e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.36s/ba]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22ba/s]\n"
     ]
    }
   ],
   "source": [
    "def create_vocabulary_from_data(\n",
    "        datasets: DatasetDict,\n",
    "        word_delimiter_token = None,\n",
    "        unk_token = None,\n",
    "        pad_token = None,\n",
    "):\n",
    "    # Given training and test labels create vocabulary\n",
    "    def extract_all_chars(batch):\n",
    "        all_text = \" \".join(batch[\"target_text\"])\n",
    "        vocab = list(set(all_text))\n",
    "        return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "    vocabs = datasets.map(\n",
    "        extract_all_chars,\n",
    "        batched=True,\n",
    "        batch_size=-1,\n",
    "        keep_in_memory=True,\n",
    "        remove_columns=datasets[\"train\"].column_names,\n",
    "    )\n",
    "\n",
    "    # take union of all unique characters in each dataset\n",
    "    vocab_set = functools.reduce(\n",
    "        lambda vocab_1, vocab_2: set(vocab_1[\"vocab\"][0]) | set(vocab_2[\"vocab\"][0]), vocabs.values()\n",
    "    )\n",
    "\n",
    "    vocab_dict = {v: k for k, v in enumerate(sorted(list(vocab_set)))}\n",
    "\n",
    "    # replace white space with delimiter token\n",
    "    if word_delimiter_token is not None:\n",
    "        vocab_dict[word_delimiter_token] = vocab_dict[\" \"]\n",
    "        del vocab_dict[\" \"]\n",
    "\n",
    "    # add unk and pad token\n",
    "    if unk_token is not None:\n",
    "        vocab_dict[unk_token] = len(vocab_dict)\n",
    "\n",
    "    if pad_token is not None:\n",
    "        vocab_dict[pad_token] = len(vocab_dict)\n",
    "\n",
    "    return vocab_dict\n",
    "\n",
    "word_delimiter_token = \"|\"\n",
    "unk_token = \"[UNK]\"\n",
    "pad_token = \"[PAD]\"\n",
    "\n",
    "vocab_dict = create_vocabulary_from_data(raw_datasets, \n",
    "                                         word_delimiter_token=word_delimiter_token, \n",
    "                                         unk_token=unk_token, \n",
    "                                         pad_token=pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4541263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\": 1,\n",
       " 'a': 2,\n",
       " 'b': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'g': 8,\n",
       " 'h': 9,\n",
       " 'i': 10,\n",
       " 'j': 11,\n",
       " 'k': 12,\n",
       " 'l': 13,\n",
       " 'm': 14,\n",
       " 'n': 15,\n",
       " 'o': 16,\n",
       " 'p': 17,\n",
       " 'q': 18,\n",
       " 'r': 19,\n",
       " 's': 20,\n",
       " 't': 21,\n",
       " 'u': 22,\n",
       " 'v': 23,\n",
       " 'w': 24,\n",
       " 'x': 25,\n",
       " 'y': 26,\n",
       " 'z': 27,\n",
       " '|': 0,\n",
       " '[UNK]': 28,\n",
       " '[PAD]': 29}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20922bdc",
   "metadata": {},
   "source": [
    "With the vocabulary generated from the normalised trascripts we create a `tokenizer` which is included in the ü§ó`transformers` library. This will later be used to encode text into indexes, and decode indexes into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9fda29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_name_or_path = \"/tmp/wav2vec2-notebook\"\n",
    "\n",
    "vocab_file = os.path.join(tokenizer_name_or_path, \"vocab.json\")\n",
    "\n",
    "if os.path.isfile(vocab_file):\n",
    "    os.remove(vocab_file)\n",
    "\n",
    "os.makedirs(tokenizer_name_or_path, exist_ok=True)\n",
    "\n",
    "with open(vocab_file, \"w\") as file:\n",
    "    json.dump(vocab_dict, file)\n",
    "    \n",
    "tokenizer_kwargs = {\n",
    "    \"config\": None,\n",
    "    \"tokenizer_type\": \"wav2vec2\",\n",
    "    \"unk_token\": unk_token,\n",
    "    \"pad_token\": pad_token,\n",
    "    \"word_delimiter_token\": word_delimiter_token,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, use_auth_token=False, **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d06af",
   "metadata": {},
   "source": [
    "Let's look at an example for using the tokenizer. The vocabulary does not contain any digits, so these will be set to `[UNK]`. Remember, any special characters (such as commas) have already been removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a82868fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [24, 2, 23, 28, 23, 6, 4, 28, 0, 7, 10, 15, 6, 21, 22, 15, 10, 15, 8, 0, 16, 15, 0, 10, 17, 22], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"wav2vec2 finetuning on ipu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdae8729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wav[UNK]vec[UNK] finetuning on ipu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"wav2vec2 finetuning on ipu\").input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bbe23",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Now we generate the feature extraction method for the model and map it across the datasets onto the audio data. In this model we are learning from raw audio signal, so the feature extraction is just used to resample the audio to the rate which the model expects. \n",
    "\n",
    "Afterwards we set the minimum and maximum input lengths in samples. These are set to 2.0 and 15.6 seconds, converted to 32000 and 249600. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e53f1437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/configuration_utils.py:359: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "dataset_sampling_rate = next(iter(raw_datasets.values())).features[\"audio\"].sampling_rate\n",
    "if dataset_sampling_rate != 16000:\n",
    "    raw_datasets = raw_datasets.cast_column(\"audio\", datasets.features.Audio(sampling_rate=16000))\n",
    "\n",
    "max_input_length = int(15.6 * feature_extractor.sampling_rate)\n",
    "min_input_length = int(2.0 * feature_extractor.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd71404",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "In this step both the feature extraction and tokenization are applied to the audio and transcript, respectively. The feature extractor resamples the audio, and the tokenizer will convert the normalised text into indexes.\n",
    "\n",
    "After the map function, the dataset will be filtered by the audio length. If the length of the raw audio is not between 2.0 and 15.6 seconds then it will be removed from the data. The result of filtering is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f052fcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-299bce06cb35f50b.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-439b13de76e6512e.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-126dce57a7d4e9cb.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-c8c5d63b1a540489.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-5c84e472820247b7.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-99dce465a13b86dc.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-e55c408c59e6e1d2.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-b2b9e4745570fe98.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-1694345bdb4420c3.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-1052d17943e61415.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2dcbb73f3665c041.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-7c0faa8ea2fd8546.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-83084a83c2d4c4f2.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-4f1b0f79a3ffa185.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-747478d73144e6fb.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-358676cc86d45f6b.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-d1e0a2550907bb04_00000_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-d1e0a2550907bb04_00001_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-d1e0a2550907bb04_00002_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-d1e0a2550907bb04_00003_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-d1e0a2550907bb04_00004_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-d1e0a2550907bb04_00005_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-d1e0a2550907bb04_00006_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-d1e0a2550907bb04_00007_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-84ada26fe2c21627_00000_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-84ada26fe2c21627_00001_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-84ada26fe2c21627_00002_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-84ada26fe2c21627_00003_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-84ada26fe2c21627_00004_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-84ada26fe2c21627_00005_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-84ada26fe2c21627_00006_of_00008.arrow\n",
      "Loading cached processed dataset at /home/thorinf/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-84ada26fe2c21627_00007_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    sample = batch[\"audio\"]\n",
    "\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    batch[\"input_values\"] = inputs.input_values[0]\n",
    "    batch[\"input_length\"] = len(inputs.input_values[0])\n",
    "    \n",
    "    batch[\"labels\"] = tokenizer(batch[\"target_text\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    try:\n",
    "        return length > min_input_length and length < max_input_length\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "vectorized_datasets = raw_datasets.map(prepare_dataset, \n",
    "                                       remove_columns=raw_datasets[\"train\"].column_names,\n",
    "                                       num_proc=8,\n",
    "                                       desc=\"preprocess datasets\")\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(is_audio_in_length_range,\n",
    "                                                 input_columns=[\"input_length\"],\n",
    "                                                 num_proc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcce07e",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "With the dataset prepared, the majority of the processing is complete nearly fit to be sent to the model. The role of the collator is to pad the resampled audio and encoded text to a static size. The padding values for audio will be set to `0.0` but for the indexes they will be `-100` so it's not confused with an index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f57898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.AutoProcessor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        return batch.data\n",
    "    \n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, pad_to_multiple_of=int(max_input_length),\n",
    "                                               pad_to_multiple_of_labels=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56b494",
   "metadata": {},
   "source": [
    "## Preparing the Model\n",
    "\n",
    "\n",
    "For the model we are using `wav2vec2-base` from the HuggingFace model-hub. This model has been pretrained only.\n",
    "Some of the defaults options for the model will need to be changed for training:\n",
    "* CTC loss will be normalised by the lengths\n",
    "* There is no masking of the features to be applied so both masks are set to 0.0, the current masking strategy isn't supported on IPU.\n",
    "* The [PAD] index and vocabulary size are later used in the model for the final output layer and CTC-loss.\n",
    "* Epsilon adjusted for FP16 training.\n",
    "\n",
    "\n",
    "The IPU config describes how to parallelise the model across several IPUs. It also includes additional options such as gradient accumulation, device iterations, and memory proportion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53b0e35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_q.bias', 'quantizer.codevectors', 'project_hid.bias', 'quantizer.weight_proj.bias', 'wav2vec2.masked_spec_embed', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting replicated_tensor_sharding to False when replication_factor=1\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "config.update(\n",
    "    {\n",
    "        \"ctc_loss_reduction\": \"mean\",\n",
    "        \"mask_time_prob\": 0.0,\n",
    "        \"mask_feature_prob\": 0.0,\n",
    "        \"layerdrop\": 0.0,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"vocab_size\": len(tokenizer),\n",
    "        \"layer_norm_eps\": 0.0001,\n",
    "    }\n",
    ")\n",
    "\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base\", config=config)\n",
    "\n",
    "ipu_config = IPUConfig.from_pretrained(\"../../examples/speech-recognition/base_4-ipu_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e79f613e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Config {\n",
       "  \"_name_or_path\": \"facebook/wav2vec2-base\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"adapter_kernel_size\": 3,\n",
       "  \"adapter_stride\": 2,\n",
       "  \"add_adapter\": false,\n",
       "  \"apply_spec_augment\": true,\n",
       "  \"architectures\": [\n",
       "    \"Wav2Vec2ForPreTraining\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classifier_proj_size\": 256,\n",
       "  \"codevector_dim\": 256,\n",
       "  \"contrastive_logits_temperature\": 0.1,\n",
       "  \"conv_bias\": false,\n",
       "  \"conv_dim\": [\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512\n",
       "  ],\n",
       "  \"conv_kernel\": [\n",
       "    10,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"conv_stride\": [\n",
       "    5,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"ctc_loss_reduction\": \"mean\",\n",
       "  \"ctc_zero_infinity\": false,\n",
       "  \"diversity_loss_weight\": 0.1,\n",
       "  \"do_stable_layer_norm\": false,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"feat_extract_activation\": \"gelu\",\n",
       "  \"feat_extract_norm\": \"group\",\n",
       "  \"feat_proj_dropout\": 0.1,\n",
       "  \"feat_quantizer_dropout\": 0.0,\n",
       "  \"final_dropout\": 0.0,\n",
       "  \"freeze_feat_extract_train\": true,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 0.0001,\n",
       "  \"layerdrop\": 0.0,\n",
       "  \"mask_channel_length\": 10,\n",
       "  \"mask_channel_min_space\": 1,\n",
       "  \"mask_channel_other\": 0.0,\n",
       "  \"mask_channel_prob\": 0.0,\n",
       "  \"mask_channel_selection\": \"static\",\n",
       "  \"mask_feature_length\": 10,\n",
       "  \"mask_feature_min_masks\": 0,\n",
       "  \"mask_feature_prob\": 0.0,\n",
       "  \"mask_time_length\": 10,\n",
       "  \"mask_time_min_masks\": 2,\n",
       "  \"mask_time_min_space\": 1,\n",
       "  \"mask_time_other\": 0.0,\n",
       "  \"mask_time_prob\": 0.0,\n",
       "  \"mask_time_selection\": \"static\",\n",
       "  \"model_type\": \"wav2vec2\",\n",
       "  \"no_mask_channel_overlap\": false,\n",
       "  \"no_mask_time_overlap\": false,\n",
       "  \"num_adapter_layers\": 3,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_codevector_groups\": 2,\n",
       "  \"num_codevectors_per_group\": 320,\n",
       "  \"num_conv_pos_embedding_groups\": 16,\n",
       "  \"num_conv_pos_embeddings\": 128,\n",
       "  \"num_feat_extract_layers\": 7,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_negatives\": 100,\n",
       "  \"output_hidden_size\": 768,\n",
       "  \"pad_token_id\": 29,\n",
       "  \"proj_codevector_dim\": 256,\n",
       "  \"tdnn_dilation\": [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    1,\n",
       "    1\n",
       "  ],\n",
       "  \"tdnn_dim\": [\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    1500\n",
       "  ],\n",
       "  \"tdnn_kernel\": [\n",
       "    5,\n",
       "    3,\n",
       "    3,\n",
       "    1,\n",
       "    1\n",
       "  ],\n",
       "  \"transformers_version\": \"4.18.0\",\n",
       "  \"use_weighted_layer_sum\": false,\n",
       "  \"vocab_size\": 32,\n",
       "  \"xvector_output_dim\": 512\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c69b73",
   "metadata": {},
   "source": [
    "Let's set our training hyperparameters using `IPUTrainingArguments`. This subclasses the Hugging Face `TrainingArguments` class, adding parameters specific to the IPU and its execution characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d0c80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = IPUTrainingArguments(output_dir=\"./demo\",\n",
    "                                     overwrite_output_dir=True,\n",
    "                                     do_train=True, \n",
    "                                     do_eval=True, \n",
    "                                     evaluation_strategy=\"epoch\",\n",
    "                                  learning_rate=3e-4,\n",
    "                                  num_train_epochs=5.0,\n",
    "                                  adam_epsilon=0.0001,\n",
    "                                  warmup_steps=400,\n",
    "                                  logging_steps=100,\n",
    "                                  dataloader_drop_last=True,\n",
    "                                  dataloader_num_workers=16,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "082a3007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./demo/tokenizer_config.json',\n",
       " './demo/special_tokens_map.json',\n",
       " './demo/vocab.json',\n",
       " './demo/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.save_pretrained(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7df96",
   "metadata": {},
   "source": [
    "The performance of the model is measured using the WER. This metric takes a predicted string and the correct string and computes an edit distance normalised by the length. For many sentences the sum of the edit distances is normalised by the sum of the lengths. \n",
    "\n",
    "To add this metric to our evaluation we define a `compute_metrics` function and load the metric from the `datasets` package. This is performed once after all the evaluation outputs have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77d1ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = {\"wer\": load_metric(\"wer\")}\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    metrics = {k: v.compute(predictions=pred_str, references=label_str) for k, v in eval_metrics.items()}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97894b6e",
   "metadata": {},
   "source": [
    "To train the model, we define a trainer using the `IPUTrainer` class which takes care of compiling the model to run on IPUs, and of performing training and evaluation. The `IPUTrainer` class works just like the HuggingFace `Trainer` class, but takes the additional `ipu_config` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f79a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "---------- Device Allocation -----------\n",
      "Conv 0  --> IPU 0\n",
      "Conv 1  --> IPU 0\n",
      "Conv 2  --> IPU 0\n",
      "Conv 3  --> IPU 0\n",
      "Conv 4  --> IPU 0\n",
      "Conv 5  --> IPU 0\n",
      "Conv 6  --> IPU 0\n",
      "Positional Embedding --> IPU 0\n",
      "Encoder 0  --> IPU 0\n",
      "Encoder 1  --> IPU 1\n",
      "Encoder 2  --> IPU 1\n",
      "Encoder 3  --> IPU 1\n",
      "Encoder 4  --> IPU 1\n",
      "Encoder 5  --> IPU 2\n",
      "Encoder 6  --> IPU 2\n",
      "Encoder 7  --> IPU 2\n",
      "Encoder 8  --> IPU 2\n",
      "Encoder 9  --> IPU 3\n",
      "Encoder 10 --> IPU 3\n",
      "Encoder 11 --> IPU 3\n",
      "Project Hidden --> IPU 3\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = IPUTrainer(\n",
    "    model=model,\n",
    "    ipu_config=ipu_config,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=vectorized_datasets[\"train\"],\n",
    "    eval_dataset=vectorized_datasets[\"eval\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e91b6",
   "metadata": {},
   "source": [
    "## Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a4b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `PipelinedWav2Vec2ForCTC.forward` and have been ignored: input_length.\n",
      "Compiling Model...\n",
      "/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/torch/nn/functional.py:2359: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n",
      "/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:637: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:674: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Graph compilation:  28%|‚ñà‚ñà‚ñä       | 28/100 [01:38<08:14]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f52049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
