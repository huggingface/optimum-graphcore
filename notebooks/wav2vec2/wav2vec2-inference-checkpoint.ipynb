{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2aa9aa",
   "metadata": {},
   "source": [
    "# wav2vec 2.0 Inference on IPU\n",
    "\n",
    "This notebook will demonstrate how to perform wav2vec 2.0 inference with PyTorch on the Graphcore IPU-POD16 system. We will use a `wav2vec2-base` model fine-tuned for a CTC downstream task using LibriSpeech.\n",
    "\n",
    "We will show how to use a wav2vec 2.0 model written in PyTorch from the ðŸ¤—`transformers` library from HuggingFace and paralllize it easily using the ðŸ¤—`optimum-graphcore` library.\n",
    "\n",
    "### Environment\n",
    "\n",
    "Requirements:\n",
    "- A Poplar SDK environment enabled (see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) guide for your IPU system)\n",
    "- Python packages installed with `python -m pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7babe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt update\n",
    "apt-get install libsndfile1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225d2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install transformer==4.20.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1157693",
   "metadata": {},
   "source": [
    "To run this Jupyter notebook on a remote IPU machine:\n",
    "1. Enable a Poplar SDK environment \n",
    "(see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) \n",
    " guide for your IPU system) and install required packages with `python -m pip install -r requirements.txt`\n",
    "2. In the same environment, install the Jupyter notebook server: `python -m pip install notebook`\n",
    "3. Launch a Jupyter Server on a specific port: `jupyter-notebook --no-browser --port <port number>`\n",
    "4. Connect via SSH to your remote machine, forwarding your chosen port:\n",
    "`ssh -NL <port number>:localhost:<port number> <your username>@<remote machine>`\n",
    "\n",
    "For more details about this process, or if you need troubleshooting, \n",
    "see our [guide on using IPUs from Jupyter notebooks](../../standard_tools/using_jupyter/README.md).\"\n",
    "\n",
    "### Graphcore Hugging Face models\n",
    "Hugging Face provides convenient access to pre-trained transformer models. The partnership between Hugging Face and Graphcore allows us to run these models on the IPU.\n",
    "\n",
    "Hugging Face models ported to the IPU can be found on the Graphcore organisation page on Hugging Face. \n",
    "\n",
    "### Utility imports\n",
    "We start by importing the utilities that will be used later in the tutorial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e2c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import poptorch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "from transformers import (\n",
    "    AutoModelForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    HfArgumentParser,\n",
    ")\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8313f3a",
   "metadata": {},
   "source": [
    "Values for machine size and cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", \"pod16\")\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\")\n",
    "checkpoint_directory = Path(os.getenv(\"CHECKPOINT_DIR\", \"/tmp\")) / \"demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bea6c",
   "metadata": {},
   "source": [
    "## Preparing the Model\n",
    "\n",
    "This notebook will be using the model output from the finetuning notebook. If you have not run the finetuning notebook, nor have a output directory, then this script will not run.\n",
    "\n",
    "As this model does not require optimising the full `base` inference model can fit on a single IPU, this makes the IPU configuration very simple. The `num_device_iterations` will control how many iterations the IPU will perform before returning to host. With this set to 10, 10 utterances will be sent to the IPU, processed, and sent back as a block of 10. \n",
    "\n",
    "We create the pipelined version of the model which makes changes for the IPU version of the model. And finally convert the model into a `poptorch.inferenceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e28c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(checkpoint_directory)\n",
    "model = AutoModelForCTC.from_pretrained(checkpoint_directory)\n",
    "\n",
    "num_device_iterations = 10\n",
    "ipu_config = IPUConfig(inference_device_iterations=num_device_iterations)\n",
    "opts = ipu_config.to_options(for_inference=True)\n",
    "\n",
    "ipu_model = to_pipelined(model, ipu_config)\n",
    "ipu_model.parallelize()\n",
    "\n",
    "inference_model = poptorch.inferenceModel(ipu_model.half().eval(), options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14432c0",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "The sample batch is an example of what a batch could look like. Effectively we are setting the static size for the model input. The first dimension is the product of the `batch_size` and `num_device_iterations`, however in this case the batch size is just 1. The second dimension is the maximum audio length in samples, we've set this to 20 seconds.\n",
    "\n",
    "The model will then compile for this input size. If the size is changed later the model will recompile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d92113",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 400000\n",
    "sample_batch = {\"input_values\": torch.zeros([num_device_iterations, max_samples], dtype=torch.half)}\n",
    "\n",
    "inference_model.compile(**sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ad903",
   "metadata": {},
   "source": [
    "### LibriSpeech Inferecence\n",
    "\n",
    "We will test the inference capabilities of a finetuned model on a portion of the LibriSpeech `test` split. First, download the dataset using the ðŸ¤—`datasets` library from HuggingFace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c7572",
   "metadata": {},
   "source": [
    "### Create a Batch\n",
    "\n",
    "Here we take examples from LibriSpeech test and place them into a `zeros` Tensor to create a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros([num_device_iterations, max_samples], dtype=torch.half)\n",
    "\n",
    "for i in range(num_device_iterations):\n",
    "    input_values = processor(\n",
    "        ds[i][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\"\n",
    "    ).input_values  # Batch size 1\n",
    "    length = input_values.size(1)\n",
    "    x[i, :length] = input_values[0]\n",
    "\n",
    "batch = {\"input_values\": x}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d5520",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Running the model will perform `num_device_iterations` on the IPU before returning to host. This means that all of our logits will be returned at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = inference_model(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa60c49",
   "metadata": {},
   "source": [
    "### Decode\n",
    "\n",
    "The max arg of the logits is taked from every frame of the output, this is a 'greedy decode' strategy. The processor will then convert the predicted indexes back into text, and the transcripts will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[0]\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689124f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
