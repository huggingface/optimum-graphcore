{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2aa9aa",
   "metadata": {},
   "source": [
    "# wav2vec 2.0 Inference on IPU\n",
    "\n",
    "This notebook will demonstrate how to perform wav2vec 2.0 inference with PyTorch on the Graphcore IPU-POD16 system. We will use a `wav2vec2-base` model fine-tuned for a CTC downstream task using LibriSpeech.\n",
    "\n",
    "We will show how to use a wav2vec 2.0 model written in PyTorch from the ðŸ¤—`transformers` library from HuggingFace and paralllize it easily using the ðŸ¤—`optimum-graphcore` library.\n",
    "\n",
    "### Environment\n",
    "\n",
    "Requirements:\n",
    "- A Poplar SDK environment enabled (see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) guide for your IPU system)\n",
    "- Python packages installed with `python -m pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225d2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "% pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1157693",
   "metadata": {},
   "source": [
    "To run this Jupyter notebook on a remote IPU machine:\n",
    "1. Enable a Poplar SDK environment \n",
    "(see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) \n",
    " guide for your IPU system) and install required packages with `python -m pip install -r requirements.txt`\n",
    "2. In the same environment, install the Jupyter notebook server: `python -m pip install notebook`\n",
    "3. Launch a Jupyter Server on a specific port: `jupyter-notebook --no-browser --port <port number>`\n",
    "4. Connect via SSH to your remote machine, forwarding your chosen port:\n",
    "`ssh -NL <port number>:localhost:<port number> <your username>@<remote machine>`\n",
    "\n",
    "For more details about this process, or if you need troubleshooting, \n",
    "see our [guide on using IPUs from Jupyter notebooks](../../standard_tools/using_jupyter/README.md).\"\n",
    "\n",
    "### Graphcore Hugging Face models\n",
    "Hugging Face provides convenient access to pre-trained transformer models. The partnership between Hugging Face and Graphcore allows us to run these models on the IPU.\n",
    "\n",
    "Hugging Face models ported to the IPU can be found on the Graphcore organisation page on Hugging Face. \n",
    "\n",
    "### Utility imports\n",
    "We start by importing the utilities that will be used later in the tutorial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e2c575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import poptorch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "from transformers import (\n",
    "    AutoModelForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    HfArgumentParser,\n",
    ")\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bea6c",
   "metadata": {},
   "source": [
    "## Preparing the Model\n",
    "\n",
    "This notebook will be using the model output from the finetuning notebook. If you have not run the finetuning notebook, nor have a output directory, then this script will not run.\n",
    "\n",
    "As this model does not require optimising the full `base` inference model can fit on a single IPU, this makes the IPU configuration very simple. The `num_device_iterations` will control how many iterations the IPU will perform before returning to host. With this set to 10, 10 utterances will be sent to the IPU, processed, and sent back as a block of 10. \n",
    "\n",
    "We create the pipelined version of the model which makes changes for the IPU version of the model. And finally convert the model into a `poptorch.inferenceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e28c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for './demo'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './demo' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36m_get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    603\u001B[0m                 \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 604\u001B[0;31m                 \u001B[0muser_agent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muser_agent\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    605\u001B[0m             )\n",
      "\u001B[0;32m/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/utils/hub.py\u001B[0m in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    296\u001B[0m         \u001B[0;31m# File, but it doesn't exist.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 297\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"file {url_or_filename} not found\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    298\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: file ./demo/config.json not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-2ccca8d0ee68>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mprocessor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWav2Vec2Processor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"./demo\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoModelForCTC\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"./demo\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mnum_device_iterations\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mipu_config\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mIPUConfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minference_device_iterations\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnum_device_iterations\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    422\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    423\u001B[0m             config, kwargs = AutoConfig.from_pretrained(\n\u001B[0;32m--> 424\u001B[0;31m                 \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_unused_kwargs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrust_remote_code\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrust_remote_code\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    425\u001B[0m             )\n\u001B[1;32m    426\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"auto_map\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_map\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    650\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"name_or_path\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    651\u001B[0m         \u001B[0mtrust_remote_code\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"trust_remote_code\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 652\u001B[0;31m         \u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_config_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    653\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m\"auto_map\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig_dict\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"AutoConfig\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"auto_map\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    654\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtrust_remote_code\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    546\u001B[0m         \u001B[0moriginal_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    547\u001B[0m         \u001B[0;31m# Get config dict associated with the base config file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 548\u001B[0;31m         \u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_config_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    549\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    550\u001B[0m         \u001B[0;31m# That config file may point us toward another config file to use.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/localdata/thorinf/workspace/venvs/poplar_sdk-ubuntu_18_04-2.6.0+1074-33d3efd05d/2.6.0+1074_poptorch/lib/python3.6/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36m_get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    635\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m             raise EnvironmentError(\n\u001B[0;32m--> 637\u001B[0;31m                 \u001B[0;34mf\"Can't load config for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    638\u001B[0m                 \u001B[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    639\u001B[0m                 \u001B[0;34mf\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: Can't load config for './demo'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './demo' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"./demo\")\n",
    "model = AutoModelForCTC.from_pretrained(\"./demo\")\n",
    "\n",
    "num_device_iterations = 10\n",
    "ipu_config = IPUConfig(inference_device_iterations=num_device_iterations)\n",
    "opts = ipu_config.to_options(for_inference=True)\n",
    "\n",
    "ipu_model = to_pipelined(model, ipu_config)\n",
    "ipu_model.parallelize()\n",
    "\n",
    "inference_model = poptorch.inferenceModel(ipu_model.half().eval(), options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14432c0",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "The sample batch is an example of what a batch could look like. Effectively we are setting the static size for the model input. The first dimension is the product of the `batch_size` and `num_device_iterations`, however in this case the batch size is just 1. The second dimension is the maximum audio length in samples, we've set this to 20 seconds.\n",
    "\n",
    "The model will then compile for this input size. If the size is changed later the model will recompile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d92113",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 400000\n",
    "sample_batch = {\"input_values\": torch.zeros([num_device_iterations, max_samples])}\n",
    "\n",
    "inference_model.compile(**sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ad903",
   "metadata": {},
   "source": [
    "### LibriSpeech Inferecence\n",
    "\n",
    "We will test the inference capabilities of a finetuned model on a portion of the LibriSpeech `test` split. First, download the dataset using the ðŸ¤—`datasets` library from HuggingFace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c7572",
   "metadata": {},
   "source": [
    "### Create a Batch\n",
    "\n",
    "Here we take examples from LibriSpeech test and place them into a `zeros` Tensor to create a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros([num_device_iterations, max_samples])\n",
    "\n",
    "for i in range(num_device_iterations):\n",
    "    input_values = processor(\n",
    "        ds[i][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\"\n",
    "    ).input_values  # Batch size 1\n",
    "    length = input_values.size(1)\n",
    "    x[i, :length] = input_values[0]\n",
    "\n",
    "batch = {\"input_values\": x}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d5520",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Running the model will perform `num_device_iterations` on the IPU before returning to host. This means that all of our logits will be returned at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = inference_model(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa60c49",
   "metadata": {},
   "source": [
    "### Decode\n",
    "\n",
    "The max arg of the logits is taked from every frame of the output, this is a 'greedy decode' strategy. The processor will then convert the predicted indexes back into text, and the transcripts will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[0]\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689124f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}