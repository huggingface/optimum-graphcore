{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b804343",
   "metadata": {},
   "source": [
    "# Speech transcription on IPU using Whisper\n",
    "\n",
    "This notebook demonstrates speech transcription on the IPU using the [Whisper implementation in ðŸ¤— Transformers library](https://huggingface.co/spaces/openai/whisper) alongside Optimum-Graphcore.\n",
    "\n",
    "##  ðŸ¤—  Optimum-Graphcore\n",
    "\n",
    "ðŸ¤— Optimum Graphcore is the interface between the ðŸ¤— Transformers library and [Graphcore IPUs](https://www.graphcore.ai/products/ipu).\n",
    "It provides a set of tools enabling model parallelization and loading on IPUs, training and fine-tuning on all the tasks already supported by Transformers while being compatible with the Hugging Face Hub and every model available on it out of the box.\n",
    "\n",
    "ðŸ¤— Optimum Graphcore was designed with one goal in mind: make training and evaluation straightforward for any ðŸ¤— Transformers user while leveraging the complete power of IPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba5be2c-ca34-403c-a56b-7828fd38d69a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:15.547319Z",
     "iopub.status.busy": "2023-03-23T14:22:15.547154Z",
     "iopub.status.idle": "2023-03-23T14:22:32.610106Z",
     "shell.execute_reply": "2023-03-23T14:22:32.609350Z",
     "shell.execute_reply.started": "2023-03-23T14:22:15.547301Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install git+https://github.com/graphcore/optimum-graphcore-fork.git@whisper/poc \"tokenizers<0.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91cfe4d0-8b7c-4c5e-a67b-bd8c8ad6e293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:32.611816Z",
     "iopub.status.busy": "2023-03-23T14:22:32.611640Z",
     "iopub.status.idle": "2023-03-23T14:22:34.637072Z",
     "shell.execute_reply": "2023-03-23T14:22:34.636273Z",
     "shell.execute_reply.started": "2023-03-23T14:22:32.611798Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241ca065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:34.638463Z",
     "iopub.status.busy": "2023-03-23T14:22:34.638285Z",
     "iopub.status.idle": "2023-03-23T14:22:36.697556Z",
     "shell.execute_reply": "2023-03-23T14:22:36.696540Z",
     "shell.execute_reply.started": "2023-03-23T14:22:34.638446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# IPU-specific imports\n",
    "import poptorch\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "\n",
    "# HF-related imports\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "130d401e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:36.699188Z",
     "iopub.status.busy": "2023-03-23T14:22:36.698815Z",
     "iopub.status.idle": "2023-03-23T14:22:36.723854Z",
     "shell.execute_reply": "2023-03-23T14:22:36.723206Z",
     "shell.execute_reply.started": "2023-03-23T14:22:36.699169Z"
    }
   },
   "outputs": [],
   "source": [
    "# A class to collect configuration related parameters\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class IPUWhisperConf:\n",
    "    \"\"\"A data class to collect IPU-related config parameters\"\"\"\n",
    "    model_spec: str\n",
    "    ipus_per_replica: int\n",
    "    pod_type: str\n",
    "\n",
    "ipu_whisper = {\n",
    "    \"tiny\": IPUWhisperConf(model_spec='openai/whisper-tiny.en', ipus_per_replica=2, pod_type=\"pod4\"),\n",
    "    \"large\": IPUWhisperConf(model_spec='openai/whisper-large-v2', ipus_per_replica=16, pod_type=\"pod16\")\n",
    "    # Other sizes will become available in due course\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "620ec022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:36.724635Z",
     "iopub.status.busy": "2023-03-23T14:22:36.724471Z",
     "iopub.status.idle": "2023-03-23T14:22:36.946998Z",
     "shell.execute_reply": "2023-03-23T14:22:36.946112Z",
     "shell.execute_reply.started": "2023-03-23T14:22:36.724618Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf /tmp/whisper_exe_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6731f6d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:36.948151Z",
     "iopub.status.busy": "2023-03-23T14:22:36.947968Z",
     "iopub.status.idle": "2023-03-23T14:22:36.973099Z",
     "shell.execute_reply": "2023-03-23T14:22:36.972480Z",
     "shell.execute_reply.started": "2023-03-23T14:22:36.948132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Whisper parameters: model size and maximum sequence length\n",
    "model_size = \"tiny\"\n",
    "max_length = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edac9c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:36.975159Z",
     "iopub.status.busy": "2023-03-23T14:22:36.974980Z",
     "iopub.status.idle": "2023-03-23T14:22:47.035611Z",
     "shell.execute_reply": "2023-03-23T14:22:47.034794Z",
     "shell.execute_reply.started": "2023-03-23T14:22:36.975141Z"
    }
   },
   "outputs": [],
   "source": [
    "iwc = ipu_whisper[model_size]\n",
    "\n",
    "# Instantiate processor and model\n",
    "processor = WhisperProcessor.from_pretrained(iwc.model_spec)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(iwc.model_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16e8a2d-8a70-4ff5-ba9b-121b317568f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:47.037076Z",
     "iopub.status.busy": "2023-03-23T14:22:47.036894Z",
     "iopub.status.idle": "2023-03-23T14:22:47.062739Z",
     "shell.execute_reply": "2023-03-23T14:22:47.062137Z",
     "shell.execute_reply.started": "2023-03-23T14:22:47.037058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose here the index of the test example to use\n",
    "test_idx = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c5d72f3-cbd6-462f-9741-1726d412c4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:22:47.063693Z",
     "iopub.status.busy": "2023-03-23T14:22:47.063519Z",
     "iopub.status.idle": "2023-03-23T14:23:03.941237Z",
     "shell.execute_reply": "2023-03-23T14:23:03.939089Z",
     "shell.execute_reply.started": "2023-03-23T14:22:47.063676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/tmp/huggingface_caches/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "# load dummy dataset and read soundfiles\n",
    "test_example = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")[test_idx]\n",
    "\n",
    "input_features = processor(test_example[\"audio\"][\"array\"], \n",
    "                           return_tensors=\"pt\",\n",
    "                           sampling_rate=test_example['audio']['sampling_rate']).input_features.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab692b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:23:03.945360Z",
     "iopub.status.busy": "2023-03-23T14:23:03.943564Z",
     "iopub.status.idle": "2023-03-23T14:23:03.981283Z",
     "shell.execute_reply": "2023-03-23T14:23:03.980635Z",
     "shell.execute_reply.started": "2023-03-23T14:23:03.945289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of audio file: 29.4s\n",
      "Expected transcription: LINNELL'S PICTURES ARE A SORT OF UP GUARDS AND AT EM PAINTINGS AND MASON'S EXQUISITE IDYLLS ARE AS NATIONAL AS A JINGO POEM MISTER BIRKET FOSTER'S LANDSCAPES SMILE AT ONE MUCH IN THE SAME WAY THAT MISTER CARKER USED TO FLASH HIS TEETH AND MISTER JOHN COLLIER GIVES HIS SITTER A CHEERFUL SLAP ON THE BACK BEFORE HE SAYS LIKE A SHAMPOOER IN A TURKISH BATH NEXT MAN\n"
     ]
    }
   ],
   "source": [
    "print(f\"Duration of audio file: {test_example['audio']['array'].shape[-1]/test_example['audio']['sampling_rate']:.1f}s\")\n",
    "print(\"Expected transcription:\",test_example[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1085e88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:23:03.982444Z",
     "iopub.status.busy": "2023-03-23T14:23:03.982234Z",
     "iopub.status.idle": "2023-03-23T14:23:04.009366Z",
     "shell.execute_reply": "2023-03-23T14:23:04.008652Z",
     "shell.execute_reply.started": "2023-03-23T14:23:03.982424Z"
    }
   },
   "outputs": [],
   "source": [
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", iwc.pod_type)\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/whisper_exe_cache/\") + \"whisper_inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd474186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:23:04.011630Z",
     "iopub.status.busy": "2023-03-23T14:23:04.011267Z",
     "iopub.status.idle": "2023-03-23T14:23:04.037391Z",
     "shell.execute_reply": "2023-03-23T14:23:04.036783Z",
     "shell.execute_reply.started": "2023-03-23T14:23:04.011609Z"
    }
   },
   "outputs": [],
   "source": [
    "ipu_config = IPUConfig(executable_cache_dir=executable_cache_dir, ipus_per_replica=iwc.ipus_per_replica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1073fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:23:04.041197Z",
     "iopub.status.busy": "2023-03-23T14:23:04.040839Z",
     "iopub.status.idle": "2023-03-23T14:23:05.386941Z",
     "shell.execute_reply": "2023-03-23T14:23:05.385762Z",
     "shell.execute_reply.started": "2023-03-23T14:23:04.041178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapt whisper to run on the IPU\n",
    "pipelined_model = to_pipelined(model, ipu_config)\n",
    "pipelined_model = pipelined_model.parallelize(for_generation=True).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17947b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T14:23:05.388837Z",
     "iopub.status.busy": "2023-03-23T14:23:05.388654Z",
     "iopub.status.idle": "2023-03-23T14:23:12.402729Z",
     "shell.execute_reply": "2023-03-23T14:23:12.401917Z",
     "shell.execute_reply.started": "2023-03-23T14:23:05.388820Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00]\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Linnell's pictures are a sort of up-guards-in-item paintings, and Mason's exquisite idles are as national as a jingo poem. Mr. Birkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap in the back, before he says, like a shampooer and a Turkish bath, next man\n",
      "The transcription consisted of 94 tokens.\n"
     ]
    }
   ],
   "source": [
    "# This triggers a compilation the first time around (unless a precompiled model is available)\n",
    "sample_output = pipelined_model.generate(input_features, max_length=max_length, min_length=3)\n",
    "transcription = processor.batch_decode(sample_output, skip_special_tokens=True)[0]\n",
    "print(\"Transcription:\",transcription)\n",
    "print(\"The transcription consisted of\",sample_output.shape[-1],\"tokens.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
