{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7a2f86",
   "metadata": {},
   "source": [
    "# Multi-lingual ASR Transcription on IPUs using Whisper - LoRA Fine-tuning\n",
    "\n",
    "This notebook demonstrates [LoRA](https://arxiv.org/abs/2106.09685) fine-tuning for multi-lingual speech transcription on the IPU using the [Whisper implementation in the Hugging Face Transformers library](https://huggingface.co/spaces/openai/whisper) alongside [Optimum Graphcore](https://github.com/huggingface/optimum-graphcore). We will be using the Spanish subset of the [Common Voice dataset](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0).\n",
    "\n",
    "Whisper is a versatile speech recognition model that can transcribe speech as well as perform multi-lingual translation and recognition tasks.\n",
    "It was trained on diverse datasets to give human-level speech recognition performance without the need for fine tuning. \n",
    "\n",
    "[ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore) is the interface between the [ðŸ¤— Transformers library](https://huggingface.co/docs/transformers/index) and [Graphcore IPUs](https://www.graphcore.ai/products/ipu).\n",
    "It provides a set of tools enabling model parallelization and loading on IPUs, training and fine-tuning on all the tasks already supported by Transformers while being compatible with the Hugging Face Hub and every model available on it out of the box.\n",
    "\n",
    "LoRA is a training method which injects lower rank trainable matrices into Transformer layers. This greatly reduces the number of trainable parameters in a model, hence accelerating the fine-tuning process whilst consuming less device memory.\n",
    "\n",
    "> **Hardware requirements:** We will fine-tune `whisper-small` with two replicas on the smallest IPU-POD4 machine.\n",
    "\n",
    "|  Domain | Tasks | Model | Datasets | Workflow |   Number of IPUs   | Execution time |\n",
    "|---------|-------|-------|----------|----------|--------------|--------------|\n",
    "| Automatic Speech Recognition | Transcription | Whisper-small | Common Voice (es) dataset | Fine-tuning | 4 | 1hr |\n",
    "\n",
    "[![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3e2a5",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to enable the Poplar SDK. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be57731",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2229d-d6f5-4776-841a-7cf328050d30",
   "metadata": {},
   "source": [
    "This notebook requires SDK >= 3.3, with best performance achieved with SDK 3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8a58c-73dc-4b7c-8f58-e843cb6f32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "sdk_version = !popc --version\n",
    "if sdk_version and (version := re.search(r'\\d+\\.\\d+\\.\\d+', sdk_version[0]).group()) >= '3.4':\n",
    "    pass\n",
    "elif sdk_version and (version := re.search(r'\\d+\\.\\d+\\.\\d+', sdk_version[0]).group()) >= '3.3':\n",
    "    warnings.warn(\"SDK versions lower than 3.4 do not support all the functionality in this notebook so performance will be reduced. We recommend you relaunch the Paperspace Notebook with the Pytorch SDK 3.4 image. You can use https://hub.docker.com/r/graphcore/pytorch-early-access\",\n",
    "                  category=Warning, stacklevel=2)\n",
    "else:\n",
    "    raise ValueError(\"SDK versions lower than 3.3 are not supported by this notebook. Please relaunch the Paperspace Notebook with the Pytorch SDK 3.3 image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d9b99",
   "metadata": {},
   "source": [
    "Install the dependencies the notebook needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde99b10-e2d2-4787-877f-fb120e327ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optimum from source \n",
    "!pip install git+https://github.com/huggingface/optimum-graphcore.git \"soundfile\" \"librosa\" \"evaluate\" \"jiwer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afdb91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "n_ipu = int(os.getenv(\"NUM_AVAILABLE_IPU\", 4))\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/whisper_lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30a5a1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3872977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "\n",
    "# IPU-specific imports\n",
    "from optimum.graphcore import (\n",
    "    pipeline,\n",
    "    IPUConfig, \n",
    "    IPUSeq2SeqTrainer, \n",
    "    IPUSeq2SeqTrainingArguments, \n",
    ")\n",
    "from optimum.graphcore.models.whisper import WhisperProcessorTorch\n",
    "\n",
    "# HF-related imports\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3615b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"es\"\n",
    "MAX_LENGTH = 224\n",
    "MODEL_NAME = \"openai/whisper-small\"\n",
    "TASK = \"transcribe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7631ad",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c7ce56",
   "metadata": {},
   "source": [
    "Common Voice datasets consist of recordings of speakers reading text from Wikipedia in different languages. ðŸ¤— Datasets enables us to download and prepare the training and evaluation splits easily.\n",
    "\n",
    "First, ensure you have accepted the terms of use on the Hugging Face Hub: [mozilla-foundation/common_voice_13_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0). Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873bf3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_13_0 (/home/gorank/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/es/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n",
      "Found cached dataset common_voice_13_0 (/home/gorank/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/es/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 296037\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 15708\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", LANGUAGE, split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"eval\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", LANGUAGE, split=\"test\", use_auth_token=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddfbefc",
   "metadata": {},
   "source": [
    "The columns of interest are `audio` - the raw audio samples - and `sentence` - the corresponding ground truth transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d261f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff4ebfb",
   "metadata": {},
   "source": [
    "Since Whisper was pre-trained on audio sampled at 16kHz, we must ensure the Common Voice samples are downsampled accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8989a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938925f",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28331a5c",
   "metadata": {},
   "source": [
    "We prepare the datasets by extracting features from the raw audio inputs and injecting labels which are simply transcriptions with some basic processing.\n",
    "\n",
    "The feature extraction is provided by ðŸ¤— Transformers `WhisperFeatureExtractor`. To decode generated tokens to text after running the model, we will similarly require a tokenizer, `WhisperTokenizer`. Both of these are wrapped by a `WhisperProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60013423",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessorTorch.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "processor.tokenizer.max_length = MAX_LENGTH\n",
    "processor.tokenizer.set_prefix_tokens(language=LANGUAGE, task=TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3633b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/296037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15708 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch, processor):\n",
    "    inputs = processor.feature_extractor(\n",
    "        raw_speech=batch[\"audio\"][\"array\"],\n",
    "        sampling_rate=batch[\"audio\"][\"sampling_rate\"],\n",
    "    )\n",
    "    batch[\"input_features\"] = inputs.input_features[0].astype(np.float16)\n",
    "\n",
    "    transcription = batch[\"sentence\"]\n",
    "    if transcription.startswith('\"') and transcription.endswith('\"'):\n",
    "        transcription = transcription[1:-1]\n",
    "    if transcription[-1] not in [\".\", \"?\", \"!\"]:\n",
    "        transcription = transcription + \".\"\n",
    "    batch[\"labels\"] = processor.tokenizer(text=transcription).input_ids\n",
    "    return batch\n",
    "\n",
    "# num_proc > 1 hangs if the num_threads are not set.\n",
    "torch.set_num_threads(1)\n",
    "columns_to_remove = common_voice.column_names[\"train\"]\n",
    "common_voice = common_voice.map(\n",
    "    lambda elem: prepare_dataset(elem, processor),\n",
    "    remove_columns=columns_to_remove,\n",
    "    num_proc=1,\n",
    ")\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "train_dataset = common_voice[\"train\"]\n",
    "eval_dataset = common_voice[\"eval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3101275",
   "metadata": {},
   "source": [
    "Lastly, we pre-process the labels by padding them with values that will be ignored during fine-tuning. We do this on the fly via the below data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd5a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithLabelProcessing:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = {}\n",
    "        batch[\"input_features\"] = torch.tensor([feature[\"input_features\"] for feature in features])\n",
    "        \n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\", padding=\"longest\", pad_to_multiple_of=MAX_LENGTH)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c40dc",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3f989",
   "metadata": {},
   "source": [
    "The performance of our fine-tuned model will be evaluated using word error rate (WER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7e7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred, tokenizer):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    pred_ids = np.where(pred_ids != -100, pred_ids, tokenizer.pad_token_id)\n",
    "    label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    normalized_pred_str = [tokenizer._normalize(pred).strip() for pred in pred_str]\n",
    "    normalized_label_str = [tokenizer._normalize(label).strip() for label in label_str]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    normalized_wer = 100 * metric.compute(predictions=normalized_pred_str, references=normalized_label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"normalized_wer\": normalized_wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f25e8",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.max_length = MAX_LENGTH\n",
    "model.generation_config.max_length = MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2db080",
   "metadata": {},
   "source": [
    "Ensure language-appropriate tokens, if any, are set for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(\n",
    "    language=LANGUAGE, task=TASK\n",
    ")\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ff556",
   "metadata": {},
   "source": [
    "## Apply LoRA Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27bd5d",
   "metadata": {},
   "source": [
    "`peft` injects the low-rank trainable matrices into the attention blocks (the query and value projection linear layers) via `get_peft_model`. Observe that the proportion of trainable parameters is only 0.73%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec75ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08888a86",
   "metadata": {},
   "source": [
    "## Fine-tuning Whisper on the IPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995852d7",
   "metadata": {},
   "source": [
    "The resulting `peft.PeftModel` can be directly fine-tuned on the IPU using the `IPUSeq2SeqTrainer`. \n",
    "\n",
    "The `IPUConfig` object specifies how the model will be pipelined across the IPUs. \n",
    "\n",
    "For fine-tuning we place the encoder on one IPU, and the decoder on a different IPU. We then create two replicas to use all four IPUs via data-parallelism.\n",
    "\n",
    "For inference, the entire model is placed on one IPU, and replicated four times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2586d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication_factor = n_ipu // 2\n",
    "ipu_config = IPUConfig.from_dict(\n",
    "    {\n",
    "        \"recompute_checkpoint_every_layer\": True,\n",
    "        \"enable_half_partials\": True,\n",
    "        \"executable_cache_dir\": \"./whisper_exe_cache\",\n",
    "        \"gradient_accumulation_steps\": 32 // replication_factor,\n",
    "        \"replication_factor\": replication_factor,\n",
    "        \"layers_per_ipu\": [12, 12],\n",
    "        \"matmul_proportion\": [0.2, 0.2],\n",
    "        \"projection_serialization_factor\": 5,\n",
    "        \"inference_replication_factor\": 4,\n",
    "        \"inference_layers_per_ipu\": [-1],\n",
    "        \"inference_matmul_proportion\": [0.15],\n",
    "        \"inference_projection_serialization_factor\": 5,\n",
    "    }\n",
    ")\n",
    "eval_parallelize_kwargs = {\n",
    "    \"use_cache\": True,\n",
    "    \"sequence_serialization_factor\": 4, \n",
    "    \"use_cond_encoder\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634f147",
   "metadata": {},
   "source": [
    "Lastly, we specify the arguments controlling the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = IPUSeq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-lora-ipu-checkpoints\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    predict_with_generate=True,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1.0,\n",
    "    warmup_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    max_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=25,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_drop_last=True,\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64aa7d",
   "metadata": {},
   "source": [
    "Then we just need to pass all of this together with our datasets to the `IPUSeq2SeqTrainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ef768",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = IPUSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    ipu_config=ipu_config,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorSpeechSeq2SeqWithLabelProcessing(processor),\n",
    "    compute_metrics=lambda x: compute_metrics(x, processor.tokenizer),\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    eval_parallelize_kwargs=eval_parallelize_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383944a",
   "metadata": {},
   "source": [
    "All that remains is to fine-tune the model! The fine-tuning process should take around an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b7673",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The model should achieve a WER of around X%. The original pretrained checkpoint achieves Y% WER on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e82738",
   "metadata": {},
   "source": [
    "## Using the model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f913e",
   "metadata": {},
   "source": [
    "The fine-tuned model can be used for generation by instantiating it as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc45730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"./whisper-small-lora-ipu-checkpoints/checkpoint-1000\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "processor = WhisperProcessorTorch.from_pretrained(peft_config.base_model_name_or_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(peft_config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "# If inference performance is paramount, we can merge the adapter weights back into the \n",
    "# base model. This should be done in a deployment scenario, but here we skip this step \n",
    "# to demonstrate support of `PeftModel` in pipelines.\n",
    "# model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff7629",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we demonstrated how to fine-tune Whisper for multi-lingual speech recognition and transcription on the IPU. To reduce the fine-tuning time by using more than 2 replicas more IPUs are required. On Paperspace, this is available using either an IPU-POD16 or a BoW-IPU-POD16. Please contact Graphcore if you need assistance running on larger platforms.\n",
    "\n",
    "For all available notebooks, check [IPU-powered Jupyter Notebooks](https://www.graphcore.ai/ipu-jupyter-notebooks) to see how IPUs perform on other tasks.\n",
    "\n",
    "Have a question? Please contact us on our [Graphcore community channel](https://www.graphcore.ai/join-community).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk1401",
   "language": "python",
   "name": "sdk1401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
