33a34,35
> from optimum.graphcore import IPUConfig, IPUTrainer
> from optimum.graphcore import IPUTrainingArguments as TrainingArguments
41,42d42
<     Trainer,
<     TrainingArguments,
102c102
<         default=0.05,
---
>         default=0.0,
290c290
<     processor: AutoProcessor
---
>     processor: Union[AutoProcessor, Wav2Vec2Processor]
307a308,311
>         # lengths = torch.sum(batch["input_values"] != 0.0, 1)
>         # attention_mask = torch.arange(batch["input_values"].shape[-1]).unsqueeze(0) < lengths.unsqueeze(1)
>         # batch["attention_mask"] = attention_mask.type(torch.int32)
> 
317,319c321
<         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
< 
<         batch["labels"] = labels
---
>         batch["labels"] = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
321c323
<         return batch
---
>         return batch.data
346c348,349
<         lambda vocab_1, vocab_2: set(vocab_1["vocab"][0]) | set(vocab_2["vocab"][0]), vocabs.values()
---
>         lambda vocab_1, vocab_2: set(vocab_1["vocab"][0]) | set(vocab_2["vocab"][0]),
>         vocabs.values(),
378a382,384
>     if training_args.gradient_checkpointing:
>         raise ValueError(f"Gradient checkpointing not supported.")
> 
404d409
<     logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)
406,413c411,417
<     # Log on each process the small summary:
<     logger.warning(
<         f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
<         f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
<     )
<     # Set the verbosity to info of the Transformers logger (on main process only):
<     if is_main_process(training_args.local_rank):
<         transformers.utils.logging.set_verbosity_info()
---
>     log_level = training_args.get_process_log_level()
>     logger.setLevel(log_level)
>     datasets.utils.logging.set_verbosity(log_level)
>     transformers.utils.logging.set_verbosity(log_level)
>     transformers.utils.logging.enable_default_handler()
>     transformers.utils.logging.enable_explicit_format()
> 
490c494,496
<         model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_auth_token=data_args.use_auth_token
---
>         model_args.model_name_or_path,
>         cache_dir=model_args.cache_dir,
>         use_auth_token=data_args.use_auth_token,
548a555
> 
550c557,565
<         model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_auth_token=data_args.use_auth_token
---
>         model_args.model_name_or_path,
>         cache_dir=model_args.cache_dir,
>         use_auth_token=data_args.use_auth_token,
>     )
> 
>     ipu_config = IPUConfig.from_pretrained(
>         training_args.ipu_config_name if training_args.ipu_config_name else model_args.model_name_or_path,
>         cache_dir=model_args.cache_dir,
>         use_auth_token=True if data_args.use_auth_token else None,
569a585,586
>             "layer_norm_eps": 0.0001,
>             "apply_spec_augment": False,  # spec_augment not currently supported
582,583c599,600
<     if model_args.freeze_feature_encoder:
<         model.freeze_feature_encoder()
---
>     if not model_args.freeze_feature_encoder:
>         raise NotImplementedError("IPU version of this model freezes the feature encoder. Must be set to True.")
594c611,612
<             data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)
---
>             data_args.audio_column_name,
>             datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate),
608c626,627
<     def prepare_dataset(batch):
---
> 
>     def prepare_dataset(batch, feature_extractor, tokenizer):
615a635,638
>         if not training_args.fp32:
>             # Cast audio inputs to FP16
>             batch["input_values"] = batch["input_values"].astype(np.float16)
> 
621a645
> 
626c650
<             prepare_dataset,
---
>             lambda batch: prepare_dataset(batch, feature_extractor, tokenizer),
672,689c696,701
<     # Now save everything to be able to create a single processor later
<     if is_main_process(training_args.local_rank):
<         # save feature extractor, tokenizer and config
<         feature_extractor.save_pretrained(training_args.output_dir)
<         tokenizer.save_pretrained(training_args.output_dir)
<         config.save_pretrained(training_args.output_dir)
< 
<     try:
<         processor = AutoProcessor.from_pretrained(training_args.output_dir)
<     except (OSError, KeyError):
<         warnings.warn(
<             "Loading a processor from a feature extractor config that does not"
<             " include a `processor_class` attribute is deprecated and will be removed in v5. Please add the following "
<             " attribute to your `preprocessor_config.json` file to suppress this warning: "
<             " `'processor_class': 'Wav2Vec2Processor'`",
<             FutureWarning,
<         )
<         processor = Wav2Vec2Processor.from_pretrained(training_args.output_dir)
---
>     # save feature extractor, tokenizer and config
>     feature_extractor.save_pretrained(training_args.output_dir)
>     tokenizer.save_pretrained(training_args.output_dir)
>     config.save_pretrained(training_args.output_dir)
> 
>     processor = Wav2Vec2Processor(feature_extractor, tokenizer)
692c704,708
<     data_collator = DataCollatorCTCWithPadding(processor=processor)
---
>     data_collator = DataCollatorCTCWithPadding(
>         processor=processor,
>         pad_to_multiple_of=int(max_input_length),
>         pad_to_multiple_of_labels=500,
>     )
695c711
<     trainer = Trainer(
---
>     trainer = IPUTrainer(
696a713
>         ipu_config=ipu_config,
702d718
<         tokenizer=feature_extractor,
709c725
< 
---
>         logger.info("*** Train ***")
