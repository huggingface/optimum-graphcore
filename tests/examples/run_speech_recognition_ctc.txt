34a35,36
> from optimum.graphcore import IPUConfig, IPUTrainer
> from optimum.graphcore import IPUTrainingArguments as TrainingArguments
42,43d43
<     Trainer,
<     TrainingArguments,
103c103
<         default=0.05,
---
>         default=0.0,
291c291
<     processor: AutoProcessor
---
>     processor: Union[AutoProcessor, Wav2Vec2Processor]
323c323
<         return batch
---
>         return batch.data
348c348,349
<         lambda vocab_1, vocab_2: set(vocab_1["vocab"][0]) | set(vocab_2["vocab"][0]), vocabs.values()
---
>         lambda vocab_1, vocab_2: set(vocab_1["vocab"][0]) | set(vocab_2["vocab"][0]),
>         vocabs.values(),
380a382,384
>     if training_args.gradient_checkpointing:
>         raise ValueError(f"Gradient checkpointing not supported.")
> 
406d409
<     logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)
408,415c411,417
<     # Log on each process the small summary:
<     logger.warning(
<         f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
<         f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
<     )
<     # Set the verbosity to info of the Transformers logger (on main process only):
<     if is_main_process(training_args.local_rank):
<         transformers.utils.logging.set_verbosity_info()
---
>     log_level = training_args.get_process_log_level()
>     logger.setLevel(log_level)
>     datasets.utils.logging.set_verbosity(log_level)
>     transformers.utils.logging.set_verbosity(log_level)
>     transformers.utils.logging.enable_default_handler()
>     transformers.utils.logging.enable_explicit_format()
> 
492c494,496
<         model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_auth_token=data_args.use_auth_token
---
>         model_args.model_name_or_path,
>         cache_dir=model_args.cache_dir,
>         use_auth_token=data_args.use_auth_token,
550a555
> 
552c557,565
<         model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_auth_token=data_args.use_auth_token
---
>         model_args.model_name_or_path,
>         cache_dir=model_args.cache_dir,
>         use_auth_token=data_args.use_auth_token,
>     )
> 
>     ipu_config = IPUConfig.from_pretrained(
>         training_args.ipu_config_name if training_args.ipu_config_name else model_args.model_name_or_path,
>         cache_dir=model_args.cache_dir,
>         use_auth_token=True if data_args.use_auth_token else None,
571a585,586
>             "layer_norm_eps": 0.0001,
>             "apply_spec_augment": False,  # spec_augment not currently supported
584,585c599,600
<     if model_args.freeze_feature_encoder:
<         model.freeze_feature_encoder()
---
>     if not model_args.freeze_feature_encoder:
>         raise NotImplementedError("IPU version of this model freezes the feature encoder. Must be set to True.")
596c611,612
<             data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)
---
>             data_args.audio_column_name,
>             datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate),
610c626,627
<     def prepare_dataset(batch):
---
> 
>     def prepare_dataset(batch, feature_extractor, tokenizer):
617a635,638
>         if not training_args.fp32:
>             # Cast audio inputs to FP16
>             batch["input_values"] = batch["input_values"].astype(np.float16)
> 
623a645
> 
628c650
<             prepare_dataset,
---
>             lambda batch: prepare_dataset(batch, feature_extractor, tokenizer),
674,691c696,701
<     # Now save everything to be able to create a single processor later
<     if is_main_process(training_args.local_rank):
<         # save feature extractor, tokenizer and config
<         feature_extractor.save_pretrained(training_args.output_dir)
<         tokenizer.save_pretrained(training_args.output_dir)
<         config.save_pretrained(training_args.output_dir)
< 
<     try:
<         processor = AutoProcessor.from_pretrained(training_args.output_dir)
<     except (OSError, KeyError):
<         warnings.warn(
<             "Loading a processor from a feature extractor config that does not"
<             " include a `processor_class` attribute is deprecated and will be removed in v5. Please add the following "
<             " attribute to your `preprocessor_config.json` file to suppress this warning: "
<             " `'processor_class': 'Wav2Vec2Processor'`",
<             FutureWarning,
<         )
<         processor = Wav2Vec2Processor.from_pretrained(training_args.output_dir)
---
>     # save feature extractor, tokenizer and config
>     feature_extractor.save_pretrained(training_args.output_dir)
>     tokenizer.save_pretrained(training_args.output_dir)
>     config.save_pretrained(training_args.output_dir)
> 
>     processor = Wav2Vec2Processor(feature_extractor, tokenizer)
694c704,708
<     data_collator = DataCollatorCTCWithPadding(processor=processor)
---
>     data_collator = DataCollatorCTCWithPadding(
>         processor=processor,
>         pad_to_multiple_of=int(max_input_length),
>         pad_to_multiple_of_labels=500,
>     )
697c711
<     trainer = Trainer(
---
>     trainer = IPUTrainer(
698a713
>         ipu_config=ipu_config,
704d718
<         tokenizer=feature_extractor,
711c725
< 
---
>         logger.info("*** Train ***")
